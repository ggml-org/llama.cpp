#!/bin/bash

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ build Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯
mkdir -p build

# ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠ server Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
if [ ! -f build/server ]; then
  echo "ğŸ”½ Downloading server binary..."
  wget -O build/server \
  https://github.com/issa261/llama.cpp/raw/main/build/server
  chmod +x build/server
fi

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ models Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯
mkdir -p models

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯
if [ ! -f models/tinyllama-1.1b-chat-v1.0.Q8_0.gguf ]; then
  echo "ğŸ”½ Downloading model..."
  wget -O models/tinyllama-1.1b-chat-v1.0.Q8_0.gguf \
  https://raw.githubusercontent.com/issa261/github-workflows-download-model.yml/main/tinyllama-1.1b-chat-v1.0.Q8_0.gguf
fi

# ØªØ´ØºÙŠÙ„ Ø§Ù„Ø®Ø§Ø¯Ù…
echo "ğŸš€ Starting server..."
./build/server -m models/tinyllama-1.1b-chat-v1.0.Q8_0.gguf -c 512 -n 256 --host 0.0.0.0 --port 8080
