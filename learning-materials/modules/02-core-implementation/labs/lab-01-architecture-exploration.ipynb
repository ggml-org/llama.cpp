{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Model Architecture Exploration\n",
    "\n",
    "**Module 2 - Core Implementation**\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Inspect model architecture from GGUF files\n",
    "- Understand transformer layer structure\n",
    "- Calculate parameter counts and memory requirements\n",
    "- Compare different model architectures\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Module 1\n",
    "- Python 3.8+\n",
    "- llama-cpp-python installed\n",
    "- At least one GGUF model file\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install llama-cpp-python numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../code')\n",
    "\n",
    "from architecture_inspector import GGUFReader, extract_architecture, ModelArchitecture\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Read Model Metadata\n",
    "\n",
    "Load and inspect model architecture from a GGUF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your GGUF model\n",
    "MODEL_PATH = \"/path/to/your/model.gguf\"  # Update this!\n",
    "\n",
    "# Read GGUF metadata\n",
    "reader = GGUFReader(MODEL_PATH)\n",
    "reader.read()\n",
    "\n",
    "# Display all metadata keys\n",
    "print(\"Available metadata keys:\")\n",
    "for key in sorted(reader.metadata.keys()):\n",
    "    print(f\"  {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract architecture\n",
    "arch = extract_architecture(reader.metadata)\n",
    "\n",
    "print(f\"Model Name: {arch.name}\")\n",
    "print(f\"Architecture: {arch.architecture}\")\n",
    "print(f\"\\nCore Dimensions:\")\n",
    "print(f\"  Layers: {arch.n_layer}\")\n",
    "print(f\"  Hidden Size: {arch.n_embd:,}\")\n",
    "print(f\"  Vocabulary: {arch.n_vocab:,}\")\n",
    "print(f\"  Context Length: {arch.n_ctx_train:,}\")\n",
    "print(f\"\\nAttention:\")\n",
    "print(f\"  Query Heads: {arch.n_head}\")\n",
    "print(f\"  KV Heads: {arch.n_head_kv}\")\n",
    "print(f\"  Head Dimension: {arch.head_dim}\")\n",
    "print(f\"  GQA Ratio: {arch.gqa_ratio}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Parameter Count Calculation\n",
    "\n",
    "Calculate the approximate number of parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate parameter breakdown\n",
    "embedding_params = arch.n_vocab * arch.n_embd\n",
    "attention_params_per_layer = 4 * arch.n_embd * arch.n_embd  # Q, K, V, O\n",
    "ffn_params_per_layer = 3 * arch.n_embd * arch.n_ff  # Gate, Up, Down\n",
    "norm_params_per_layer = 2 * arch.n_embd  # 2 layer norms\n",
    "\n",
    "layer_params = attention_params_per_layer + ffn_params_per_layer + norm_params_per_layer\n",
    "total_layer_params = layer_params * arch.n_layer\n",
    "output_params = arch.n_vocab * arch.n_embd\n",
    "\n",
    "total_params = embedding_params + total_layer_params + output_params\n",
    "\n",
    "print(\"Parameter Breakdown:\")\n",
    "print(f\"  Embedding:        {embedding_params:>15,} ({embedding_params/total_params*100:>5.1f}%)\")\n",
    "print(f\"  Transformer:      {total_layer_params:>15,} ({total_layer_params/total_params*100:>5.1f}%)\")\n",
    "print(f\"    - Attention:    {attention_params_per_layer * arch.n_layer:>15,}\")\n",
    "print(f\"    - FFN:          {ffn_params_per_layer * arch.n_layer:>15,}\")\n",
    "print(f\"    - Layer Norm:   {norm_params_per_layer * arch.n_layer:>15,}\")\n",
    "print(f\"  Output:           {output_params:>15,} ({output_params/total_params*100:>5.1f}%)\")\n",
    "print(f\"  {'─'*50}\")\n",
    "print(f\"  Total:            {total_params:>15,} (~{total_params/1e9:.1f}B)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize parameter distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "components = ['Embedding', 'Attention\\n(all layers)', 'FFN\\n(all layers)', 'Layer Norms', 'Output']\n",
    "params = [\n",
    "    embedding_params,\n",
    "    attention_params_per_layer * arch.n_layer,\n",
    "    ffn_params_per_layer * arch.n_layer,\n",
    "    norm_params_per_layer * arch.n_layer,\n",
    "    output_params\n",
    "]\n",
    "\n",
    "colors = plt.cm.Set3(range(len(components)))\n",
    "ax.bar(components, [p/1e9 for p in params], color=colors)\n",
    "ax.set_ylabel('Parameters (Billions)')\n",
    "ax.set_title(f'{arch.name} Parameter Distribution')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, (comp, param) in enumerate(zip(components, params)):\n",
    "    ax.text(i, param/1e9, f'{param/1e9:.2f}B', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Memory Requirements\n",
    "\n",
    "Calculate memory requirements for different quantization levels and context lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model size for different quantizations\n",
    "quantizations = {\n",
    "    'FP32': 4,\n",
    "    'FP16': 2,\n",
    "    'Q8_0': 1,\n",
    "    'Q4_0': 0.5,\n",
    "}\n",
    "\n",
    "print(\"Model Size (weights only):\")\n",
    "print(f\"{'Quantization':<12} {'Bytes/Param':<12} {'Total Size':<12}\")\n",
    "print(\"─\" * 40)\n",
    "\n",
    "for quant, bytes_per_param in quantizations.items():\n",
    "    size_bytes = total_params * bytes_per_param\n",
    "    size_gb = size_bytes / (1024**3)\n",
    "    print(f\"{quant:<12} {bytes_per_param:<12.1f} {size_gb:<12.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KV cache size for different context lengths\n",
    "context_lengths = [512, 1024, 2048, 4096, 8192, 16384, 32768]\n",
    "\n",
    "print(\"\\nKV Cache Size (FP16):\")\n",
    "print(f\"{'Context Length':<15} {'Cache Size':<15}\")\n",
    "print(\"─\" * 30)\n",
    "\n",
    "for n_ctx in context_lengths:\n",
    "    cache_size = arch.estimate_kv_cache_size(n_ctx, bytes_per_elem=2)\n",
    "    size_str = arch.format_size(cache_size)\n",
    "    print(f\"{n_ctx:<15,} {size_str:<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize KV cache growth\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "cache_sizes_mb = [arch.estimate_kv_cache_size(n_ctx, 2) / (1024**2) for n_ctx in context_lengths]\n",
    "\n",
    "ax.plot(context_lengths, cache_sizes_mb, marker='o', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Context Length (tokens)')\n",
    "ax.set_ylabel('KV Cache Size (MB)')\n",
    "ax.set_title(f'{arch.name} KV Cache Memory Growth')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xscale('log')\n",
    "\n",
    "# Annotate points\n",
    "for ctx, size in zip(context_lengths, cache_sizes_mb):\n",
    "    ax.annotate(f'{size:.0f}MB', (ctx, size), textcoords=\"offset points\", \n",
    "                xytext=(0,10), ha='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Attention Mechanism Analysis\n",
    "\n",
    "Analyze the attention configuration and its impact on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare MHA vs GQA vs MQA\n",
    "print(\"Attention Configuration Analysis:\")\n",
    "print(f\"\\nCurrent Model: {arch.name}\")\n",
    "print(f\"  Query Heads: {arch.n_head}\")\n",
    "print(f\"  KV Heads: {arch.n_head_kv}\")\n",
    "\n",
    "if arch.n_head == arch.n_head_kv:\n",
    "    attention_type = \"Multi-Head Attention (MHA)\"\n",
    "    efficiency_note = \"Standard, baseline memory usage\"\n",
    "elif arch.n_head_kv == 1:\n",
    "    attention_type = \"Multi-Query Attention (MQA)\"\n",
    "    efficiency_note = f\"{arch.n_head}x more memory efficient than MHA\"\n",
    "else:\n",
    "    attention_type = f\"Grouped-Query Attention (GQA)\"\n",
    "    efficiency_note = f\"{arch.gqa_ratio}x more memory efficient than MHA\"\n",
    "\n",
    "print(f\"\\nType: {attention_type}\")\n",
    "print(f\"Efficiency: {efficiency_note}\")\n",
    "\n",
    "# Calculate KV cache comparison\n",
    "n_ctx = 4096\n",
    "mha_cache = 2 * arch.n_layer * n_ctx * arch.n_head * arch.head_dim * 2\n",
    "current_cache = arch.estimate_kv_cache_size(n_ctx, 2)\n",
    "savings_ratio = mha_cache / current_cache\n",
    "\n",
    "print(f\"\\nKV Cache Comparison @ {n_ctx} context:\")\n",
    "print(f\"  If MHA ({arch.n_head} KV heads): {arch.format_size(mha_cache)}\")\n",
    "print(f\"  Current ({arch.n_head_kv} KV heads): {arch.format_size(current_cache)}\")\n",
    "print(f\"  Savings: {savings_ratio:.1f}x smaller\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: FLOPs Estimation\n",
    "\n",
    "Estimate computational requirements (FLOPs) for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLOPs per token (forward pass)\n",
    "def estimate_flops_per_token(arch: ModelArchitecture, seq_len: int) -> int:\n",
    "    \"\"\"\n",
    "    Estimate FLOPs for generating one token\n",
    "    \n",
    "    For matrix multiply: M×N @ N×K = 2*M*N*K FLOPs\n",
    "    \"\"\"\n",
    "    flops = 0\n",
    "    \n",
    "    for layer in range(arch.n_layer):\n",
    "        # Attention: Q, K, V projections\n",
    "        flops += 3 * 2 * arch.n_embd * arch.n_embd\n",
    "        \n",
    "        # Attention computation (simplified)\n",
    "        flops += 2 * arch.n_head * seq_len * arch.head_dim\n",
    "        \n",
    "        # Attention output projection\n",
    "        flops += 2 * arch.n_embd * arch.n_embd\n",
    "        \n",
    "        # FFN: gate, up, down projections\n",
    "        flops += 2 * arch.n_embd * arch.n_ff  # gate\n",
    "        flops += 2 * arch.n_embd * arch.n_ff  # up\n",
    "        flops += 2 * arch.n_ff * arch.n_embd  # down\n",
    "    \n",
    "    # Output projection\n",
    "    flops += 2 * arch.n_embd * arch.n_vocab\n",
    "    \n",
    "    return flops\n",
    "\n",
    "# Calculate for different sequence lengths\n",
    "print(\"FLOPs per Token Estimation:\")\n",
    "print(f\"{'Seq Length':<12} {'FLOPs':<20} {'GFLOPs':<12}\")\n",
    "print(\"─\" * 45)\n",
    "\n",
    "for seq_len in [1, 100, 1000, 2048, 4096]:\n",
    "    flops = estimate_flops_per_token(arch, seq_len)\n",
    "    gflops = flops / 1e9\n",
    "    print(f\"{seq_len:<12,} {flops:<20,} {gflops:<12.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "1. Load multiple models and compare their architectures\n",
    "2. Calculate the theoretical maximum tokens/second for your hardware\n",
    "3. Estimate the cost of training this model (hint: FLOPs × tokens × 3)\n",
    "4. Design a custom architecture optimized for your specific use case\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lab, you:\n",
    "- ✅ Inspected model architecture from GGUF files\n",
    "- ✅ Calculated parameter counts and memory requirements\n",
    "- ✅ Analyzed attention configurations (MHA/GQA/MQA)\n",
    "- ✅ Estimated computational requirements (FLOPs)\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [Lab 2: Tokenization Deep Dive](lab-02-tokenization-deep-dive.ipynb)\n",
    "- [Documentation: Model Architecture](../docs/01-model-architecture-deep-dive.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
