# .github/workflows/llama-ci.yml
# Comprehensive CI/CD workflow for llama.cpp inference service

name: LLaMA Inference CI/CD

on:
  push:
    branches: [ main, develop ]
    tags: [ 'v*' ]
  pull_request:
    branches: [ main ]
  schedule:
    - cron: '0 0 * * 0'  # Weekly build

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}/llama-inference

jobs:
  # Job 1: Build and Test
  build-and-test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        backend: [cpu, cuda]
        build_type: [Release, Debug]

    steps:
    - name: Checkout code
      uses: actions/checkout@v3
      with:
        submodules: recursive

    - name: Set up environment
      run: |
        sudo apt-get update
        sudo apt-get install -y cmake build-essential

    - name: Set up CUDA (if needed)
      if: matrix.backend == 'cuda'
      uses: Jimver/cuda-toolkit@v0.2.11
      with:
        cuda: '12.2.0'

    - name: Cache build artifacts
      uses: actions/cache@v3
      with:
        path: |
          build
          ~/.cache/ccache
        key: ${{ runner.os }}-${{ matrix.backend }}-${{ matrix.build_type }}-${{ hashFiles('**/CMakeLists.txt') }}

    - name: Configure CMake
      run: |
        mkdir -p build
        cd build
        cmake .. \
          -DCMAKE_BUILD_TYPE=${{ matrix.build_type }} \
          -DLLAMA_CUDA=${{ matrix.backend == 'cuda' && 'ON' || 'OFF' }} \
          -DLLAMA_BUILD_TESTS=ON \
          -DLLAMA_BUILD_SERVER=ON

    - name: Build
      run: |
        cd build
        cmake --build . --config ${{ matrix.build_type }} -j$(nproc)

    - name: Run unit tests
      run: |
        cd build
        ctest --output-on-failure --verbose

    - name: Upload build artifacts
      uses: actions/upload-artifact@v3
      with:
        name: llama-${{ matrix.backend }}-${{ matrix.build_type }}
        path: |
          build/bin/llama-server
          build/bin/llama-cli
        retention-days: 7

  # Job 2: Integration Tests
  integration-test:
    runs-on: ubuntu-latest
    needs: build-and-test

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Download build artifacts
      uses: actions/download-artifact@v3
      with:
        name: llama-cpu-Release

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install Python dependencies
      run: |
        pip install pytest requests aiohttp pytest-asyncio

    - name: Download test model
      run: |
        mkdir -p models
        wget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf \
          -O models/test-model.gguf

    - name: Run integration tests
      run: |
        chmod +x llama-server
        pytest tests/integration/ -v --tb=short

  # Job 3: Performance Benchmarks
  benchmark:
    runs-on: ubuntu-latest
    needs: build-and-test

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Download build artifacts
      uses: actions/download-artifact@v3
      with:
        name: llama-cpu-Release

    - name: Download benchmark model
      run: |
        mkdir -p models
        wget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf \
          -O models/test-model.gguf

    - name: Run benchmarks
      run: |
        chmod +x llama-bench
        ./llama-bench -m models/test-model.gguf -p 512 -n 128 -r 3 | tee benchmark-results.txt

    - name: Check performance regression
      run: |
        python scripts/check_performance.py \
          --current benchmark-results.txt \
          --baseline benchmarks/baseline.txt \
          --threshold 0.05

    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results
        path: benchmark-results.txt

  # Job 4: Security Scanning
  security-scan:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'
        severity: 'CRITICAL,HIGH'

    - name: Upload Trivy results to GitHub Security
      uses: github/codeql-action/upload-sarif@v2
      if: always()
      with:
        sarif_file: 'trivy-results.sarif'

    - name: Run Bandit SAST
      run: |
        pip install bandit
        bandit -r . -f json -o bandit-report.json || true

    - name: Check for secrets
      uses: trufflesecurity/trufflehog@main
      with:
        path: ./
        base: ${{ github.event.repository.default_branch }}
        head: HEAD

  # Job 5: Build Docker Image
  build-docker:
    runs-on: ubuntu-latest
    needs: [integration-test, benchmark]
    if: github.event_name == 'push' || github.event_name == 'release'

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v2

    - name: Log in to Container Registry
      uses: docker/login-action@v2
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v4
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=semver,pattern={{version}}
          type=semver,pattern={{major}}.{{minor}}
          type=sha

    - name: Build and push Docker image
      uses: docker/build-push-action@v4
      with:
        context: .
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=registry,ref=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:buildcache
        cache-to: type=registry,ref=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:buildcache,mode=max
        build-args: |
          BUILD_TYPE=Release
          CUDA_VERSION=12.2

  # Job 6: Deploy to Staging
  deploy-staging:
    runs-on: ubuntu-latest
    needs: build-docker
    if: github.ref == 'refs/heads/main'
    environment:
      name: staging
      url: https://staging.inference.example.com

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Configure kubectl
      uses: azure/k8s-set-context@v3
      with:
        method: kubeconfig
        kubeconfig: ${{ secrets.KUBE_CONFIG_STAGING }}

    - name: Deploy to staging
      run: |
        kubectl set image deployment/llama-inference \
          llama-server=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }} \
          -n staging

        kubectl rollout status deployment/llama-inference -n staging --timeout=5m

    - name: Run smoke tests
      run: |
        pip install requests
        python tests/smoke/test_endpoints.py \
          --url https://staging.inference.example.com \
          --timeout 300

  # Job 7: Deploy to Production
  deploy-production:
    runs-on: ubuntu-latest
    needs: deploy-staging
    if: startsWith(github.ref, 'refs/tags/v')
    environment:
      name: production
      url: https://inference.example.com

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Configure kubectl
      uses: azure/k8s-set-context@v3
      with:
        method: kubeconfig
        kubeconfig: ${{ secrets.KUBE_CONFIG_PRODUCTION }}

    - name: Deploy canary (10%)
      run: |
        kubectl set image deployment/llama-inference-canary \
          llama-server=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }} \
          -n production

        kubectl rollout status deployment/llama-inference-canary -n production

    - name: Monitor canary metrics
      run: |
        sleep 300  # Wait 5 minutes
        python scripts/check_canary_metrics.py \
          --duration 300 \
          --error-threshold 0.01 \
          --latency-threshold 2.0

    - name: Promote to full production
      if: success()
      run: |
        kubectl set image deployment/llama-inference \
          llama-server=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }} \
          -n production

        kubectl rollout status deployment/llama-inference -n production

    - name: Rollback on failure
      if: failure()
      run: |
        kubectl rollout undo deployment/llama-inference-canary -n production
        echo "Canary deployment failed, rolled back"
        exit 1

    - name: Create GitHub Release
      if: success()
      uses: actions/create-release@v1
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      with:
        tag_name: ${{ github.ref }}
        release_name: Release ${{ github.ref }}
        body: |
          Production deployment successful
          Docker image: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}
        draft: false
        prerelease: false

  # Job 8: Notify
  notify:
    runs-on: ubuntu-latest
    needs: [build-and-test, integration-test, security-scan]
    if: always()

    steps:
    - name: Notify Slack
      uses: slackapi/slack-github-action@v1
      with:
        payload: |
          {
            "text": "CI/CD Pipeline ${{ job.status }}",
            "blocks": [
              {
                "type": "section",
                "text": {
                  "type": "mrkdwn",
                  "text": "*${{ github.workflow }}* pipeline ${{ job.status }}\n*Branch:* ${{ github.ref }}\n*Commit:* ${{ github.sha }}"
                }
              }
            ]
          }
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
