{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your First 10 Minutes with llama.cpp\n",
    "\n",
    "**Welcome!** You're about to run your first large language model locally on your machine. This is exciting!\n",
    "\n",
    "**Time to complete:** 10 minutes  \n",
    "**Prerequisites:** Basic command line familiarity  \n",
    "**What you'll learn:**\n",
    "- How to build llama.cpp\n",
    "- Download your first model\n",
    "- Generate text with a language model\n",
    "- Troubleshoot common issues\n",
    "\n",
    "**Confidence boost:** By the end of this tutorial, you'll have a working local LLM generating text. Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Verify Your Environment\n",
    "\n",
    "First, let's make sure we're in the right place and have the necessary tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if we're in the llama.cpp directory\n",
    "llama_cpp_root = Path.cwd()\n",
    "while llama_cpp_root.name != \"llama.cpp-learn\" and llama_cpp_root != llama_cpp_root.parent:\n",
    "    llama_cpp_root = llama_cpp_root.parent\n",
    "\n",
    "if llama_cpp_root.name != \"llama.cpp-learn\":\n",
    "    print(\"âš ï¸  Please run this notebook from within the llama.cpp-learn directory\")\n",
    "    llama_cpp_root = Path(\"/home/user/llama.cpp-learn\")\n",
    "\n",
    "print(f\"âœ… Working directory: {llama_cpp_root}\")\n",
    "print(f\"âœ… Python version: {sys.version}\")\n",
    "\n",
    "# Check for required tools\n",
    "try:\n",
    "    result = subprocess.run(['cmake', '--version'], capture_output=True, text=True)\n",
    "    print(f\"âœ… CMake found: {result.stdout.split()[2]}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ CMake not found. Please install CMake to continue.\")\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(['make', '--version'], capture_output=True, text=True)\n",
    "    print(f\"âœ… Make found\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ Make not found. Please install build tools to continue.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build llama.cpp\n",
    "\n",
    "Now let's compile llama.cpp. This creates the executable programs we'll use to run models.\n",
    "\n",
    "**What's happening?** CMake configures the build for your system, and make compiles the C++ code into executables.\n",
    "\n",
    "**Time estimate:** 2-3 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(llama_cpp_root)\n",
    "\n",
    "# Create build directory\n",
    "build_dir = llama_cpp_root / \"build\"\n",
    "build_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"ðŸ”¨ Configuring build with CMake...\")\n",
    "result = subprocess.run(\n",
    "    ['cmake', '..', '-DCMAKE_BUILD_TYPE=Release'],\n",
    "    cwd=build_dir,\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"âœ… CMake configuration successful\")\n",
    "else:\n",
    "    print(\"âŒ CMake configuration failed:\")\n",
    "    print(result.stderr)\n",
    "    raise Exception(\"Build configuration failed\")\n",
    "\n",
    "print(\"\\nðŸ”¨ Compiling llama.cpp... (this may take 2-3 minutes)\")\n",
    "result = subprocess.run(\n",
    "    ['cmake', '--build', '.', '--config', 'Release', '--parallel'],\n",
    "    cwd=build_dir,\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"âœ… Build successful!\")\n",
    "    \n",
    "    # Check for the main executable\n",
    "    main_executable = build_dir / \"bin\" / \"llama-cli\"\n",
    "    if not main_executable.exists():\n",
    "        main_executable = build_dir / \"bin\" / \"main\"  # Older versions\n",
    "    \n",
    "    if main_executable.exists():\n",
    "        print(f\"âœ… Main executable found: {main_executable}\")\n",
    "    else:\n",
    "        print(\"âš ï¸  Executable not in expected location. Checking build/bin/...\")\n",
    "        for exe in (build_dir / \"bin\").glob(\"*\"):\n",
    "            print(f\"   Found: {exe.name}\")\n",
    "else:\n",
    "    print(\"âŒ Build failed:\")\n",
    "    print(result.stderr[-1000:])  # Last 1000 chars of error\n",
    "    raise Exception(\"Build failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Download Your First Model\n",
    "\n",
    "Let's download a small, efficient model to get started. We'll use **TinyLlama-1.1B** - it's small (around 600MB) but surprisingly capable!\n",
    "\n",
    "**Why TinyLlama?**\n",
    "- Small size: Quick to download\n",
    "- Fast inference: Runs on any hardware\n",
    "- Good quality: Surprisingly coherent for its size\n",
    "- Perfect for learning!\n",
    "\n",
    "**File format:** We're downloading the GGUF format (Q4_K_M quantization), which balances quality and size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Create models directory\n",
    "models_dir = llama_cpp_root / \"models\"\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Model details\n",
    "model_name = \"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n",
    "model_path = models_dir / model_name\n",
    "model_url = f\"https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/{model_name}\"\n",
    "\n",
    "if model_path.exists():\n",
    "    print(f\"âœ… Model already downloaded: {model_path}\")\n",
    "    print(f\"   Size: {model_path.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "else:\n",
    "    print(f\"ðŸ“¥ Downloading {model_name}...\")\n",
    "    print(f\"   From: {model_url}\")\n",
    "    print(f\"   To: {model_path}\")\n",
    "    print(\"   This will take 2-5 minutes depending on your connection...\")\n",
    "    \n",
    "    class DownloadProgressBar(tqdm):\n",
    "        def update_to(self, b=1, bsize=1, tsize=None):\n",
    "            if tsize is not None:\n",
    "                self.total = tsize\n",
    "            self.update(b * bsize - self.n)\n",
    "    \n",
    "    try:\n",
    "        with DownloadProgressBar(unit='B', unit_scale=True, miniters=1, desc=model_name) as t:\n",
    "            urllib.request.urlretrieve(model_url, model_path, reporthook=t.update_to)\n",
    "        print(f\"\\nâœ… Download complete! Size: {model_path.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Download failed: {e}\")\n",
    "        print(\"\\nðŸ’¡ Troubleshooting:\")\n",
    "        print(\"   - Check your internet connection\")\n",
    "        print(\"   - Try downloading manually from HuggingFace\")\n",
    "        print(f\"   - URL: {model_url}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Your First Text Generation!\n",
    "\n",
    "The moment you've been waiting for - let's generate some text!\n",
    "\n",
    "We'll use the `llama-cli` program with some basic parameters:\n",
    "- `-m`: Model file path\n",
    "- `-p`: Prompt (what you want the model to write about)\n",
    "- `-n`: Number of tokens to generate\n",
    "- `--temp`: Temperature (creativity level, 0.7 is balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the llama-cli executable\n",
    "llama_cli = build_dir / \"bin\" / \"llama-cli\"\n",
    "if not llama_cli.exists():\n",
    "    llama_cli = build_dir / \"bin\" / \"main\"  # Older versions\n",
    "\n",
    "if not llama_cli.exists():\n",
    "    print(\"âŒ Could not find llama-cli executable\")\n",
    "    print(f\"   Searched in: {build_dir / 'bin'}\")\n",
    "    print(\"   Available files:\")\n",
    "    for f in (build_dir / \"bin\").glob(\"*\"):\n",
    "        print(f\"     - {f.name}\")\n",
    "else:\n",
    "    print(f\"âœ… Found executable: {llama_cli}\")\n",
    "    \n",
    "    # Simple prompt\n",
    "    prompt = \"Once upon a time, in a land of machine learning,\"\n",
    "    \n",
    "    print(f\"\\nðŸ¤– Generating text...\\n\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Run inference\n",
    "    result = subprocess.run(\n",
    "        [\n",
    "            str(llama_cli),\n",
    "            '-m', str(model_path),\n",
    "            '-p', prompt,\n",
    "            '-n', '100',  # Generate 100 tokens\n",
    "            '--temp', '0.7',\n",
    "            '-ngl', '0'  # No GPU layers for compatibility\n",
    "        ],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        cwd=llama_cpp_root\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(result.stdout)\n",
    "        print(\"=\" * 70)\n",
    "        print(\"\\nðŸŽ‰ SUCCESS! You just ran your first local LLM!\")\n",
    "    else:\n",
    "        print(\"âŒ Generation failed:\")\n",
    "        print(result.stderr)\n",
    "        print(\"\\nSee troubleshooting section below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Try It Yourself!\n",
    "\n",
    "Now it's your turn! Modify the prompt below and run your own generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Change this prompt to anything you like!\n",
    "your_prompt = \"Explain what a large language model is in simple terms:\"\n",
    "\n",
    "print(f\"ðŸ¤– Your prompt: {your_prompt}\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\n",
    "        str(llama_cli),\n",
    "        '-m', str(model_path),\n",
    "        '-p', your_prompt,\n",
    "        '-n', '150',\n",
    "        '--temp', '0.7',\n",
    "        '-ngl', '0'\n",
    "    ],\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    cwd=llama_cpp_root\n",
    ")\n",
    "\n",
    "print(result.stdout)\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding What Just Happened\n",
    "\n",
    "Congratulations! Let's break down what you just accomplished:\n",
    "\n",
    "### 1. **You built llama.cpp from source**\n",
    "   - Compiled C++ code optimized for your CPU\n",
    "   - Created executables for running LLMs efficiently\n",
    "\n",
    "### 2. **You downloaded a GGUF model**\n",
    "   - GGUF = GPT-Generated Unified Format\n",
    "   - Q4_K_M = 4-bit quantization (compressed for efficiency)\n",
    "   - Went from ~4GB (original) to ~600MB (quantized)\n",
    "\n",
    "### 3. **You ran inference**\n",
    "   - Loaded the model into memory\n",
    "   - Fed it a prompt\n",
    "   - Generated new text token by token\n",
    "   - All running locally on your machine!\n",
    "\n",
    "### 4. **Key concepts you used:**\n",
    "   - **Prompt:** Your input text\n",
    "   - **Tokens:** Pieces of words the model processes\n",
    "   - **Temperature:** Controls randomness (0 = deterministic, 1 = creative)\n",
    "   - **Max tokens:** How much text to generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Issues & Solutions\n",
    "\n",
    "### Problem: Build fails\n",
    "**Solution:**\n",
    "- Make sure you have CMake 3.14+ and a C++ compiler\n",
    "- On Ubuntu/Debian: `sudo apt install build-essential cmake`\n",
    "- On macOS: `brew install cmake`\n",
    "- On Windows: Install Visual Studio with C++ tools\n",
    "\n",
    "### Problem: Download fails\n",
    "**Solution:**\n",
    "- Check internet connection\n",
    "- Download manually from [HuggingFace](https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF)\n",
    "- Place in `models/` directory\n",
    "\n",
    "### Problem: Generation is slow\n",
    "**Solution:**\n",
    "- This is normal on CPU! TinyLlama should do 5-20 tokens/second\n",
    "- For faster inference, see Tutorial 3 on GPU acceleration\n",
    "- Smaller context = faster generation\n",
    "\n",
    "### Problem: Nonsensical output\n",
    "**Solution:**\n",
    "- Try adjusting temperature (0.5-0.9 is usually good)\n",
    "- Make prompts more specific\n",
    "- TinyLlama is small - try a larger model for better quality\n",
    "\n",
    "### Problem: Out of memory\n",
    "**Solution:**\n",
    "- TinyLlama needs ~1GB RAM\n",
    "- Close other applications\n",
    "- Try an even smaller model (Q2_K quantization)\n",
    "- Reduce context size with `-c` parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Reference: Command Parameters\n",
    "\n",
    "Here are the most useful parameters for `llama-cli`:\n",
    "\n",
    "| Parameter | Description | Example |\n",
    "|-----------|-------------|----------|\n",
    "| `-m` | Model file path | `-m models/model.gguf` |\n",
    "| `-p` | Prompt text | `-p \"Hello world\"` |\n",
    "| `-n` | Max tokens to generate | `-n 100` |\n",
    "| `--temp` | Temperature (0-2) | `--temp 0.7` |\n",
    "| `-c` | Context size | `-c 2048` |\n",
    "| `-ngl` | GPU layers | `-ngl 32` |\n",
    "| `--top-p` | Nucleus sampling | `--top-p 0.9` |\n",
    "| `--top-k` | Top-k sampling | `--top-k 40` |\n",
    "| `--repeat-penalty` | Repetition penalty | `--repeat-penalty 1.1` |\n",
    "\n",
    "Run `./build/bin/llama-cli --help` for the complete list!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Time! \n",
    "\n",
    "Now that you have the basics, try these experiments:\n",
    "\n",
    "### Experiment 1: Temperature\n",
    "Generate the same prompt with different temperatures and compare:\n",
    "- Temperature 0.1 (very focused)\n",
    "- Temperature 0.7 (balanced)\n",
    "- Temperature 1.5 (very creative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The three laws of robotics are:\"\n",
    "temperatures = [0.1, 0.7, 1.5]\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Temperature: {temp}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    result = subprocess.run(\n",
    "        [str(llama_cli), '-m', str(model_path), '-p', prompt, \n",
    "         '-n', '80', '--temp', str(temp), '-ngl', '0'],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    \n",
    "    # Extract just the generated text (skip metadata)\n",
    "    output_lines = result.stdout.split('\\n')\n",
    "    for line in output_lines:\n",
    "        if not line.startswith('llama') and not line.startswith('log_') and line.strip():\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Length\n",
    "Try generating different lengths (tokens) and observe how the model maintains coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Here's a recipe for chocolate chip cookies:\"\n",
    "lengths = [50, 150, 300]\n",
    "\n",
    "for length in lengths:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Generating {length} tokens\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    result = subprocess.run(\n",
    "        [str(llama_cli), '-m', str(model_path), '-p', prompt,\n",
    "         '-n', str(length), '--temp', '0.7', '-ngl', '0'],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    \n",
    "    print(result.stdout[:500])  # Show first 500 chars\n",
    "    print(\"...\" if len(result.stdout) > 500 else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What You've Learned\n",
    "\n",
    "In just 10 minutes, you've:\n",
    "\n",
    "âœ… Built llama.cpp from source  \n",
    "âœ… Downloaded and understood GGUF format  \n",
    "âœ… Generated your first text with a local LLM  \n",
    "âœ… Experimented with parameters  \n",
    "âœ… Learned troubleshooting basics  \n",
    "\n",
    "**You now have:**\n",
    "- A working llama.cpp installation\n",
    "- Your first model ready to use\n",
    "- Understanding of basic inference parameters\n",
    "- Confidence to experiment further!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "### Continue Your Learning Journey:\n",
    "\n",
    "1. **Tutorial 2: Understanding GGUF** (20-30 minutes)\n",
    "   - Deep dive into model formats\n",
    "   - Read model metadata programmatically\n",
    "   - Compare different quantizations\n",
    "   - Choose the right model for your needs\n",
    "\n",
    "2. **Tutorial 3: Inference Parameters** (30-40 minutes)\n",
    "   - Master temperature, top-p, top-k\n",
    "   - Understand sampling strategies\n",
    "   - Optimize for quality vs speed\n",
    "   - Best practices for different tasks\n",
    "\n",
    "### Try Different Models:\n",
    "- **Llama 3.2 1B/3B** - Better quality, still fast\n",
    "- **Phi-3** - Excellent reasoning for size\n",
    "- **Qwen 2.5** - Multilingual capabilities\n",
    "\n",
    "### Explore Advanced Topics:\n",
    "- GPU acceleration (Module 3)\n",
    "- Custom model fine-tuning (Module 5)\n",
    "- Production deployment (Module 6)\n",
    "\n",
    "### Join the Community:\n",
    "- [llama.cpp GitHub](https://github.com/ggerganov/llama.cpp)\n",
    "- [Discord](https://discord.gg/llama-cpp)\n",
    "- [HuggingFace Models](https://huggingface.co/models?library=gguf)\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've taken your first steps into the world of local LLMs. The journey ahead is exciting - you can now:\n",
    "- Run AI completely privately on your machine\n",
    "- Experiment with cutting-edge models\n",
    "- Build your own AI-powered applications\n",
    "- Learn how these fascinating systems work\n",
    "\n",
    "**Keep experimenting, stay curious, and have fun! ** ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
