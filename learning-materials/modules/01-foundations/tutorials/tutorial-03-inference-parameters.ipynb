{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mastering Inference Parameters: A Deep Dive\n",
    "\n",
    "**Welcome to the advanced tutorial!** Now that you can run models and understand GGUF format, let's master the art of controlling text generation.\n",
    "\n",
    "**Time to complete:** 30-40 minutes  \n",
    "**Prerequisites:** Tutorials 1 & 2 completed  \n",
    "**What you'll learn:**\n",
    "- How sampling parameters control generation\n",
    "- Temperature, top-p, top-k, and their effects\n",
    "- Repetition penalties and frequency control\n",
    "- Context management and memory\n",
    "- Performance optimization techniques\n",
    "- Best practices for different use cases\n",
    "\n",
    "**Goal:** Master the parameters to get exactly the output you want!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Text Generation\n",
    "\n",
    "Before diving into parameters, let's understand how language models generate text:\n",
    "\n",
    "### The Generation Process:\n",
    "\n",
    "1. **Input:** You provide a prompt\n",
    "2. **Encoding:** Prompt is converted to tokens\n",
    "3. **Processing:** Model computes probabilities for next token\n",
    "4. **Sampling:** A token is selected based on probabilities\n",
    "5. **Output:** Token is decoded to text\n",
    "6. **Repeat:** New token added to context, process continues\n",
    "\n",
    "### Why Parameters Matter:\n",
    "\n",
    "Parameters control **step 4 (Sampling)** - how we choose the next token from the probability distribution.\n",
    "\n",
    "Different sampling strategies produce very different results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Setup paths\n",
    "llama_cpp_root = Path(\"/home/user/llama.cpp-learn\")\n",
    "model_path = llama_cpp_root / \"models\" / \"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n",
    "build_dir = llama_cpp_root / \"build\"\n",
    "llama_cli = build_dir / \"bin\" / \"llama-cli\"\n",
    "\n",
    "if not llama_cli.exists():\n",
    "    llama_cli = build_dir / \"bin\" / \"main\"\n",
    "\n",
    "# Verify setup\n",
    "print(\"üîß Environment Check:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model: {'‚úÖ' if model_path.exists() else '‚ùå'} {model_path.name}\")\n",
    "print(f\"Executable: {'‚úÖ' if llama_cli.exists() else '‚ùå'} {llama_cli.name}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if not model_path.exists() or not llama_cli.exists():\n",
    "    print(\"\\n‚ö†Ô∏è  Please complete Tutorial 1 first!\")\n",
    "    raise FileNotFoundError(\"Required files not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Let's create utilities to make experimentation easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt: str, **kwargs) -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Generate text with specified parameters.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input text\n",
    "        **kwargs: Generation parameters (temp, top_p, top_k, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with generated text, timing, and parameters\n",
    "    \"\"\"\n",
    "    # Default parameters\n",
    "    params = {\n",
    "        'n_predict': 100,\n",
    "        'temp': 0.7,\n",
    "        'top_p': 0.9,\n",
    "        'top_k': 40,\n",
    "        'repeat_penalty': 1.1,\n",
    "        'n_ctx': 2048,\n",
    "        'n_gpu_layers': 0,\n",
    "    }\n",
    "    params.update(kwargs)\n",
    "    \n",
    "    # Build command\n",
    "    cmd = [\n",
    "        str(llama_cli),\n",
    "        '-m', str(model_path),\n",
    "        '-p', prompt,\n",
    "        '-n', str(params['n_predict']),\n",
    "        '--temp', str(params['temp']),\n",
    "        '--top-p', str(params['top_p']),\n",
    "        '--top-k', str(params['top_k']),\n",
    "        '--repeat-penalty', str(params['repeat_penalty']),\n",
    "        '-c', str(params['n_ctx']),\n",
    "        '-ngl', str(params['n_gpu_layers']),\n",
    "        '--no-display-prompt',  # Don't repeat prompt in output\n",
    "    ]\n",
    "    \n",
    "    # Run generation\n",
    "    start_time = time.time()\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, cwd=llama_cpp_root)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # Extract generated text (remove metadata)\n",
    "    output = result.stdout\n",
    "    \n",
    "    # Parse tokens/sec if available\n",
    "    tokens_per_sec = None\n",
    "    if 'eval time' in result.stderr:\n",
    "        match = re.search(r'(\\d+\\.\\d+) tokens per second', result.stderr)\n",
    "        if match:\n",
    "            tokens_per_sec = float(match.group(1))\n",
    "    \n",
    "    return {\n",
    "        'text': output.strip(),\n",
    "        'elapsed_time': elapsed,\n",
    "        'tokens_per_sec': tokens_per_sec,\n",
    "        'parameters': params,\n",
    "        'prompt': prompt,\n",
    "    }\n",
    "\n",
    "def compare_generations(prompt: str, param_sets: List[Dict], param_name: str):\n",
    "    \"\"\"\n",
    "    Generate with multiple parameter sets and display comparison.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input prompt\n",
    "        param_sets: List of parameter dictionaries\n",
    "        param_name: Name of parameter being varied (for display)\n",
    "    \"\"\"\n",
    "    print(f\"\\nüî¨ Comparing {param_name}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    results = []\n",
    "    for i, params in enumerate(param_sets, 1):\n",
    "        print(f\"\\n[{i}/{len(param_sets)}] Generating with {param_name}={params.get(param_name.lower().replace(' ', '_').replace('-', '_'), 'default')}...\")\n",
    "        result = generate_text(prompt, **params)\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"\\n{param_name}: {params.get(param_name.lower().replace(' ', '_').replace('-', '_'), 'default')}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(result['text'][:500])  # First 500 chars\n",
    "        if len(result['text']) > 500:\n",
    "            print(\"...\")\n",
    "        print(\"-\" * 80)\n",
    "        if result['tokens_per_sec']:\n",
    "            print(f\"Speed: {result['tokens_per_sec']:.1f} tokens/sec\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Helper functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter 1: Temperature\n",
    "\n",
    "**Temperature** controls randomness in token selection.\n",
    "\n",
    "### How it works:\n",
    "- Adjusts the probability distribution before sampling\n",
    "- Lower temp ‚Üí sharper distribution (more focused)\n",
    "- Higher temp ‚Üí flatter distribution (more random)\n",
    "\n",
    "### Values:\n",
    "- **0.0-0.3:** Very focused, deterministic, factual\n",
    "- **0.5-0.7:** Balanced, coherent, some creativity\n",
    "- **0.8-1.0:** Creative, diverse, less predictable\n",
    "- **1.2-2.0:** Very creative, possibly incoherent\n",
    "\n",
    "### Best for:\n",
    "- **Low (0.1-0.3):** Technical writing, Q&A, code generation\n",
    "- **Medium (0.6-0.8):** General chat, stories, content creation\n",
    "- **High (0.9-1.5):** Creative writing, brainstorming, variety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different temperatures\n",
    "prompt = \"Write a haiku about artificial intelligence:\"\n",
    "\n",
    "temps = [\n",
    "    {'temp': 0.1, 'n_predict': 50},\n",
    "    {'temp': 0.7, 'n_predict': 50},\n",
    "    {'temp': 1.5, 'n_predict': 50},\n",
    "]\n",
    "\n",
    "temp_results = compare_generations(prompt, temps, \"Temperature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature Analysis\n",
    "\n",
    "Notice the differences:\n",
    "- **Low temp (0.1):** Consistent, predictable output\n",
    "- **Medium temp (0.7):** Balanced creativity and coherence\n",
    "- **High temp (1.5):** More varied, possibly unexpected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter 2: Top-P (Nucleus Sampling)\n",
    "\n",
    "**Top-P** selects from the smallest set of tokens whose cumulative probability exceeds P.\n",
    "\n",
    "### How it works:\n",
    "1. Sort tokens by probability\n",
    "2. Add tokens until cumulative probability > P\n",
    "3. Sample only from this set\n",
    "\n",
    "### Values:\n",
    "- **0.1-0.5:** Very focused, limited vocabulary\n",
    "- **0.9:** Recommended default (balanced)\n",
    "- **0.95-1.0:** More diverse, includes rare tokens\n",
    "\n",
    "### Best for:\n",
    "- **Low (0.5-0.7):** Factual content, consistency\n",
    "- **Medium (0.9):** General purpose (recommended)\n",
    "- **High (0.95-1.0):** Creative writing, variety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different top-p values\n",
    "prompt = \"The future of technology will be\"\n",
    "\n",
    "top_p_values = [\n",
    "    {'top_p': 0.5, 'temp': 0.7, 'n_predict': 80},\n",
    "    {'top_p': 0.9, 'temp': 0.7, 'n_predict': 80},\n",
    "    {'top_p': 1.0, 'temp': 0.7, 'n_predict': 80},\n",
    "]\n",
    "\n",
    "top_p_results = compare_generations(prompt, top_p_values, \"Top-P\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter 3: Top-K\n",
    "\n",
    "**Top-K** limits sampling to the K most likely tokens.\n",
    "\n",
    "### How it works:\n",
    "1. Sort tokens by probability\n",
    "2. Keep only top K tokens\n",
    "3. Renormalize and sample\n",
    "\n",
    "### Values:\n",
    "- **10-20:** Very focused\n",
    "- **40:** Recommended default\n",
    "- **100+:** More diverse\n",
    "\n",
    "### Top-K vs Top-P:\n",
    "- Top-K is fixed: always K tokens\n",
    "- Top-P is dynamic: varies with probability distribution\n",
    "- Use together for best control!\n",
    "\n",
    "### Best for:\n",
    "- **Low (10-30):** Factual, consistent\n",
    "- **Medium (40-60):** Balanced\n",
    "- **High (80-100):** Creative, varied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different top-k values\n",
    "prompt = \"List three benefits of machine learning:\"\n",
    "\n",
    "top_k_values = [\n",
    "    {'top_k': 10, 'temp': 0.7, 'n_predict': 100},\n",
    "    {'top_k': 40, 'temp': 0.7, 'n_predict': 100},\n",
    "    {'top_k': 100, 'temp': 0.7, 'n_predict': 100},\n",
    "]\n",
    "\n",
    "top_k_results = compare_generations(prompt, top_k_values, \"Top-K\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter 4: Repeat Penalty\n",
    "\n",
    "**Repeat Penalty** discourages the model from repeating tokens.\n",
    "\n",
    "### How it works:\n",
    "- Reduces probability of tokens already used\n",
    "- Helps avoid repetitive text\n",
    "- Applied to recent context window\n",
    "\n",
    "### Values:\n",
    "- **1.0:** No penalty (may repeat)\n",
    "- **1.1:** Slight penalty (recommended)\n",
    "- **1.2-1.5:** Strong penalty (very diverse)\n",
    "- **1.5+:** May become incoherent\n",
    "\n",
    "### Best for:\n",
    "- **1.0-1.05:** Poetry, code (where repetition is ok)\n",
    "- **1.1:** General purpose (recommended)\n",
    "- **1.2-1.5:** When model gets stuck repeating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different repeat penalties\n",
    "# Use a prompt that might trigger repetition\n",
    "prompt = \"The most important thing to remember is\"\n",
    "\n",
    "repeat_penalties = [\n",
    "    {'repeat_penalty': 1.0, 'temp': 0.7, 'n_predict': 100},\n",
    "    {'repeat_penalty': 1.1, 'temp': 0.7, 'n_predict': 100},\n",
    "    {'repeat_penalty': 1.3, 'temp': 0.7, 'n_predict': 100},\n",
    "]\n",
    "\n",
    "penalty_results = compare_generations(prompt, repeat_penalties, \"Repeat Penalty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Parameter Explorer\n",
    "\n",
    "Now let's create an interactive tool to experiment with combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_generation():\n",
    "    \"\"\"\n",
    "    Interactive parameter tuning interface\n",
    "    \"\"\"\n",
    "    print(\"\\nüéÆ Interactive Parameter Explorer\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Get user input\n",
    "    prompt = input(\"Enter your prompt: \")\n",
    "    if not prompt:\n",
    "        prompt = \"Once upon a time in a world of AI\"\n",
    "    \n",
    "    print(\"\\nConfigure parameters (press Enter for defaults):\")\n",
    "    \n",
    "    try:\n",
    "        temp = input(\"  Temperature [0.7]: \")\n",
    "        temp = float(temp) if temp else 0.7\n",
    "        \n",
    "        top_p = input(\"  Top-P [0.9]: \")\n",
    "        top_p = float(top_p) if top_p else 0.9\n",
    "        \n",
    "        top_k = input(\"  Top-K [40]: \")\n",
    "        top_k = int(top_k) if top_k else 40\n",
    "        \n",
    "        repeat_penalty = input(\"  Repeat Penalty [1.1]: \")\n",
    "        repeat_penalty = float(repeat_penalty) if repeat_penalty else 1.1\n",
    "        \n",
    "        n_predict = input(\"  Max Tokens [150]: \")\n",
    "        n_predict = int(n_predict) if n_predict else 150\n",
    "        \n",
    "    except ValueError:\n",
    "        print(\"\\n‚ö†Ô∏è  Invalid input, using defaults\")\n",
    "        temp, top_p, top_k, repeat_penalty, n_predict = 0.7, 0.9, 40, 1.1, 150\n",
    "    \n",
    "    # Generate\n",
    "    print(\"\\nü§ñ Generating...\\n\")\n",
    "    result = generate_text(\n",
    "        prompt,\n",
    "        temp=temp,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        repeat_penalty=repeat_penalty,\n",
    "        n_predict=n_predict\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    print(\"=\" * 70)\n",
    "    print(\"GENERATED TEXT:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(result['text'])\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nGeneration took {result['elapsed_time']:.2f} seconds\")\n",
    "    if result['tokens_per_sec']:\n",
    "        print(f\"Speed: {result['tokens_per_sec']:.1f} tokens/second\")\n",
    "    print(\"\\nParameters used:\")\n",
    "    for key, value in result['parameters'].items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Uncomment to run interactively:\n",
    "# interactive_generation()\n",
    "\n",
    "print(\"‚úÖ Interactive explorer ready!\")\n",
    "print(\"   Uncomment the line above to try it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Window Management\n",
    "\n",
    "The **context window** is the maximum number of tokens the model can process.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "**Context Size (-c):**\n",
    "- Maximum tokens in prompt + generation\n",
    "- TinyLlama default: 2048 tokens\n",
    "- Larger context = more memory needed\n",
    "- Context = ~0.75 words per token (English)\n",
    "\n",
    "**Memory Usage:**\n",
    "- Linear relationship: 2x context ‚âà 2x memory\n",
    "- Example (TinyLlama Q4_K_M):\n",
    "  - 2K context: ~1GB RAM\n",
    "  - 4K context: ~1.5GB RAM\n",
    "  - 8K context: ~2.5GB RAM\n",
    "\n",
    "**Performance:**\n",
    "- Larger context = slower generation\n",
    "- Model must attend to all tokens\n",
    "- Use minimum context needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different context sizes\n",
    "prompt = \"Explain quantum computing in simple terms:\"\n",
    "\n",
    "print(\"\\n‚è±Ô∏è  Context Size Performance Test\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "context_sizes = [512, 2048, 4096]\n",
    "timings = []\n",
    "\n",
    "for ctx_size in context_sizes:\n",
    "    print(f\"\\nTesting context size: {ctx_size}...\")\n",
    "    result = generate_text(\n",
    "        prompt,\n",
    "        n_ctx=ctx_size,\n",
    "        n_predict=50,\n",
    "        temp=0.7\n",
    "    )\n",
    "    \n",
    "    timings.append({\n",
    "        'context_size': ctx_size,\n",
    "        'time': result['elapsed_time'],\n",
    "        'tokens_per_sec': result['tokens_per_sec']\n",
    "    })\n",
    "    \n",
    "    print(f\"  Time: {result['elapsed_time']:.2f}s\")\n",
    "    if result['tokens_per_sec']:\n",
    "        print(f\"  Speed: {result['tokens_per_sec']:.1f} tokens/sec\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nüí° Observation: Larger context may be slower to load, but generation\")\n",
    "print(\"   speed stays similar. Use the minimum context you need!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Parameters\n",
    "\n",
    "### Presence Penalty & Frequency Penalty\n",
    "\n",
    "More fine-grained control over repetition:\n",
    "\n",
    "**Presence Penalty:**\n",
    "- Reduces likelihood of tokens that appeared at all\n",
    "- Binary: did it appear? Then penalize\n",
    "- Good for avoiding topic repetition\n",
    "\n",
    "**Frequency Penalty:**\n",
    "- Scales with how often token appeared\n",
    "- More appearances = stronger penalty\n",
    "- Good for avoiding phrase repetition\n",
    "\n",
    "### Mirostat Sampling\n",
    "\n",
    "An advanced sampling method that aims for consistent perplexity:\n",
    "\n",
    "**Mirostat 1:**\n",
    "- Classic algorithm\n",
    "- Good for coherent long text\n",
    "\n",
    "**Mirostat 2:**\n",
    "- Improved version\n",
    "- Better quality, more stable\n",
    "\n",
    "**When to use:**\n",
    "- Long-form generation\n",
    "- Maintaining consistent quality\n",
    "- Alternative to temp/top-p tuning\n",
    "\n",
    "### RoPE Scaling\n",
    "\n",
    "Extends context beyond model's training:\n",
    "- Model trained on 2K ‚Üí can handle 4K+\n",
    "- Quality degrades with extreme scaling\n",
    "- Use `--rope-scale` parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices by Use Case\n",
    "\n",
    "Here are recommended parameter combinations for different scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practice parameter sets\n",
    "best_practices = {\n",
    "    'Factual Q&A': {\n",
    "        'temp': 0.1,\n",
    "        'top_p': 0.9,\n",
    "        'top_k': 20,\n",
    "        'repeat_penalty': 1.05,\n",
    "        'description': 'Low temperature for consistency, focused sampling'\n",
    "    },\n",
    "    'General Chat': {\n",
    "        'temp': 0.7,\n",
    "        'top_p': 0.9,\n",
    "        'top_k': 40,\n",
    "        'repeat_penalty': 1.1,\n",
    "        'description': 'Balanced parameters, good for conversation'\n",
    "    },\n",
    "    'Creative Writing': {\n",
    "        'temp': 0.9,\n",
    "        'top_p': 0.95,\n",
    "        'top_k': 60,\n",
    "        'repeat_penalty': 1.15,\n",
    "        'description': 'Higher creativity, diverse vocabulary'\n",
    "    },\n",
    "    'Code Generation': {\n",
    "        'temp': 0.2,\n",
    "        'top_p': 0.95,\n",
    "        'top_k': 50,\n",
    "        'repeat_penalty': 1.05,\n",
    "        'description': 'Low temp for correctness, allow code patterns'\n",
    "    },\n",
    "    'Brainstorming': {\n",
    "        'temp': 1.2,\n",
    "        'top_p': 0.98,\n",
    "        'top_k': 80,\n",
    "        'repeat_penalty': 1.3,\n",
    "        'description': 'Maximum diversity, encourage new ideas'\n",
    "    },\n",
    "    'Technical Writing': {\n",
    "        'temp': 0.3,\n",
    "        'top_p': 0.9,\n",
    "        'top_k': 30,\n",
    "        'repeat_penalty': 1.1,\n",
    "        'description': 'Focused and precise, moderate repetition control'\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"\\nüìö Best Practice Parameter Sets\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'Use Case':<20} {'Temp':<8} {'Top-P':<8} {'Top-K':<8} {'Repeat':<8} {'Notes':<30}\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "for use_case, params in best_practices.items():\n",
    "    print(f\"{use_case:<20} {params['temp']:<8.1f} {params['top_p']:<8.2f} \"\n",
    "          f\"{params['top_k']:<8} {params['repeat_penalty']:<8.2f} \"\n",
    "          f\"{params['description']:<30}\")\n",
    "\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Best Practices\n",
    "\n",
    "Let's test a couple of these presets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test factual Q&A vs creative writing\n",
    "prompt = \"Describe a sunset\"\n",
    "\n",
    "print(\"\\nüé® Comparing Use Case Presets\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Prompt: '{prompt}'\\n\")\n",
    "\n",
    "# Factual approach\n",
    "print(\"[1] FACTUAL Q&A PRESET:\")\n",
    "print(\"-\" * 80)\n",
    "result1 = generate_text(\n",
    "    prompt,\n",
    "    temp=0.1,\n",
    "    top_p=0.9,\n",
    "    top_k=20,\n",
    "    repeat_penalty=1.05,\n",
    "    n_predict=100\n",
    ")\n",
    "print(result1['text'][:400])\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "\n",
    "# Creative approach\n",
    "print(\"\\n[2] CREATIVE WRITING PRESET:\")\n",
    "print(\"-\" * 80)\n",
    "result2 = generate_text(\n",
    "    prompt,\n",
    "    temp=0.9,\n",
    "    top_p=0.95,\n",
    "    top_k=60,\n",
    "    repeat_penalty=1.15,\n",
    "    n_predict=100\n",
    ")\n",
    "print(result2['text'][:400])\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "\n",
    "print(\"\\nüí° Notice how the same prompt produces very different results!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Optimization\n",
    "\n",
    "### Speed vs Quality Tradeoffs:\n",
    "\n",
    "**For Maximum Speed:**\n",
    "1. Reduce context size (`-c`)\n",
    "2. Use smaller quantization (Q4_K_M vs Q8_0)\n",
    "3. Enable GPU layers (`-ngl`)\n",
    "4. Reduce max tokens (`-n`)\n",
    "5. Use batch processing\n",
    "\n",
    "**For Maximum Quality:**\n",
    "1. Use larger quantization (Q6_K, Q8_0)\n",
    "2. Increase context if needed\n",
    "3. Fine-tune sampling parameters\n",
    "4. Use larger models (7B, 13B)\n",
    "\n",
    "**Balanced Approach:**\n",
    "- Q4_K_M or Q5_K_M quantization\n",
    "- Context size matching your needs\n",
    "- GPU acceleration if available\n",
    "- Optimized sampling parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison\n",
    "prompt = \"Explain neural networks briefly:\"\n",
    "\n",
    "print(\"\\n‚ö° Performance Optimization Test\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "configs = [\n",
    "    {'name': 'Fast (small context)', 'n_ctx': 512, 'n_predict': 50},\n",
    "    {'name': 'Balanced (normal)', 'n_ctx': 2048, 'n_predict': 50},\n",
    "    {'name': 'Quality (large context)', 'n_ctx': 4096, 'n_predict': 50},\n",
    "]\n",
    "\n",
    "perf_results = []\n",
    "\n",
    "for config in configs:\n",
    "    print(f\"\\nTesting: {config['name']}...\")\n",
    "    result = generate_text(\n",
    "        prompt,\n",
    "        n_ctx=config['n_ctx'],\n",
    "        n_predict=config['n_predict'],\n",
    "        temp=0.7\n",
    "    )\n",
    "    \n",
    "    perf_results.append({\n",
    "        'name': config['name'],\n",
    "        'context': config['n_ctx'],\n",
    "        'time': result['elapsed_time'],\n",
    "        'speed': result['tokens_per_sec']\n",
    "    })\n",
    "    \n",
    "    print(f\"  Context: {config['n_ctx']}\")\n",
    "    print(f\"  Time: {result['elapsed_time']:.2f}s\")\n",
    "    if result['tokens_per_sec']:\n",
    "        print(f\"  Speed: {result['tokens_per_sec']:.1f} tokens/sec\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Parameter Effects\n",
    "\n",
    "Let's create a visual guide to parameter effects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('LLM Inference Parameters: Effects and Tradeoffs', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Temperature effect on distribution\n",
    "ax = axes[0, 0]\n",
    "x = np.linspace(0, 10, 100)\n",
    "for temp in [0.1, 0.5, 1.0, 2.0]:\n",
    "    # Simulate probability distribution at different temps\n",
    "    y = np.exp(-((x - 5) ** 2) / (2 * temp ** 2))\n",
    "    y = y / y.sum()\n",
    "    ax.plot(x, y, label=f'T={temp}', linewidth=2)\n",
    "ax.set_xlabel('Token Space', fontsize=12)\n",
    "ax.set_ylabel('Probability', fontsize=12)\n",
    "ax.set_title('Temperature Effect on Token Distribution', fontsize=13, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.text(0.5, 0.95, 'Lower temp = sharper peak (focused)\\nHigher temp = flatter curve (diverse)',\n",
    "        transform=ax.transAxes, ha='center', va='top', fontsize=10,\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# 2. Top-P vs Top-K\n",
    "ax = axes[0, 1]\n",
    "probabilities = np.array([0.3, 0.25, 0.15, 0.1, 0.08, 0.05, 0.04, 0.02, 0.01])\n",
    "tokens = np.arange(len(probabilities))\n",
    "\n",
    "# Top-K = 5\n",
    "top_k_mask = tokens < 5\n",
    "ax.bar(tokens, probabilities, alpha=0.3, label='All tokens')\n",
    "ax.bar(tokens[top_k_mask], probabilities[top_k_mask], \n",
    "       alpha=0.8, label='Top-K=5', color='green')\n",
    "\n",
    "# Top-P = 0.9 (cumsum)\n",
    "cumsum = np.cumsum(probabilities)\n",
    "top_p_mask = cumsum <= 0.9\n",
    "if not any(top_p_mask):\n",
    "    top_p_mask[0] = True\n",
    "ax.scatter(tokens[top_p_mask], probabilities[top_p_mask], \n",
    "          s=200, marker='*', color='red', label='Top-P=0.9', zorder=10)\n",
    "\n",
    "ax.set_xlabel('Token Rank', fontsize=12)\n",
    "ax.set_ylabel('Probability', fontsize=12)\n",
    "ax.set_title('Top-K vs Top-P Sampling', fontsize=13, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Use case recommendations\n",
    "ax = axes[1, 0]\n",
    "use_cases = list(best_practices.keys())\n",
    "temps = [best_practices[uc]['temp'] for uc in use_cases]\n",
    "colors_map = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(use_cases)))\n",
    "\n",
    "bars = ax.barh(use_cases, temps, color=colors_map)\n",
    "ax.set_xlabel('Temperature', fontsize=12)\n",
    "ax.set_title('Recommended Temperature by Use Case', fontsize=13, fontweight='bold')\n",
    "ax.set_xlim([0, 1.5])\n",
    "ax.axvline(x=0.7, color='blue', linestyle='--', alpha=0.5, label='Default (0.7)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, temp) in enumerate(zip(bars, temps)):\n",
    "    ax.text(temp + 0.02, i, f'{temp:.1f}', va='center', fontsize=10)\n",
    "\n",
    "# 4. Parameter interaction heatmap\n",
    "ax = axes[1, 1]\n",
    "params = ['Temperature', 'Top-P', 'Top-K', 'Repeat\\nPenalty']\n",
    "effects = ['Creativity', 'Consistency', 'Diversity', 'Coherence']\n",
    "\n",
    "# Impact matrix (higher = stronger effect)\n",
    "impact = np.array([\n",
    "    [9, 3, 8, 6],  # Temperature\n",
    "    [6, 7, 7, 5],  # Top-P\n",
    "    [5, 6, 8, 4],  # Top-K\n",
    "    [2, 8, 5, 9],  # Repeat Penalty\n",
    "])\n",
    "\n",
    "im = ax.imshow(impact, cmap='YlOrRd', aspect='auto')\n",
    "ax.set_xticks(np.arange(len(effects)))\n",
    "ax.set_yticks(np.arange(len(params)))\n",
    "ax.set_xticklabels(effects)\n",
    "ax.set_yticklabels(params)\n",
    "ax.set_title('Parameter Impact on Output Characteristics', \n",
    "             fontsize=13, fontweight='bold')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(params)):\n",
    "    for j in range(len(effects)):\n",
    "        text = ax.text(j, i, impact[i, j],\n",
    "                      ha=\"center\", va=\"center\", color=\"black\", fontsize=12)\n",
    "\n",
    "plt.colorbar(im, ax=ax, label='Impact Level')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/tmp/inference_parameters_guide.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualization saved to /tmp/inference_parameters_guide.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Reference Card\n",
    "\n",
    "Save this for future reference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_card = \"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë              LLAMA.CPP INFERENCE PARAMETERS - QUICK REFERENCE        ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë                                                                      ‚ïë\n",
    "‚ïë  PARAMETER          RANGE        DEFAULT   EFFECT                   ‚ïë\n",
    "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚ïë\n",
    "‚ïë  Temperature        0.0 - 2.0    0.7       Randomness/creativity    ‚ïë\n",
    "‚ïë  Top-P              0.0 - 1.0    0.9       Nucleus sampling         ‚ïë\n",
    "‚ïë  Top-K              1 - 100+     40        Token limit              ‚ïë\n",
    "‚ïë  Repeat Penalty     1.0 - 2.0    1.1       Avoid repetition         ‚ïë\n",
    "‚ïë  Context Size       128 - 8192+  2048      Max tokens               ‚ïë\n",
    "‚ïë  Max Tokens (-n)    1 - 2048+    128       Output length            ‚ïë\n",
    "‚ïë                                                                      ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë  PRESETS BY USE CASE                                                ‚ïë\n",
    "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚ïë\n",
    "‚ïë                                                                      ‚ïë\n",
    "‚ïë  Factual Q&A:        temp=0.1,  top_p=0.9,  top_k=20,  rp=1.05     ‚ïë\n",
    "‚ïë  General Chat:       temp=0.7,  top_p=0.9,  top_k=40,  rp=1.1      ‚ïë\n",
    "‚ïë  Creative Writing:   temp=0.9,  top_p=0.95, top_k=60,  rp=1.15     ‚ïë\n",
    "‚ïë  Code Generation:    temp=0.2,  top_p=0.95, top_k=50,  rp=1.05     ‚ïë\n",
    "‚ïë  Brainstorming:      temp=1.2,  top_p=0.98, top_k=80,  rp=1.3      ‚ïë\n",
    "‚ïë                                                                      ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë  TROUBLESHOOTING                                                    ‚ïë\n",
    "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚ïë\n",
    "‚ïë                                                                      ‚ïë\n",
    "‚ïë  Problem                    Solution                                ‚ïë\n",
    "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚ïë\n",
    "‚ïë  Too repetitive             ‚Üë repeat_penalty (1.2-1.5)              ‚ïë\n",
    "‚ïë  Too random/incoherent      ‚Üì temperature (0.3-0.5)                 ‚ïë\n",
    "‚ïë  Too predictable/boring     ‚Üë temperature (0.8-1.0)                 ‚ïë\n",
    "‚ïë  Limited vocabulary         ‚Üë top_p (0.95-1.0)                      ‚ïë\n",
    "‚ïë  Too diverse/unfocused      ‚Üì top_k (20-30)                         ‚ïë\n",
    "‚ïë  Slow generation            ‚Üì context size, use GPU                 ‚ïë\n",
    "‚ïë  Out of memory              ‚Üì context size, smaller quantization    ‚ïë\n",
    "‚ïë                                                                      ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë  PERFORMANCE TIPS                                                   ‚ïë\n",
    "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚ïë\n",
    "‚ïë                                                                      ‚ïë\n",
    "‚ïë  ‚Ä¢ Start with defaults, adjust one parameter at a time              ‚ïë\n",
    "‚ïë  ‚Ä¢ Temperature has the biggest impact on output style               ‚ïë\n",
    "‚ïë  ‚Ä¢ Use top-p AND top-k together for best control                    ‚ïë\n",
    "‚ïë  ‚Ä¢ Repeat penalty 1.1-1.2 works for most use cases                  ‚ïë\n",
    "‚ïë  ‚Ä¢ Larger context = more memory, not always better quality          ‚ïë\n",
    "‚ïë  ‚Ä¢ Test with consistent prompts to compare settings                 ‚ïë\n",
    "‚ïë  ‚Ä¢ Save good parameter combinations for reuse                       ‚ïë\n",
    "‚ïë                                                                      ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\"\"\"\n",
    "\n",
    "print(reference_card)\n",
    "\n",
    "# Save to file\n",
    "with open('/tmp/inference_parameters_reference.txt', 'w') as f:\n",
    "    f.write(reference_card)\n",
    "\n",
    "print(\"\\n‚úÖ Reference card saved to /tmp/inference_parameters_reference.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: What You've Mastered\n",
    "\n",
    "Congratulations! You now have expert-level understanding of LLM inference parameters.\n",
    "\n",
    "### Core Concepts:\n",
    "‚úÖ **How text generation works** - token-by-token sampling  \n",
    "‚úÖ **Temperature** - creativity vs consistency control  \n",
    "‚úÖ **Top-P** - dynamic vocabulary limiting  \n",
    "‚úÖ **Top-K** - fixed vocabulary limiting  \n",
    "‚úÖ **Repeat Penalty** - avoiding repetitive output  \n",
    "‚úÖ **Context Management** - memory and performance tradeoffs  \n",
    "\n",
    "### Practical Skills:\n",
    "‚úÖ **Parameter tuning** for different use cases  \n",
    "‚úÖ **Performance optimization** techniques  \n",
    "‚úÖ **Troubleshooting** common quality issues  \n",
    "‚úÖ **Best practices** for various scenarios  \n",
    "‚úÖ **Interactive experimentation** workflow  \n",
    "\n",
    "### Advanced Knowledge:\n",
    "‚úÖ **Sampling strategies** and their effects  \n",
    "‚úÖ **Parameter interactions** and tradeoffs  \n",
    "‚úÖ **Memory management** for large contexts  \n",
    "‚úÖ **Quality vs speed** optimization  \n",
    "\n",
    "### Tools Created:\n",
    "‚úÖ Interactive parameter explorer  \n",
    "‚úÖ Comparison utilities  \n",
    "‚úÖ Visual guides and references  \n",
    "‚úÖ Quick reference card  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "You've completed the **foundations module**! Here's where to go from here:\n",
    "\n",
    "### Module 2: Architecture Deep Dive\n",
    "- Transformer architecture internals\n",
    "- Attention mechanisms\n",
    "- Model variants (Llama, Mistral, Phi, etc.)\n",
    "- Understanding model capabilities\n",
    "\n",
    "### Module 3: Performance Optimization\n",
    "- GPU acceleration with CUDA/Metal/Vulkan\n",
    "- Quantization deep dive\n",
    "- Batch processing\n",
    "- Caching strategies\n",
    "- Multi-GPU setups\n",
    "\n",
    "### Module 4: Advanced Usage\n",
    "- Chat interfaces and conversation memory\n",
    "- Function calling and tool use\n",
    "- Constrained generation\n",
    "- Custom sampling algorithms\n",
    "- Embedding generation\n",
    "\n",
    "### Module 5: Model Conversion & Customization\n",
    "- Converting PyTorch to GGUF\n",
    "- Custom quantization schemes\n",
    "- Model merging and fine-tuning\n",
    "- Creating adapters (LoRA)\n",
    "\n",
    "### Module 6: Production Deployment\n",
    "- Server setup (llama-server)\n",
    "- API integration\n",
    "- Load balancing\n",
    "- Monitoring and logging\n",
    "- Security best practices\n",
    "\n",
    "### Hands-on Projects:\n",
    "1. **Build a chatbot** with conversation memory\n",
    "2. **Create a code assistant** with specialized parameters\n",
    "3. **Implement RAG** (Retrieval-Augmented Generation)\n",
    "4. **Deploy a production API** with llama-server\n",
    "5. **Optimize for your hardware** (benchmark different configs)\n",
    "\n",
    "### Advanced Experiments:\n",
    "- Compare different model architectures\n",
    "- Test quantization quality tradeoffs\n",
    "- Implement custom sampling strategies\n",
    "- Build a parameter auto-tuner\n",
    "- Create model evaluation framework\n",
    "\n",
    "### Community & Resources:\n",
    "- [llama.cpp GitHub](https://github.com/ggerganov/llama.cpp)\n",
    "- [Discord Community](https://discord.gg/llama-cpp)\n",
    "- [HuggingFace GGUF Models](https://huggingface.co/models?library=gguf)\n",
    "- [Reddit r/LocalLLaMA](https://reddit.com/r/LocalLLaMA)\n",
    "- [Awesome llama.cpp](https://github.com/awesome-llama-cpp)\n",
    "\n",
    "---\n",
    "\n",
    "## Final Thoughts\n",
    "\n",
    "You've completed three comprehensive tutorials and gained solid foundational knowledge of llama.cpp:\n",
    "\n",
    "**Tutorial 1:** Your first steps - building, downloading, and running  \n",
    "**Tutorial 2:** Understanding GGUF - format internals and quantization  \n",
    "**Tutorial 3:** Mastering parameters - controlling generation quality  \n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Start simple** - defaults work well, adjust as needed\n",
    "2. **Experiment freely** - every model and use case is different\n",
    "3. **Save what works** - document good parameter combinations\n",
    "4. **Optimize iteratively** - one parameter at a time\n",
    "5. **Stay curious** - the field evolves rapidly!\n",
    "\n",
    "### You're Ready To:\n",
    "- Run any GGUF model efficiently\n",
    "- Choose optimal quantization for your needs\n",
    "- Tune parameters for specific use cases\n",
    "- Troubleshoot quality and performance issues\n",
    "- Build applications with local LLMs\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've graduated from the **llama.cpp Foundations** module. You now have the knowledge and skills to:\n",
    "- Confidently work with local language models\n",
    "- Understand the technical details\n",
    "- Optimize for your specific needs\n",
    "- Continue learning advanced topics\n",
    "\n",
    "**The journey doesn't end here** - it's just beginning. The world of local LLMs is yours to explore!\n",
    "\n",
    "Keep building, keep learning, and most importantly - **have fun!** üöÄüéâ\n",
    "\n",
    "---\n",
    "\n",
    "*Questions? Issues? Ideas? Share them with the community and keep the learning going!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
