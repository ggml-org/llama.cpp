{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding GGUF: The Model Format Deep Dive\n",
    "\n",
    "**Welcome back!** In this tutorial, we'll explore GGUF (GPT-Generated Unified Format) - the file format that makes llama.cpp so efficient.\n",
    "\n",
    "**Time to complete:** 20-30 minutes  \n",
    "**Prerequisites:** Tutorial 1 completed  \n",
    "**What you'll learn:**\n",
    "- What GGUF is and why it matters\n",
    "- How to read model metadata programmatically\n",
    "- Understanding quantization levels\n",
    "- Comparing different model formats\n",
    "- Choosing the right quantization for your needs\n",
    "\n",
    "**Bonus:** You'll be able to inspect any GGUF model like a pro!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is GGUF?\n",
    "\n",
    "GGUF (GPT-Generated Unified Format) is a binary format designed for efficient storage and loading of large language models.\n",
    "\n",
    "### Key Features:\n",
    "1. **Single file**: Everything in one place (weights, metadata, vocabulary)\n",
    "2. **Memory-mapped**: Fast loading, minimal RAM overhead\n",
    "3. **Quantization**: Compressed models with minimal quality loss\n",
    "4. **Extensible**: Rich metadata support\n",
    "5. **Platform-agnostic**: Works on any system\n",
    "\n",
    "### Why GGUF vs Other Formats?\n",
    "\n",
    "| Format | Size | Load Speed | Metadata | Multi-part |\n",
    "|--------|------|------------|----------|------------|\n",
    "| **GGUF** | Small | Fast | Rich | Single file |\n",
    "| PyTorch | Large | Slow | Limited | Multi-file |\n",
    "| SafeTensors | Large | Medium | Good | Multi-file |\n",
    "| ONNX | Medium | Medium | Limited | Single file |\n",
    "\n",
    "GGUF is optimized specifically for inference efficiency!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Install gguf package if not already installed\n",
    "try:\n",
    "    import gguf\n",
    "    print(\"‚úÖ gguf library already installed\")\n",
    "except ImportError:\n",
    "    print(\"üì¶ Installing gguf library...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"gguf\", \"-q\"])\n",
    "    import gguf\n",
    "    print(\"‚úÖ gguf library installed\")\n",
    "\n",
    "# Import other libraries\n",
    "import os\n",
    "import struct\n",
    "from pathlib import Path\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "\n",
    "print(f\"‚úÖ Using gguf version: {gguf.__version__ if hasattr(gguf, '__version__') else 'unknown'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading GGUF Metadata\n",
    "\n",
    "Every GGUF file contains rich metadata about the model. Let's explore it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Find our model from Tutorial 1\n",
    "llama_cpp_root = Path(\"/home/user/llama.cpp-learn\")\n",
    "model_path = llama_cpp_root / \"models\" / \"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n",
    "\n",
    "if not model_path.exists():\n",
    "    print(f\"‚ùå Model not found at {model_path}\")\n",
    "    print(\"   Please complete Tutorial 1 first!\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found model: {model_path.name}\")\n",
    "    print(f\"   Size: {model_path.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "    print(f\"   Path: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and read the GGUF file\n",
    "reader = gguf.GGUFReader(str(model_path))\n",
    "\n",
    "print(\"üìñ GGUF File Structure:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Version: {reader.version}\")\n",
    "print(f\"Tensor count: {reader.tensor_count}\")\n",
    "print(f\"Metadata fields: {len(reader.fields)}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Model Metadata\n",
    "\n",
    "Let's extract and display all the interesting metadata stored in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metadata\n",
    "metadata = {}\n",
    "for key, field in reader.fields.items():\n",
    "    # Convert field value to readable format\n",
    "    try:\n",
    "        if hasattr(field.parts, '__iter__') and not isinstance(field.parts, (str, bytes)):\n",
    "            value = list(field.parts)\n",
    "        else:\n",
    "            value = field.parts\n",
    "        metadata[key] = value\n",
    "    except:\n",
    "        metadata[key] = str(field.parts)\n",
    "\n",
    "# Display key metadata\n",
    "important_keys = [\n",
    "    'general.name',\n",
    "    'general.architecture', \n",
    "    'general.quantization_version',\n",
    "    'general.file_type',\n",
    "    'llama.context_length',\n",
    "    'llama.embedding_length',\n",
    "    'llama.block_count',\n",
    "    'llama.feed_forward_length',\n",
    "    'llama.attention.head_count',\n",
    "    'llama.attention.head_count_kv',\n",
    "    'tokenizer.ggml.model',\n",
    "    'tokenizer.ggml.tokens',\n",
    "]\n",
    "\n",
    "print(\"\\nüîç Model Information:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for key in important_keys:\n",
    "    if key in metadata:\n",
    "        value = metadata[key]\n",
    "        # Format large lists\n",
    "        if isinstance(value, list) and len(value) > 10:\n",
    "            display_value = f\"[{len(value)} items]\"\n",
    "        else:\n",
    "            display_value = value\n",
    "        print(f\"{key:40s}: {display_value}\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Architecture Parameters\n",
    "\n",
    "Let's visualize what these parameters mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract architecture details\n",
    "arch_info = {\n",
    "    'Context Length': metadata.get('llama.context_length', 'N/A'),\n",
    "    'Embedding Size': metadata.get('llama.embedding_length', 'N/A'),\n",
    "    'Layers': metadata.get('llama.block_count', 'N/A'),\n",
    "    'Attention Heads': metadata.get('llama.attention.head_count', 'N/A'),\n",
    "    'KV Heads': metadata.get('llama.attention.head_count_kv', 'N/A'),\n",
    "    'FFN Size': metadata.get('llama.feed_forward_length', 'N/A'),\n",
    "}\n",
    "\n",
    "print(\"\\nüèóÔ∏è  Model Architecture Breakdown:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for param, value in arch_info.items():\n",
    "    print(f\"\\n{param}: {value}\")\n",
    "    \n",
    "    if param == 'Context Length':\n",
    "        print(\"  ‚Üí Maximum input tokens the model can process at once\")\n",
    "        print(\"  ‚Üí Longer context = more memory needed\")\n",
    "    \n",
    "    elif param == 'Embedding Size':\n",
    "        print(\"  ‚Üí Dimensionality of token representations\")\n",
    "        print(\"  ‚Üí Larger = more expressive, but slower\")\n",
    "    \n",
    "    elif param == 'Layers':\n",
    "        print(\"  ‚Üí Number of transformer blocks\")\n",
    "        print(\"  ‚Üí More layers = better quality, but slower\")\n",
    "    \n",
    "    elif param == 'Attention Heads':\n",
    "        print(\"  ‚Üí Parallel attention mechanisms\")\n",
    "        print(\"  ‚Üí More heads = better at capturing different patterns\")\n",
    "    \n",
    "    elif param == 'KV Heads':\n",
    "        print(\"  ‚Üí Key-Value heads (for grouped-query attention)\")\n",
    "        print(\"  ‚Üí Fewer than attention heads = more efficient\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Analysis\n",
    "\n",
    "GGUF files contain tensors (multi-dimensional arrays of numbers). Let's explore them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze tensors\n",
    "tensor_info = []\n",
    "total_parameters = 0\n",
    "tensor_types = {}\n",
    "\n",
    "for tensor in reader.tensors:\n",
    "    # Calculate number of parameters in this tensor\n",
    "    n_elements = np.prod(tensor.shape)\n",
    "    total_parameters += n_elements\n",
    "    \n",
    "    # Track tensor types\n",
    "    tensor_type = str(tensor.tensor_type)\n",
    "    tensor_types[tensor_type] = tensor_types.get(tensor_type, 0) + 1\n",
    "    \n",
    "    tensor_info.append({\n",
    "        'name': tensor.name,\n",
    "        'shape': tensor.shape,\n",
    "        'type': tensor_type,\n",
    "        'n_elements': n_elements\n",
    "    })\n",
    "\n",
    "print(f\"\\nüìä Tensor Statistics:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total tensors: {len(tensor_info)}\")\n",
    "print(f\"Total parameters: {total_parameters:,}\")\n",
    "print(f\"Approximate size: {total_parameters / 1e9:.2f}B parameters\")\n",
    "print(\"\\nTensor types distribution:\")\n",
    "for ttype, count in sorted(tensor_types.items()):\n",
    "    print(f\"  {ttype:30s}: {count:4d} tensors\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample tensors\n",
    "print(\"\\nüî¨ Sample Tensor Details (first 10):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, t in enumerate(tensor_info[:10]):\n",
    "    print(f\"\\n{i+1}. {t['name']}\")\n",
    "    print(f\"   Shape: {t['shape']}\")\n",
    "    print(f\"   Type: {t['type']}\")\n",
    "    print(f\"   Elements: {t['n_elements']:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Quantization\n",
    "\n",
    "Quantization reduces model size by using fewer bits per parameter. Let's explore different quantization levels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization comparison data\n",
    "quantization_info = {\n",
    "    'F16': {\n",
    "        'bits': 16,\n",
    "        'size_multiplier': 1.0,\n",
    "        'quality': 100,\n",
    "        'description': 'Full precision - highest quality, largest size'\n",
    "    },\n",
    "    'Q8_0': {\n",
    "        'bits': 8,\n",
    "        'size_multiplier': 0.5,\n",
    "        'quality': 99,\n",
    "        'description': '8-bit - excellent quality, good compression'\n",
    "    },\n",
    "    'Q6_K': {\n",
    "        'bits': 6,\n",
    "        'size_multiplier': 0.38,\n",
    "        'quality': 97,\n",
    "        'description': '6-bit - very good quality, better compression'\n",
    "    },\n",
    "    'Q5_K_M': {\n",
    "        'bits': 5,\n",
    "        'size_multiplier': 0.31,\n",
    "        'quality': 95,\n",
    "        'description': '5-bit - good quality, balanced'\n",
    "    },\n",
    "    'Q4_K_M': {\n",
    "        'bits': 4,\n",
    "        'size_multiplier': 0.25,\n",
    "        'quality': 90,\n",
    "        'description': '4-bit - recommended balance (what we use!)'\n",
    "    },\n",
    "    'Q3_K_M': {\n",
    "        'bits': 3,\n",
    "        'size_multiplier': 0.19,\n",
    "        'quality': 80,\n",
    "        'description': '3-bit - noticeable quality loss, very small'\n",
    "    },\n",
    "    'Q2_K': {\n",
    "        'bits': 2,\n",
    "        'size_multiplier': 0.13,\n",
    "        'quality': 65,\n",
    "        'description': '2-bit - significant quality loss, tiny size'\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"\\nüìâ Quantization Level Comparison:\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'Format':<12} {'Bits':<6} {'Size':<12} {'Quality':<10} {'Description':<40}\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "for fmt, info in quantization_info.items():\n",
    "    size_pct = info['size_multiplier'] * 100\n",
    "    quality_bar = '‚ñà' * (info['quality'] // 10)\n",
    "    print(f\"{fmt:<12} {info['bits']:<6} {size_pct:>5.0f}% {quality_bar:<10} {info['description']:<40}\")\n",
    "\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Quantization Impact\n",
    "\n",
    "Let's create visualizations to understand the tradeoffs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Prepare data for visualization\n",
    "formats = list(quantization_info.keys())\n",
    "sizes = [info['size_multiplier'] * 100 for info in quantization_info.values()]\n",
    "qualities = [info['quality'] for info in quantization_info.values()]\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Size comparison\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(formats)))\n",
    "axes[0].barh(formats, sizes, color=colors)\n",
    "axes[0].set_xlabel('Relative Size (%)', fontsize=12)\n",
    "axes[0].set_title('Model Size by Quantization', fontsize=14, fontweight='bold')\n",
    "axes[0].axvline(x=25, color='red', linestyle='--', alpha=0.5, label='Q4_K_M (Our model)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 2: Quality comparison  \n",
    "axes[1].barh(formats, qualities, color=colors)\n",
    "axes[1].set_xlabel('Quality Score', fontsize=12)\n",
    "axes[1].set_title('Quality by Quantization', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlim([0, 105])\n",
    "axes[1].axvline(x=90, color='red', linestyle='--', alpha=0.5, label='Q4_K_M (Our model)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 3: Size vs Quality tradeoff\n",
    "axes[2].scatter(sizes, qualities, s=200, c=colors, alpha=0.6)\n",
    "for i, fmt in enumerate(formats):\n",
    "    axes[2].annotate(fmt, (sizes[i], qualities[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "axes[2].set_xlabel('Relative Size (%)', fontsize=12)\n",
    "axes[2].set_ylabel('Quality Score', fontsize=12)\n",
    "axes[2].set_title('Size vs Quality Tradeoff', fontsize=14, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Highlight Q4_K_M\n",
    "q4_idx = formats.index('Q4_K_M')\n",
    "axes[2].scatter([sizes[q4_idx]], [qualities[q4_idx]], \n",
    "               s=300, c='red', marker='*', zorder=10, label='Q4_K_M (Our model)')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/tmp/quantization_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualization saved to /tmp/quantization_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Quantization Guide\n",
    "\n",
    "### Which quantization should you choose?\n",
    "\n",
    "**For Chat/General Use:**\n",
    "- **Q4_K_M** or **Q5_K_M** - Best balance of size and quality\n",
    "- This is what we use in the tutorials!\n",
    "\n",
    "**For Maximum Quality:**\n",
    "- **Q8_0** - Minimal quality loss\n",
    "- **Q6_K** - Excellent quality, reasonable size\n",
    "\n",
    "**For Resource-Constrained Systems:**\n",
    "- **Q3_K_M** - Still usable for many tasks\n",
    "- **Q2_K** - Last resort, significant quality drop\n",
    "\n",
    "**For Production/Critical Applications:**\n",
    "- **Q5_K_M** or **Q6_K** - Reliable quality\n",
    "- Test thoroughly with your specific use case!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Model Memory Requirements\n",
    "\n",
    "Let's calculate how much RAM different quantizations would need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model size (F16)\n",
    "base_size_mb = model_path.stat().st_size / 1024 / 1024 / 0.25  # Our model is Q4_K_M (25%)\n",
    "\n",
    "print(\"\\nüíæ Memory Requirements (TinyLlama 1.1B):\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Format':<12} {'Model Size':<15} {'Context (2K)':<15} {'Total RAM':<15}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for fmt, info in quantization_info.items():\n",
    "    model_size = base_size_mb * info['size_multiplier']\n",
    "    context_overhead = 200  # Approximate overhead for 2K context\n",
    "    total_ram = model_size + context_overhead\n",
    "    \n",
    "    print(f\"{fmt:<12} {model_size:>6.0f} MB       \"\n",
    "          f\"{context_overhead:>6.0f} MB       \"\n",
    "          f\"{total_ram:>6.0f} MB\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nNote: Actual RAM usage depends on context length and batch size.\")\n",
    "print(\"      These are estimates for single-user inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Model Files\n",
    "\n",
    "If you have multiple models, let's compare them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all GGUF models\n",
    "models_dir = llama_cpp_root / \"models\"\n",
    "gguf_files = list(models_dir.glob(\"*.gguf\"))\n",
    "\n",
    "if len(gguf_files) > 0:\n",
    "    print(f\"\\nüìö Found {len(gguf_files)} GGUF model(s):\")\n",
    "    print(\"=\" * 90)\n",
    "    print(f\"{'Filename':<45} {'Size (MB)':<12} {'Quantization':<15}\")\n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    for model_file in sorted(gguf_files):\n",
    "        size_mb = model_file.stat().st_size / 1024 / 1024\n",
    "        \n",
    "        # Try to extract quantization from filename\n",
    "        name = model_file.name\n",
    "        quant = 'Unknown'\n",
    "        for q_type in quantization_info.keys():\n",
    "            if q_type in name:\n",
    "                quant = q_type\n",
    "                break\n",
    "        \n",
    "        print(f\"{name:<45} {size_mb:>10.1f}   {quant:<15}\")\n",
    "    \n",
    "    print(\"=\" * 90)\n",
    "else:\n",
    "    print(\"\\nüìö No additional models found.\")\n",
    "    print(\"   Download more models from HuggingFace to compare!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Reading Vocabulary\n",
    "\n",
    "Every model has a vocabulary (tokenizer). Let's explore it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract vocabulary\n",
    "vocab_key = 'tokenizer.ggml.tokens'\n",
    "\n",
    "if vocab_key in metadata:\n",
    "    tokens = metadata[vocab_key]\n",
    "    \n",
    "    print(f\"\\nüìñ Vocabulary Information:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Total tokens: {len(tokens):,}\")\n",
    "    print(f\"\\nFirst 20 tokens:\")\n",
    "    \n",
    "    for i, token in enumerate(tokens[:20]):\n",
    "        # Handle byte tokens that might not display well\n",
    "        try:\n",
    "            if isinstance(token, bytes):\n",
    "                display = token.decode('utf-8', errors='replace')\n",
    "            else:\n",
    "                display = str(token)\n",
    "            # Escape special characters\n",
    "            display = repr(display)[1:-1]  # Remove outer quotes\n",
    "            print(f\"  {i:5d}: {display[:50]}\")\n",
    "        except:\n",
    "            print(f\"  {i:5d}: [binary token]\")\n",
    "    \n",
    "    print(f\"\\nLast 10 tokens:\")\n",
    "    for i, token in enumerate(tokens[-10:], start=len(tokens)-10):\n",
    "        try:\n",
    "            if isinstance(token, bytes):\n",
    "                display = token.decode('utf-8', errors='replace')\n",
    "            else:\n",
    "                display = str(token)\n",
    "            display = repr(display)[1:-1]\n",
    "            print(f\"  {i:5d}: {display[:50]}\")\n",
    "        except:\n",
    "            print(f\"  {i:5d}: [binary token]\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Vocabulary not found in metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison Tool\n",
    "\n",
    "Let's create a function to quickly compare any two GGUF models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_gguf_models(model1_path, model2_path):\n",
    "    \"\"\"Compare two GGUF models side by side\"\"\"\n",
    "    \n",
    "    def get_model_info(path):\n",
    "        reader = gguf.GGUFReader(str(path))\n",
    "        metadata = {}\n",
    "        for key, field in reader.fields.items():\n",
    "            try:\n",
    "                metadata[key] = field.parts\n",
    "            except:\n",
    "                metadata[key] = str(field.parts)\n",
    "        \n",
    "        return {\n",
    "            'name': path.name,\n",
    "            'size_mb': path.stat().st_size / 1024 / 1024,\n",
    "            'architecture': metadata.get('general.architecture', 'Unknown'),\n",
    "            'context_length': metadata.get('llama.context_length', 'Unknown'),\n",
    "            'layers': metadata.get('llama.block_count', 'Unknown'),\n",
    "            'embedding_size': metadata.get('llama.embedding_length', 'Unknown'),\n",
    "            'tensor_count': reader.tensor_count,\n",
    "        }\n",
    "    \n",
    "    info1 = get_model_info(Path(model1_path))\n",
    "    info2 = get_model_info(Path(model2_path))\n",
    "    \n",
    "    print(\"\\nüîÑ Model Comparison:\")\n",
    "    print(\"=\" * 90)\n",
    "    print(f\"{'Property':<25} {'Model 1':<30} {'Model 2':<30}\")\n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    for key in ['name', 'size_mb', 'architecture', 'context_length', \n",
    "                'layers', 'embedding_size', 'tensor_count']:\n",
    "        val1 = info1[key]\n",
    "        val2 = info2[key]\n",
    "        \n",
    "        if key == 'size_mb':\n",
    "            val1_str = f\"{val1:.1f} MB\"\n",
    "            val2_str = f\"{val2:.1f} MB\"\n",
    "        else:\n",
    "            val1_str = str(val1)\n",
    "            val2_str = str(val2)\n",
    "        \n",
    "        print(f\"{key.replace('_', ' ').title():<25} {val1_str:<30} {val2_str:<30}\")\n",
    "    \n",
    "    print(\"=\" * 90)\n",
    "\n",
    "# Example usage (uncomment if you have multiple models)\n",
    "# compare_gguf_models(model_path, \"path/to/another/model.gguf\")\n",
    "\n",
    "print(\"‚úÖ Comparison function ready!\")\n",
    "print(\"   Usage: compare_gguf_models('path1.gguf', 'path2.gguf')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Model Information\n",
    "\n",
    "Let's save all this information to a JSON file for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Prepare exportable metadata\n",
    "export_data = {\n",
    "    'model_file': str(model_path),\n",
    "    'file_size_mb': model_path.stat().st_size / 1024 / 1024,\n",
    "    'gguf_version': reader.version,\n",
    "    'tensor_count': reader.tensor_count,\n",
    "    'total_parameters': total_parameters,\n",
    "    'metadata': {},\n",
    "    'tensor_types': tensor_types,\n",
    "}\n",
    "\n",
    "# Add serializable metadata\n",
    "for key, value in metadata.items():\n",
    "    # Convert to JSON-serializable format\n",
    "    if isinstance(value, (list, str, int, float, bool)):\n",
    "        if isinstance(value, list) and len(value) > 100:\n",
    "            export_data['metadata'][key] = f\"[{len(value)} items]\"\n",
    "        else:\n",
    "            export_data['metadata'][key] = value\n",
    "    else:\n",
    "        export_data['metadata'][key] = str(value)\n",
    "\n",
    "# Save to file\n",
    "output_file = \"/tmp/model_info.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(export_data, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Model information exported to: {output_file}\")\n",
    "print(f\"   You can open this file to see all model details in JSON format!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "Congratulations! You now understand:\n",
    "\n",
    "### GGUF Format:\n",
    "‚úÖ Single-file format optimized for inference  \n",
    "‚úÖ Contains weights, metadata, and vocabulary  \n",
    "‚úÖ Memory-mapped for efficient loading  \n",
    "‚úÖ Rich metadata for model introspection  \n",
    "\n",
    "### Quantization:\n",
    "‚úÖ Reduces model size by using fewer bits  \n",
    "‚úÖ Q4_K_M and Q5_K_M are sweet spots  \n",
    "‚úÖ Trade size vs quality based on your needs  \n",
    "‚úÖ Can reduce size by 4-8x with minimal quality loss  \n",
    "\n",
    "### Practical Skills:\n",
    "‚úÖ Read GGUF metadata programmatically  \n",
    "‚úÖ Compare different models and quantizations  \n",
    "‚úÖ Calculate memory requirements  \n",
    "‚úÖ Choose the right model for your hardware  \n",
    "\n",
    "### Technical Knowledge:\n",
    "‚úÖ Model architecture parameters (layers, heads, dimensions)  \n",
    "‚úÖ Tensor organization and types  \n",
    "‚úÖ Vocabulary/tokenizer structure  \n",
    "‚úÖ File format internals  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "### Tutorial 3: Inference Parameters (30-40 minutes)\n",
    "Deep dive into generation parameters:\n",
    "- Temperature, top-p, top-k\n",
    "- Sampling strategies\n",
    "- Context window management\n",
    "- Performance optimization\n",
    "- Best practices for different tasks\n",
    "\n",
    "### Experiment More:\n",
    "1. **Download different quantizations** of the same model and compare quality\n",
    "2. **Try different model sizes** (3B, 7B, 13B) and measure memory usage\n",
    "3. **Explore model architectures** - Llama, Mistral, Phi, Qwen\n",
    "4. **Build a model selector** tool based on available RAM\n",
    "\n",
    "### Advanced Topics:\n",
    "- Custom quantization with llama-quantize\n",
    "- Converting PyTorch models to GGUF\n",
    "- Merging and splitting models\n",
    "- Creating custom vocabularies\n",
    "\n",
    "### Resources:\n",
    "- [GGUF Specification](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)\n",
    "- [Quantization Guide](https://github.com/ggerganov/llama.cpp/blob/master/examples/quantize/README.md)\n",
    "- [HuggingFace GGUF Models](https://huggingface.co/models?library=gguf)\n",
    "- [TheBloke's Model Collection](https://huggingface.co/TheBloke)\n",
    "\n",
    "---\n",
    "\n",
    "## Great Work!\n",
    "\n",
    "You've mastered GGUF format understanding - a crucial skill for working with llama.cpp effectively. You can now:\n",
    "- Choose the right model and quantization\n",
    "- Understand model specifications\n",
    "- Optimize for your hardware\n",
    "- Debug model-related issues\n",
    "\n",
    "**Ready for Tutorial 3?** Let's dive into inference parameters! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
