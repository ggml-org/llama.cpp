{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: GGUF Format Exploration\n",
    "\n",
    "**Module**: Module 1 - Foundations  \n",
    "**Estimated Time**: 45-60 minutes  \n",
    "**Difficulty**: Beginner to Intermediate  \n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this lab, you will:\n",
    "- [ ] Understand the GGUF file format structure\n",
    "- [ ] Use gguf-py to read and inspect model metadata\n",
    "- [ ] Extract and analyze model architecture information\n",
    "- [ ] Compare different quantization formats\n",
    "- [ ] Understand the relationship between quantization and model size\n",
    "- [ ] Inspect tensor information and data layout\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Lab 1 (Setup and First Inference)\n",
    "- Basic understanding of neural network architectures\n",
    "- Python programming knowledge\n",
    "- At least one GGUF model file downloaded\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "GGUF (GPT-Generated Unified Format) is the file format used by llama.cpp to store and load models efficiently. In this lab, you'll explore the internals of GGUF files, understand quantization schemes, and learn to inspect model architectures programmatically.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Understanding GGUF (10 minutes)\n",
    "\n",
    "### What is GGUF?\n",
    "\n",
    "GGUF is a binary file format that contains:\n",
    "1. **Metadata**: Model information, hyperparameters, tokenizer data\n",
    "2. **Tensor Information**: Names, shapes, and types of all model tensors\n",
    "3. **Tensor Data**: The actual model weights (quantized or full precision)\n",
    "\n",
    "### File Structure\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Magic Number       ‚îÇ 4 bytes: \"GGUF\" (0x46554747)\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Version            ‚îÇ 4 bytes: uint32\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Tensor Count       ‚îÇ 8 bytes: uint64\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Metadata Count     ‚îÇ 8 bytes: uint64\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Metadata KV Pairs  ‚îÇ Variable size\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Tensor Info        ‚îÇ Variable size\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Padding            ‚îÇ Alignment to 32 bytes\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Tensor Data        ‚îÇ Bulk data (quantized weights)\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install gguf library\n",
    "!pip install gguf -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gguf\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "print(f\"‚úì gguf library version: {gguf.__version__ if hasattr(gguf, '__version__') else 'unknown'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the model from Lab 1\n",
    "MODEL_PATH = Path(\"./models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\")\n",
    "\n",
    "if not MODEL_PATH.exists():\n",
    "    print(f\"‚úó Model not found at {MODEL_PATH}\")\n",
    "    print(\"Please complete Lab 1 first to download the model.\")\n",
    "else:\n",
    "    print(f\"‚úì Model found: {MODEL_PATH}\")\n",
    "    print(f\"‚úì File size: {MODEL_PATH.stat().st_size / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Reading GGUF Metadata (15 minutes)\n",
    "\n",
    "Let's start by reading the model's metadata. This includes information about the model architecture, training parameters, and tokenizer configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GGUF file\n",
    "reader = gguf.GGUFReader(MODEL_PATH)\n",
    "\n",
    "print(f\"GGUF Version: {reader.version}\")\n",
    "print(f\"Tensor Count: {reader.tensor_count}\")\n",
    "print(f\"Metadata Fields: {len(reader.fields)}\")\n",
    "print(\"\\n‚úì GGUF file loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all metadata\n",
    "def get_all_metadata(reader):\n",
    "    \"\"\"Extract all metadata as a dictionary.\"\"\"\n",
    "    metadata = {}\n",
    "    for field in reader.fields.values():\n",
    "        # Get field name and value\n",
    "        field_name = field.name\n",
    "        \n",
    "        # Handle different field types\n",
    "        if hasattr(field.parts, '__iter__') and not isinstance(field.parts, (str, bytes)):\n",
    "            # Array or list\n",
    "            field_value = list(field.parts)\n",
    "        else:\n",
    "            field_value = field.parts\n",
    "        \n",
    "        metadata[field_name] = field_value\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "metadata = get_all_metadata(reader)\n",
    "print(f\"Extracted {len(metadata)} metadata fields\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display key model information\n",
    "def print_model_info(metadata):\n",
    "    \"\"\"Print human-readable model information.\"\"\"\n",
    "    print(\"=== Model Information ===\")\n",
    "    \n",
    "    # Look for common metadata keys\n",
    "    interesting_keys = [\n",
    "        'general.architecture',\n",
    "        'general.name',\n",
    "        'general.file_type',\n",
    "        'general.quantization_version',\n",
    "        'llama.context_length',\n",
    "        'llama.embedding_length',\n",
    "        'llama.block_count',\n",
    "        'llama.feed_forward_length',\n",
    "        'llama.attention.head_count',\n",
    "        'llama.attention.head_count_kv',\n",
    "        'tokenizer.ggml.model',\n",
    "        'tokenizer.ggml.tokens_length'\n",
    "    ]\n",
    "    \n",
    "    for key in interesting_keys:\n",
    "        if key in metadata:\n",
    "            print(f\"{key}: {metadata[key]}\")\n",
    "\n",
    "print_model_info(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Extract Model Architecture Details\n",
    "\n",
    "From the metadata, extract and calculate:\n",
    "1. Total number of parameters (approximate)\n",
    "2. Number of attention heads\n",
    "3. Hidden dimension size\n",
    "4. Number of transformer layers\n",
    "5. Vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Extract architecture details from metadata\n",
    "# YOUR CODE HERE\n",
    "\n",
    "architecture = metadata.get('general.architecture', 'unknown')\n",
    "n_layers = None  # Extract from metadata\n",
    "n_heads = None   # Extract from metadata\n",
    "hidden_dim = None  # Extract from metadata (embedding_length)\n",
    "vocab_size = None  # Extract from metadata (tokens_length)\n",
    "\n",
    "print(f\"Architecture: {architecture}\")\n",
    "print(f\"Layers: {n_layers}\")\n",
    "print(f\"Attention Heads: {n_heads}\")\n",
    "print(f\"Hidden Dimension: {hidden_dim}\")\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "\n",
    "# Calculate approximate parameter count\n",
    "# Formula (simplified): params ‚âà 12 √ó n_layers √ó hidden_dim¬≤\n",
    "if n_layers and hidden_dim:\n",
    "    approx_params = 12 * n_layers * (hidden_dim ** 2)\n",
    "    print(f\"\\nApproximate Parameters: {approx_params / 1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-grading cell - DO NOT MODIFY\n",
    "def test_architecture_extraction():\n",
    "    assert n_layers is not None and n_layers > 0, \"Number of layers not extracted\"\n",
    "    assert n_heads is not None and n_heads > 0, \"Number of heads not extracted\"\n",
    "    assert hidden_dim is not None and hidden_dim > 0, \"Hidden dimension not extracted\"\n",
    "    assert vocab_size is not None and vocab_size > 0, \"Vocabulary size not extracted\"\n",
    "    print(\"‚úì Architecture details extracted correctly!\")\n",
    "    return True\n",
    "\n",
    "test_architecture_extraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Inspecting Tensors (15 minutes)\n",
    "\n",
    "Now let's look at the actual model tensors. Each tensor has:\n",
    "- **Name**: Identifies its role (e.g., `token_embd.weight`, `blk.0.attn_q.weight`)\n",
    "- **Shape**: Dimensions of the tensor\n",
    "- **Type**: Data type and quantization scheme\n",
    "- **Offset**: Location in the file\n",
    "- **Size**: Number of bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all tensors\n",
    "tensors = list(reader.tensors)\n",
    "\n",
    "print(f\"Total tensors in model: {len(tensors)}\")\n",
    "print(\"\\nFirst 10 tensors:\")\n",
    "for i, tensor in enumerate(tensors[:10]):\n",
    "    print(f\"{i+1}. {tensor.name}\")\n",
    "    print(f\"   Shape: {tensor.shape}\")\n",
    "    print(f\"   Type: {tensor.tensor_type}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze tensor types\n",
    "def analyze_tensor_types(tensors):\n",
    "    \"\"\"Analyze the distribution of tensor types in the model.\"\"\"\n",
    "    type_counts = defaultdict(int)\n",
    "    type_sizes = defaultdict(int)\n",
    "    \n",
    "    for tensor in tensors:\n",
    "        tensor_type = str(tensor.tensor_type)\n",
    "        type_counts[tensor_type] += 1\n",
    "        \n",
    "        # Calculate tensor size in bytes\n",
    "        # Note: This is approximate as we'd need to know exact quantization sizes\n",
    "        n_elements = np.prod(tensor.shape)\n",
    "        type_sizes[tensor_type] += n_elements\n",
    "    \n",
    "    return type_counts, type_sizes\n",
    "\n",
    "type_counts, type_sizes = analyze_tensor_types(tensors)\n",
    "\n",
    "print(\"=== Tensor Type Distribution ===\")\n",
    "for tensor_type, count in sorted(type_counts.items()):\n",
    "    print(f\"{tensor_type}: {count} tensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Tensor Names\n",
    "\n",
    "LLaMA model tensor naming follows a pattern:\n",
    "\n",
    "- `token_embd.weight`: Token embedding matrix\n",
    "- `blk.{N}.attn_q.weight`: Query weights for attention in layer N\n",
    "- `blk.{N}.attn_k.weight`: Key weights for attention in layer N\n",
    "- `blk.{N}.attn_v.weight`: Value weights for attention in layer N\n",
    "- `blk.{N}.attn_output.weight`: Attention output projection\n",
    "- `blk.{N}.ffn_up.weight`: Feed-forward \"up\" projection\n",
    "- `blk.{N}.ffn_down.weight`: Feed-forward \"down\" projection\n",
    "- `output.weight`: Final output projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Group Tensors by Layer\n",
    "\n",
    "Create a function that groups tensors by their layer number and counts tensors per layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_tensors_by_layer(tensors):\n",
    "    \"\"\"\n",
    "    Group tensors by layer number.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {layer_num: [tensor_names]}\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    layers = defaultdict(list)\n",
    "    \n",
    "    for tensor in tensors:\n",
    "        # Extract layer number from tensor name (e.g., \"blk.0.attn_q.weight\" -> 0)\n",
    "        # Hint: Use string parsing or regex\n",
    "        pass\n",
    "    \n",
    "    return dict(layers)\n",
    "\n",
    "# Test your implementation\n",
    "layer_groups = group_tensors_by_layer(tensors)\n",
    "print(f\"Found {len(layer_groups)} unique layers\")\n",
    "if layer_groups:\n",
    "    first_layer = min(layer_groups.keys())\n",
    "    print(f\"\\nTensors in layer {first_layer}:\")\n",
    "    for name in layer_groups[first_layer][:5]:\n",
    "        print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Calculate Total Model Size\n",
    "\n",
    "Calculate the total size of the model by summing all tensor data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_model_size(tensors):\n",
    "    \"\"\"\n",
    "    Calculate total model size in bytes.\n",
    "    \n",
    "    Note: This is approximate as different quantization types\n",
    "    use different bytes per element.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Mapping of tensor types to approximate bytes per element\n",
    "    bytes_per_type = {\n",
    "        'F32': 4,  # Full precision float\n",
    "        'F16': 2,  # Half precision\n",
    "        'Q8_0': 1.125,  # 8-bit quantization\n",
    "        'Q4_K': 0.5625,  # 4-bit K-quant\n",
    "        'Q5_K': 0.6875,  # 5-bit K-quant\n",
    "        'Q6_K': 0.8125,  # 6-bit K-quant\n",
    "    }\n",
    "    \n",
    "    total_bytes = 0\n",
    "    \n",
    "    for tensor in tensors:\n",
    "        # Calculate size based on shape and type\n",
    "        pass\n",
    "    \n",
    "    return total_bytes\n",
    "\n",
    "# Test your implementation\n",
    "calculated_size = calculate_model_size(tensors)\n",
    "actual_size = MODEL_PATH.stat().st_size\n",
    "\n",
    "print(f\"Calculated model size: {calculated_size / (1024**2):.2f} MB\")\n",
    "print(f\"Actual file size: {actual_size / (1024**2):.2f} MB\")\n",
    "print(f\"Difference: {abs(calculated_size - actual_size) / (1024**2):.2f} MB (overhead from metadata)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Quantization Comparison (15 minutes)\n",
    "\n",
    "Different quantization schemes offer different trade-offs between model size and quality.\n",
    "\n",
    "### Common Quantization Types\n",
    "\n",
    "- **Q4_0**: 4-bit quantization, basic (4.5 bits per weight)\n",
    "- **Q4_K_M**: 4-bit K-quant, medium (4.85 bits per weight)\n",
    "- **Q5_K_M**: 5-bit K-quant, medium (5.54 bits per weight)\n",
    "- **Q6_K**: 6-bit K-quant (6.56 bits per weight)\n",
    "- **Q8_0**: 8-bit quantization (8.5 bits per weight)\n",
    "- **F16**: Half precision float (16 bits per weight)\n",
    "\n",
    "### K-Quants\n",
    "\n",
    "K-quants use mixed precision:\n",
    "- Important tensors (attention) get higher precision\n",
    "- Less important tensors get lower precision\n",
    "- Result: Better quality at similar size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theoretical size comparison for a 7B parameter model\n",
    "def compare_quantization_sizes(param_count_billions=7.0):\n",
    "    \"\"\"\n",
    "    Compare file sizes for different quantization schemes.\n",
    "    \n",
    "    Args:\n",
    "        param_count_billions: Model size in billions of parameters\n",
    "    \"\"\"\n",
    "    param_count = param_count_billions * 1e9\n",
    "    \n",
    "    quantizations = {\n",
    "        'F32 (Full Precision)': 32,\n",
    "        'F16 (Half Precision)': 16,\n",
    "        'Q8_0': 8.5,\n",
    "        'Q6_K': 6.56,\n",
    "        'Q5_K_M': 5.54,\n",
    "        'Q4_K_M': 4.85,\n",
    "        'Q4_0': 4.5,\n",
    "    }\n",
    "    \n",
    "    print(f\"=== Size Comparison for {param_count_billions}B Model ===\")\n",
    "    print(f\"{'Quantization':<25} {'Bits/Weight':<12} {'Size (GB)':<10} {'vs F32'}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    f32_size = param_count * 32 / 8 / (1024**3)\n",
    "    \n",
    "    for quant_name, bits_per_weight in quantizations.items():\n",
    "        size_gb = param_count * bits_per_weight / 8 / (1024**3)\n",
    "        ratio = size_gb / f32_size\n",
    "        print(f\"{quant_name:<25} {bits_per_weight:<12.2f} {size_gb:<10.2f} {ratio:.1%}\")\n",
    "\n",
    "compare_quantization_sizes(1.1)  # TinyLlama size\n",
    "print()\n",
    "compare_quantization_sizes(7.0)  # LLaMA-2-7B size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Download and Compare Multiple Quantizations\n",
    "\n",
    "If time permits, download the same model in different quantizations and compare:\n",
    "1. File sizes\n",
    "2. Loading times\n",
    "3. Inference speeds\n",
    "4. Output quality (subjective)\n",
    "\n",
    "Note: This exercise is optional as it requires downloading multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compare two quantizations you have downloaded\n",
    "# YOUR CODE HERE (optional)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Tokenizer Exploration (10 minutes)\n",
    "\n",
    "The GGUF file also contains tokenizer information. Let's explore the tokenizer vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tokenizer information\n",
    "def get_tokenizer_info(metadata):\n",
    "    \"\"\"Extract tokenizer metadata.\"\"\"\n",
    "    tokenizer_info = {}\n",
    "    \n",
    "    for key, value in metadata.items():\n",
    "        if 'tokenizer' in key.lower():\n",
    "            tokenizer_info[key] = value\n",
    "    \n",
    "    return tokenizer_info\n",
    "\n",
    "tokenizer_info = get_tokenizer_info(metadata)\n",
    "\n",
    "print(\"=== Tokenizer Information ===\")\n",
    "for key, value in tokenizer_info.items():\n",
    "    if isinstance(value, (list, bytes)):\n",
    "        print(f\"{key}: <{type(value).__name__} of length {len(value)}>\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.1: Vocabulary Analysis\n",
    "\n",
    "If the tokenizer vocabulary is accessible, analyze:\n",
    "1. Total vocabulary size\n",
    "2. Sample tokens\n",
    "3. Special tokens (e.g., BOS, EOS, PAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze vocabulary\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# This will depend on the specific metadata structure\n",
    "# Look for keys like 'tokenizer.ggml.tokens'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Validation\n",
    "\n",
    "Run this cell to validate your lab completion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_lab():\n",
    "    \"\"\"Validate lab completion.\"\"\"\n",
    "    checks = []\n",
    "    \n",
    "    # Check 1: GGUF file loaded\n",
    "    checks.append((\"GGUF file loaded\", reader is not None))\n",
    "    \n",
    "    # Check 2: Metadata extracted\n",
    "    checks.append((\"Metadata extracted\", len(metadata) > 0))\n",
    "    \n",
    "    # Check 3: Architecture details extracted\n",
    "    checks.append((\"Architecture extracted\", \n",
    "                   n_layers is not None and n_heads is not None))\n",
    "    \n",
    "    # Check 4: Tensors analyzed\n",
    "    checks.append((\"Tensors analyzed\", len(tensors) > 0))\n",
    "    \n",
    "    # Check 5: Type distribution calculated\n",
    "    checks.append((\"Type distribution\", len(type_counts) > 0))\n",
    "    \n",
    "    # Print results\n",
    "    print(\"=== Lab Validation ===\")\n",
    "    all_passed = True\n",
    "    for check_name, passed in checks:\n",
    "        status = \"‚úì\" if passed else \"‚úó\"\n",
    "        print(f\"{status} {check_name}\")\n",
    "        if not passed:\n",
    "            all_passed = False\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    if all_passed:\n",
    "        print(\"üéâ Congratulations! You've completed Lab 2!\")\n",
    "        print(\"\\nYou now understand:\")\n",
    "        print(\"  - GGUF file format structure\")\n",
    "        print(\"  - Model metadata and architecture\")\n",
    "        print(\"  - Tensor organization and types\")\n",
    "        print(\"  - Quantization schemes and trade-offs\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Please complete all exercises before moving on.\")\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "validate_lab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Extension Challenges\n",
    "\n",
    "### Challenge 1: GGUF Converter\n",
    "Write a tool that reads a GGUF file and exports its metadata to JSON for easy inspection.\n",
    "\n",
    "### Challenge 2: Quantization Recommender\n",
    "Create a function that recommends the best quantization for a given use case (quality, size, speed).\n",
    "\n",
    "### Challenge 3: Model Comparator\n",
    "Build a tool that compares two GGUF models side-by-side showing their architecture differences.\n",
    "\n",
    "### Challenge 4: Tensor Visualizer\n",
    "Visualize the distribution of tensor sizes and types in a model using matplotlib.\n",
    "\n",
    "### Challenge 5: Memory Estimator\n",
    "Create a function that predicts the RAM required to run a model based on its GGUF metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extension Challenge: Your implementation here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "In this lab, you learned:\n",
    "\n",
    "1. **GGUF Structure**: Understanding the binary format for storing LLMs\n",
    "2. **Metadata**: How model information is stored and accessed\n",
    "3. **Tensors**: Organization and naming conventions for model weights\n",
    "4. **Quantization**: Different schemes and their size/quality trade-offs\n",
    "5. **Inspection Tools**: Using gguf-py to programmatically analyze models\n",
    "\n",
    "### Quantization Quick Reference\n",
    "\n",
    "| Quantization | Size | Quality | Use Case |\n",
    "|--------------|------|---------|----------|\n",
    "| Q4_K_M | Smallest | Good | Limited RAM, speed priority |\n",
    "| Q5_K_M | Small | Better | Balanced use |\n",
    "| Q6_K | Medium | Very good | Quality priority |\n",
    "| Q8_0 | Larger | Excellent | Maximum quality |\n",
    "| F16 | Largest | Best | Development/research |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Lab 3**: Memory profiling and KV cache optimization\n",
    "- **Module 2**: Deep dive into quantization algorithms\n",
    "- **Read**: GGUF specification document\n",
    "\n",
    "---\n",
    "\n",
    "**Lab Created By**: Agent 4 (Lab Designer)  \n",
    "**Last Updated**: 2025-11-18  \n",
    "**Feedback**: [Submit feedback](../../feedback/)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
