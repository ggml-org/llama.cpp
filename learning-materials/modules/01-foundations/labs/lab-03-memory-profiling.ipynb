{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Memory Profiling and Optimization\n",
    "\n",
    "**Module**: Module 1 - Foundations  \n",
    "**Estimated Time**: 30-45 minutes  \n",
    "**Difficulty**: Intermediate  \n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this lab, you will:\n",
    "- [ ] Profile memory usage during model loading and inference\n",
    "- [ ] Calculate theoretical memory requirements for LLMs\n",
    "- [ ] Understand the KV cache and its impact on memory\n",
    "- [ ] Experiment with different context window sizes\n",
    "- [ ] Observe memory scaling with batch size and sequence length\n",
    "- [ ] Optimize memory usage for your hardware constraints\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Lab 1 (Setup and First Inference)\n",
    "- Completed Lab 2 (GGUF Exploration)\n",
    "- Understanding of model architecture basics\n",
    "- Python programming knowledge\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "Memory is often the limiting factor when running LLMs. In this lab, you'll learn to:\n",
    "- Profile and measure actual memory usage\n",
    "- Calculate memory requirements before loading models\n",
    "- Understand the KV cache and its growth\n",
    "- Optimize for your available RAM\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Memory Monitoring (10 minutes)\n",
    "\n",
    "First, let's set up tools for monitoring memory usage in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install psutil memory_profiler matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Set up matplotlib\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"‚úì All packages imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get current memory usage\n",
    "def get_memory_usage():\n",
    "    \"\"\"\n",
    "    Get current process memory usage in MB.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Memory usage statistics\n",
    "    \"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    \n",
    "    return {\n",
    "        'rss_mb': mem_info.rss / (1024 ** 2),  # Resident Set Size\n",
    "        'vms_mb': mem_info.vms / (1024 ** 2),  # Virtual Memory Size\n",
    "        'percent': process.memory_percent()\n",
    "    }\n",
    "\n",
    "# Test it\n",
    "initial_memory = get_memory_usage()\n",
    "print(f\"Current memory usage:\")\n",
    "print(f\"  RSS: {initial_memory['rss_mb']:.2f} MB\")\n",
    "print(f\"  VMS: {initial_memory['vms_mb']:.2f} MB\")\n",
    "print(f\"  Percent: {initial_memory['percent']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory tracking context manager\n",
    "class MemoryTracker:\n",
    "    \"\"\"Context manager for tracking memory usage.\"\"\"\n",
    "    \n",
    "    def __init__(self, name=\"Operation\", interval=0.1):\n",
    "        self.name = name\n",
    "        self.interval = interval\n",
    "        self.memory_samples = []\n",
    "        self.timestamps = []\n",
    "        self.start_memory = None\n",
    "        self.peak_memory = 0\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.start_memory = get_memory_usage()['rss_mb']\n",
    "        self.start_time = time.time()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.end_memory = get_memory_usage()['rss_mb']\n",
    "        self.end_time = time.time()\n",
    "        self.duration = self.end_time - self.start_time\n",
    "        self.memory_delta = self.end_memory - self.start_memory\n",
    "        \n",
    "        print(f\"\\n=== {self.name} ===\")\n",
    "        print(f\"Start memory: {self.start_memory:.2f} MB\")\n",
    "        print(f\"End memory: {self.end_memory:.2f} MB\")\n",
    "        print(f\"Delta: {self.memory_delta:+.2f} MB\")\n",
    "        print(f\"Duration: {self.duration:.2f} seconds\")\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Take a memory sample.\"\"\"\n",
    "        current = get_memory_usage()['rss_mb']\n",
    "        self.memory_samples.append(current)\n",
    "        self.timestamps.append(time.time() - self.start_time)\n",
    "        self.peak_memory = max(self.peak_memory, current)\n",
    "\n",
    "# Test the tracker\n",
    "with MemoryTracker(\"Test allocation\"):\n",
    "    # Allocate some memory\n",
    "    big_array = np.zeros((1000, 1000))\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Model Loading Memory Profile (10 minutes)\n",
    "\n",
    "Let's measure how much memory is consumed when loading a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model path from previous labs\n",
    "MODEL_PATH = Path(\"./models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\")\n",
    "\n",
    "if not MODEL_PATH.exists():\n",
    "    print(f\"‚úó Model not found. Please complete Lab 1 first.\")\n",
    "else:\n",
    "    model_size_mb = MODEL_PATH.stat().st_size / (1024 ** 2)\n",
    "    print(f\"‚úì Model file size: {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure model loading\n",
    "print(\"Loading model with memory tracking...\\n\")\n",
    "\n",
    "with MemoryTracker(\"Model Loading\") as tracker:\n",
    "    llm = Llama(\n",
    "        model_path=str(MODEL_PATH),\n",
    "        n_ctx=2048,\n",
    "        n_threads=4,\n",
    "        verbose=False\n",
    "    )\n",
    "    tracker.sample()\n",
    "\n",
    "print(f\"\\nModel file size: {model_size_mb:.2f} MB\")\n",
    "print(f\"Memory overhead: {tracker.memory_delta - model_size_mb:.2f} MB\")\n",
    "print(f\"Overhead ratio: {((tracker.memory_delta / model_size_mb) - 1) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Memory Overhead\n",
    "\n",
    "The memory usage is typically larger than the model file because:\n",
    "1. **Decompression**: Quantized weights may be partially decompressed\n",
    "2. **Context Buffers**: Space for KV cache and activations\n",
    "3. **Runtime Structures**: Data structures for inference\n",
    "4. **Alignment**: Memory alignment requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Test Different Context Sizes\n",
    "\n",
    "Load the model with different context window sizes and observe memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test different context sizes\n",
    "# YOUR CODE HERE\n",
    "\n",
    "context_sizes = [512, 1024, 2048, 4096]\n",
    "memory_usage = []\n",
    "\n",
    "for ctx_size in context_sizes:\n",
    "    # Clear previous model\n",
    "    if 'llm' in locals():\n",
    "        del llm\n",
    "    \n",
    "    # Force garbage collection\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Load with new context size\n",
    "    # Measure memory\n",
    "    # Store results\n",
    "    pass\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Plot memory usage vs context size\n",
    "plt.xlabel('Context Size (tokens)')\n",
    "plt.ylabel('Memory Usage (MB)')\n",
    "plt.title('Memory Usage vs Context Window Size')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Understanding KV Cache (15 minutes)\n",
    "\n",
    "### What is the KV Cache?\n",
    "\n",
    "The KV (Key-Value) cache stores attention keys and values from previous tokens to avoid recomputing them. This is crucial for efficient autoregressive generation.\n",
    "\n",
    "### KV Cache Memory Formula\n",
    "\n",
    "```\n",
    "KV_cache_size = 2 √ó n_layers √ó n_heads √ó d_head √ó seq_len √ó bytes_per_element\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **2**: For both keys and values\n",
    "- **n_layers**: Number of transformer layers\n",
    "- **n_heads**: Number of attention heads\n",
    "- **d_head**: Dimension per head (typically hidden_size / n_heads)\n",
    "- **seq_len**: Sequence length (context window)\n",
    "- **bytes_per_element**: Typically 2 (FP16) or 4 (FP32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_kv_cache_size(\n",
    "    n_layers,\n",
    "    n_heads,\n",
    "    hidden_size,\n",
    "    seq_len,\n",
    "    bytes_per_element=2\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate KV cache size in MB.\n",
    "    \n",
    "    Args:\n",
    "        n_layers: Number of transformer layers\n",
    "        n_heads: Number of attention heads\n",
    "        hidden_size: Hidden dimension size\n",
    "        seq_len: Sequence length / context window\n",
    "        bytes_per_element: Bytes per element (2 for FP16, 4 for FP32)\n",
    "    \n",
    "    Returns:\n",
    "        float: KV cache size in MB\n",
    "    \"\"\"\n",
    "    d_head = hidden_size // n_heads\n",
    "    \n",
    "    # 2 for K and V, multiply by all dimensions\n",
    "    total_elements = 2 * n_layers * n_heads * d_head * seq_len\n",
    "    total_bytes = total_elements * bytes_per_element\n",
    "    total_mb = total_bytes / (1024 ** 2)\n",
    "    \n",
    "    return total_mb\n",
    "\n",
    "# Example: TinyLlama-1.1B\n",
    "tinyllama_params = {\n",
    "    'n_layers': 22,\n",
    "    'n_heads': 32,\n",
    "    'hidden_size': 2048,\n",
    "    'seq_len': 2048\n",
    "}\n",
    "\n",
    "kv_cache_mb = calculate_kv_cache_size(**tinyllama_params)\n",
    "print(f\"TinyLlama KV cache (2048 tokens, FP16): {kv_cache_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Calculate KV Cache for Different Models\n",
    "\n",
    "Calculate the KV cache size for different model sizes and context lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate KV cache for various configurations\n",
    "# YOUR CODE HERE\n",
    "\n",
    "models = {\n",
    "    'TinyLlama-1.1B': {'n_layers': 22, 'n_heads': 32, 'hidden_size': 2048},\n",
    "    'LLaMA-2-7B': {'n_layers': 32, 'n_heads': 32, 'hidden_size': 4096},\n",
    "    'LLaMA-2-13B': {'n_layers': 40, 'n_heads': 40, 'hidden_size': 5120},\n",
    "    'LLaMA-2-70B': {'n_layers': 80, 'n_heads': 64, 'hidden_size': 8192},\n",
    "}\n",
    "\n",
    "context_lengths = [512, 1024, 2048, 4096, 8192]\n",
    "\n",
    "# Create a table showing KV cache sizes\n",
    "print(\"=== KV Cache Size (MB) - FP16 ===\")\n",
    "print(f\"{'Model':<20}\", end=\"\")\n",
    "for ctx in context_lengths:\n",
    "    print(f\"{ctx:>10}\", end=\"\")\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "\n",
    "for model_name, params in models.items():\n",
    "    print(f\"{model_name:<20}\", end=\"\")\n",
    "    for ctx_len in context_lengths:\n",
    "        # Calculate KV cache size\n",
    "        size = calculate_kv_cache_size(\n",
    "            params['n_layers'],\n",
    "            params['n_heads'],\n",
    "            params['hidden_size'],\n",
    "            ctx_len\n",
    "        )\n",
    "        print(f\"{size:>10.1f}\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Total Memory Calculator\n",
    "\n",
    "Create a comprehensive memory calculator that estimates total memory requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_total_memory(\n",
    "    param_count_billions,\n",
    "    bits_per_weight,\n",
    "    n_layers,\n",
    "    n_heads,\n",
    "    hidden_size,\n",
    "    seq_len,\n",
    "    batch_size=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Estimate total memory required to run a model.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Memory breakdown in MB\n",
    "    \"\"\"\n",
    "    # TODO: Implement comprehensive memory calculation\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # 1. Model weights\n",
    "    model_weights_mb = (param_count_billions * 1e9 * bits_per_weight / 8) / (1024 ** 2)\n",
    "    \n",
    "    # 2. KV cache\n",
    "    kv_cache_mb = calculate_kv_cache_size(\n",
    "        n_layers, n_heads, hidden_size, seq_len\n",
    "    ) * batch_size\n",
    "    \n",
    "    # 3. Activation memory (approximate)\n",
    "    # Rule of thumb: ~2x hidden_size per layer per token\n",
    "    activation_mb = (2 * n_layers * hidden_size * seq_len * 2 * batch_size) / (1024 ** 2)\n",
    "    \n",
    "    # 4. Overhead (runtime structures, etc.)\n",
    "    overhead_mb = model_weights_mb * 0.1  # ~10% overhead\n",
    "    \n",
    "    total_mb = model_weights_mb + kv_cache_mb + activation_mb + overhead_mb\n",
    "    \n",
    "    return {\n",
    "        'model_weights_mb': model_weights_mb,\n",
    "        'kv_cache_mb': kv_cache_mb,\n",
    "        'activation_mb': activation_mb,\n",
    "        'overhead_mb': overhead_mb,\n",
    "        'total_mb': total_mb,\n",
    "        'total_gb': total_mb / 1024\n",
    "    }\n",
    "\n",
    "# Test with TinyLlama\n",
    "memory = estimate_total_memory(\n",
    "    param_count_billions=1.1,\n",
    "    bits_per_weight=4.85,  # Q4_K_M\n",
    "    n_layers=22,\n",
    "    n_heads=32,\n",
    "    hidden_size=2048,\n",
    "    seq_len=2048,\n",
    "    batch_size=1\n",
    ")\n",
    "\n",
    "print(\"=== TinyLlama-1.1B Q4_K_M Memory Estimate ===\")\n",
    "print(f\"Model Weights: {memory['model_weights_mb']:.2f} MB\")\n",
    "print(f\"KV Cache: {memory['kv_cache_mb']:.2f} MB\")\n",
    "print(f\"Activations: {memory['activation_mb']:.2f} MB\")\n",
    "print(f\"Overhead: {memory['overhead_mb']:.2f} MB\")\n",
    "print(f\"‚îÄ\" * 50)\n",
    "print(f\"Total: {memory['total_mb']:.2f} MB ({memory['total_gb']:.2f} GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Inference Memory Profiling (10 minutes)\n",
    "\n",
    "Let's observe how memory changes during actual inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile memory during inference with different sequence lengths\n",
    "def profile_inference(prompt, max_tokens, label=\"Inference\"):\n",
    "    \"\"\"Profile memory usage during inference.\"\"\"\n",
    "    with MemoryTracker(label) as tracker:\n",
    "        tracker.sample()  # Before generation\n",
    "        \n",
    "        output = llm(\n",
    "            prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "            echo=False\n",
    "        )\n",
    "        \n",
    "        tracker.sample()  # After generation\n",
    "    \n",
    "    return {\n",
    "        'memory_delta': tracker.memory_delta,\n",
    "        'duration': tracker.duration,\n",
    "        'tokens': output['usage']['completion_tokens']\n",
    "    }\n",
    "\n",
    "# Test with different generation lengths\n",
    "prompt = \"Explain machine learning:\"\n",
    "\n",
    "print(\"Profiling inference with different token counts...\\n\")\n",
    "\n",
    "for max_tokens in [50, 100, 200, 500]:\n",
    "    result = profile_inference(prompt, max_tokens, f\"Generation ({max_tokens} tokens)\")\n",
    "    print(f\"Tokens/MB: {result['tokens'] / max(result['memory_delta'], 0.1):.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Batch Size Impact\n",
    "\n",
    "Investigate how memory scales with batch size (if your llama.cpp version supports batching)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: llama-cpp-python may not directly expose batch inference\n",
    "# This exercise demonstrates the concept\n",
    "\n",
    "# TODO: Profile multiple sequential inferences to simulate batching\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Simulate batch processing\n",
    "prompts = [\n",
    "    \"What is AI?\",\n",
    "    \"Explain neural networks:\",\n",
    "    \"What is deep learning?\",\n",
    "    \"Define machine learning:\"\n",
    "]\n",
    "\n",
    "# Process and measure memory for different \"batch sizes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Memory Optimization Strategies (5 minutes)\n",
    "\n",
    "Let's explore strategies to reduce memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy comparison function\n",
    "def compare_memory_strategies():\n",
    "    \"\"\"Compare different memory optimization strategies.\"\"\"\n",
    "    \n",
    "    strategies = []\n",
    "    \n",
    "    # Strategy 1: Baseline (2048 context)\n",
    "    mem1 = estimate_total_memory(\n",
    "        1.1, 4.85, 22, 32, 2048, 2048\n",
    "    )\n",
    "    strategies.append((\"Baseline (2048 ctx)\", mem1['total_mb']))\n",
    "    \n",
    "    # Strategy 2: Reduced context\n",
    "    mem2 = estimate_total_memory(\n",
    "        1.1, 4.85, 22, 32, 2048, 1024\n",
    "    )\n",
    "    strategies.append((\"Reduced context (1024)\", mem2['total_mb']))\n",
    "    \n",
    "    # Strategy 3: More aggressive quantization (Q4_0)\n",
    "    mem3 = estimate_total_memory(\n",
    "        1.1, 4.5, 22, 32, 2048, 2048\n",
    "    )\n",
    "    strategies.append((\"Q4_0 quantization\", mem3['total_mb']))\n",
    "    \n",
    "    # Strategy 4: Both optimizations\n",
    "    mem4 = estimate_total_memory(\n",
    "        1.1, 4.5, 22, 32, 2048, 1024\n",
    "    )\n",
    "    strategies.append((\"Q4_0 + reduced ctx\", mem4['total_mb']))\n",
    "    \n",
    "    # Print comparison\n",
    "    print(\"=== Memory Optimization Strategies ===\")\n",
    "    baseline = strategies[0][1]\n",
    "    \n",
    "    for name, memory in strategies:\n",
    "        saving = ((baseline - memory) / baseline) * 100\n",
    "        print(f\"{name:<25} {memory:>8.2f} MB   {saving:>6.1f}% saving\")\n",
    "\n",
    "compare_memory_strategies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Optimization Checklist\n",
    "\n",
    "To reduce memory usage:\n",
    "\n",
    "1. **Use aggressive quantization** (Q4_0, Q4_K_M)\n",
    "2. **Reduce context window** (n_ctx parameter)\n",
    "3. **Smaller model variant** (7B instead of 13B)\n",
    "4. **Optimize batch size** (smaller batches)\n",
    "5. **Enable memory mapping** (mmap in llama.cpp)\n",
    "6. **Reduce thread count** (less overhead)\n",
    "7. **Close other applications** (free system RAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Validation\n",
    "\n",
    "Run this cell to validate your lab completion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_lab():\n",
    "    \"\"\"Validate lab completion.\"\"\"\n",
    "    checks = []\n",
    "    \n",
    "    # Check 1: Memory tracking works\n",
    "    checks.append((\"Memory tracking\", 'get_memory_usage' in dir()))\n",
    "    \n",
    "    # Check 2: Model loaded\n",
    "    checks.append((\"Model loaded\", 'llm' in dir()))\n",
    "    \n",
    "    # Check 3: KV cache calculator exists\n",
    "    checks.append((\"KV cache calculator\", 'calculate_kv_cache_size' in dir()))\n",
    "    \n",
    "    # Check 4: Memory estimator exists\n",
    "    checks.append((\"Memory estimator\", 'estimate_total_memory' in dir()))\n",
    "    \n",
    "    # Check 5: Profiling completed\n",
    "    checks.append((\"Inference profiling\", 'profile_inference' in dir()))\n",
    "    \n",
    "    # Print results\n",
    "    print(\"=== Lab Validation ===\")\n",
    "    all_passed = True\n",
    "    for check_name, passed in checks:\n",
    "        status = \"‚úì\" if passed else \"‚úó\"\n",
    "        print(f\"{status} {check_name}\")\n",
    "        if not passed:\n",
    "            all_passed = False\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    if all_passed:\n",
    "        print(\"üéâ Congratulations! You've completed Lab 3!\")\n",
    "        print(\"\\nYou now understand:\")\n",
    "        print(\"  - How to profile memory usage\")\n",
    "        print(\"  - KV cache and its memory impact\")\n",
    "        print(\"  - Memory requirements calculation\")\n",
    "        print(\"  - Optimization strategies for limited RAM\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Please complete all exercises before moving on.\")\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "validate_lab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Extension Challenges\n",
    "\n",
    "### Challenge 1: Real-time Memory Monitor\n",
    "Build a real-time memory monitoring dashboard that tracks memory during long generations.\n",
    "\n",
    "### Challenge 2: Memory Budget Optimizer\n",
    "Create a tool that, given available RAM, recommends the largest model and context size that will fit.\n",
    "\n",
    "### Challenge 3: Batch Processing Simulator\n",
    "Implement a batch processing system that maximizes throughput within memory constraints.\n",
    "\n",
    "### Challenge 4: Multi-Model Memory Analyzer\n",
    "Compare memory usage across different model architectures (LLaMA vs. GPT vs. Mistral).\n",
    "\n",
    "### Challenge 5: Memory Leak Detective\n",
    "Build a tool to detect and visualize memory leaks during extended inference sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extension Challenge: Your implementation here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "In this lab, you learned:\n",
    "\n",
    "1. **Memory Profiling**: How to monitor and measure memory usage in Python\n",
    "2. **KV Cache**: The largest variable memory component in LLM inference\n",
    "3. **Memory Calculation**: Formulas to estimate memory requirements\n",
    "4. **Scaling Factors**: How batch size and context length affect memory\n",
    "5. **Optimization**: Strategies to reduce memory usage\n",
    "\n",
    "### Memory Budget Quick Reference (FP16)\n",
    "\n",
    "| Model Size | Q4_K_M | Context | KV Cache | Total RAM Needed |\n",
    "|------------|--------|---------|----------|------------------|\n",
    "| 1.1B | ~700 MB | 2048 | ~180 MB | ~1.5 GB |\n",
    "| 7B | ~4.5 GB | 2048 | ~1.0 GB | ~7 GB |\n",
    "| 13B | ~8.5 GB | 2048 | ~1.9 GB | ~12 GB |\n",
    "| 70B | ~42 GB | 2048 | ~10 GB | ~60 GB |\n",
    "\n",
    "*Note: These are approximate values. Actual usage may vary.*\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Module 2**: Advanced quantization techniques\n",
    "- **Module 3**: GPU acceleration and optimization\n",
    "- **Module 4**: Production deployment strategies\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "**Out of memory errors**: Reduce context size, use more aggressive quantization, or choose a smaller model.\n",
    "\n",
    "**Slow inference**: May be swapping to disk. Check system RAM usage and close other applications.\n",
    "\n",
    "**Memory not freed**: Python garbage collection may be delayed. Use `gc.collect()` and `del` explicitly.\n",
    "\n",
    "---\n",
    "\n",
    "**Lab Created By**: Agent 4 (Lab Designer)  \n",
    "**Last Updated**: 2025-11-18  \n",
    "**Feedback**: [Submit feedback](../../feedback/)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
