{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Setup and First Inference\n",
    "\n",
    "**Module**: Module 1 - Foundations  \n",
    "**Estimated Time**: 30-45 minutes  \n",
    "**Difficulty**: Beginner  \n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this lab, you will:\n",
    "- [ ] Install llama.cpp and its Python bindings successfully\n",
    "- [ ] Download and set up your first language model\n",
    "- [ ] Execute basic inference and generate text\n",
    "- [ ] Understand and experiment with key inference parameters\n",
    "- [ ] Observe the impact of parameters on generation quality and speed\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python 3.8 or higher installed\n",
    "- Basic Python programming knowledge\n",
    "- At least 4GB of free disk space\n",
    "- Internet connection for downloading models\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "In this lab, you'll set up a complete local LLM inference environment and run your first text generation. You'll experiment with different parameters to understand how they affect the model's behavior.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup (10 minutes)\n",
    "\n",
    "Let's start by setting up the necessary tools. We'll install the llama-cpp-python bindings, which provide a convenient Python interface to llama.cpp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Python version\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "\n",
    "# Verify we're running Python 3.8+\n",
    "assert sys.version_info >= (3, 8), \"Python 3.8 or higher is required\"\n",
    "print(\"‚úì Python version check passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install llama-cpp-python\n",
    "# Note: This may take a few minutes as it compiles C++ code\n",
    "!pip install llama-cpp-python --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installation\n",
    "try:\n",
    "    import llama_cpp\n",
    "    print(f\"‚úì llama-cpp-python version: {llama_cpp.__version__}\")\n",
    "    print(\"‚úì Installation successful!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚úó Installation failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Install Additional Dependencies\n",
    "\n",
    "Install the following helpful packages that we'll use for monitoring and analysis:\n",
    "- `psutil` for memory monitoring\n",
    "- `tqdm` for progress bars\n",
    "- `requests` for downloading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Install psutil, tqdm, and requests\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-grading cell - DO NOT MODIFY\n",
    "def test_dependencies():\n",
    "    try:\n",
    "        import psutil\n",
    "        import tqdm\n",
    "        import requests\n",
    "        print(\"‚úì All dependencies installed successfully!\")\n",
    "        return True\n",
    "    except ImportError as e:\n",
    "        print(f\"‚úó Missing dependency: {e}\")\n",
    "        return False\n",
    "\n",
    "test_dependencies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Download a Model (10 minutes)\n",
    "\n",
    "For this lab, we'll use a small quantized model that's perfect for learning. We'll download **TinyLlama-1.1B-Chat** in Q4_K_M quantization (~700MB).\n",
    "\n",
    "### Understanding Model Naming\n",
    "\n",
    "Model files follow this pattern: `{model-name}-{size}-{variant}.{quantization}.gguf`\n",
    "\n",
    "- **TinyLlama**: The model family\n",
    "- **1.1B**: Number of parameters (1.1 billion)\n",
    "- **Chat**: Fine-tuned for chat/instruction following\n",
    "- **Q4_K_M**: Quantization type (4-bit, K-quant, Medium)\n",
    "- **.gguf**: File format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create models directory\n",
    "models_dir = Path(\"./models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Model information\n",
    "MODEL_URL = \"https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n",
    "MODEL_FILE = models_dir / \"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n",
    "\n",
    "print(f\"Model will be saved to: {MODEL_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model with progress bar\n",
    "def download_model(url, output_path):\n",
    "    \"\"\"Download a model file with progress bar.\"\"\"\n",
    "    if output_path.exists():\n",
    "        print(f\"‚úì Model already exists at {output_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Downloading model from {url}...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    with open(output_path, 'wb') as f, tqdm(\n",
    "        desc=output_path.name,\n",
    "        total=total_size,\n",
    "        unit='iB',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as pbar:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            size = f.write(chunk)\n",
    "            pbar.update(size)\n",
    "    \n",
    "    print(f\"‚úì Download complete!\")\n",
    "\n",
    "# Note: For classroom/workshop settings, you may want to:\n",
    "# 1. Pre-download models to a shared location\n",
    "# 2. Copy from a local cache\n",
    "# 3. Use a smaller model if bandwidth is limited\n",
    "\n",
    "download_model(MODEL_URL, MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the model file\n",
    "if MODEL_FILE.exists():\n",
    "    size_mb = MODEL_FILE.stat().st_size / (1024 * 1024)\n",
    "    print(f\"‚úì Model file exists\")\n",
    "    print(f\"‚úì File size: {size_mb:.2f} MB\")\n",
    "else:\n",
    "    print(\"‚úó Model file not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Load the Model and Run First Inference (10 minutes)\n",
    "\n",
    "Now for the exciting part - let's load the model and generate some text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import time\n",
    "\n",
    "# Load the model\n",
    "print(\"Loading model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=str(MODEL_FILE),\n",
    "    n_ctx=2048,  # Context window size\n",
    "    n_threads=4,  # Number of CPU threads\n",
    "    verbose=False  # Set to True to see detailed loading info\n",
    ")\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "print(f\"‚úì Model loaded in {load_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your first inference!\n",
    "prompt = \"What is machine learning?\"\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(\"Generating response...\\n\")\n",
    "\n",
    "output = llm(\n",
    "    prompt,\n",
    "    max_tokens=100,  # Maximum tokens to generate\n",
    "    temperature=0.7,  # Randomness (0.0 = deterministic, 2.0 = very random)\n",
    "    top_p=0.9,       # Nucleus sampling\n",
    "    echo=False       # Don't include prompt in output\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(output['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Output\n",
    "\n",
    "The output is a dictionary containing:\n",
    "- `choices`: List of generated completions (usually just one)\n",
    "- `usage`: Token usage statistics\n",
    "- Other metadata\n",
    "\n",
    "Let's explore the full output structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Pretty print the output structure\n",
    "print(json.dumps(output, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Extract Key Metrics\n",
    "\n",
    "From the output dictionary, extract and display:\n",
    "1. The generated text\n",
    "2. The number of tokens generated\n",
    "3. The finish reason (why generation stopped)\n",
    "4. The total tokens used (prompt + completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Extract and print the metrics\n",
    "# YOUR CODE HERE\n",
    "\n",
    "generated_text = None  # Extract from output\n",
    "tokens_generated = None  # Extract from output['usage']\n",
    "finish_reason = None  # Extract from output['choices'][0]\n",
    "total_tokens = None  # Extract from output['usage']\n",
    "\n",
    "print(f\"Generated Text: {generated_text}\")\n",
    "print(f\"Tokens Generated: {tokens_generated}\")\n",
    "print(f\"Finish Reason: {finish_reason}\")\n",
    "print(f\"Total Tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-grading cell - DO NOT MODIFY\n",
    "def test_metrics_extraction():\n",
    "    assert generated_text is not None and len(generated_text) > 0, \"Generated text not extracted\"\n",
    "    assert isinstance(tokens_generated, int) and tokens_generated > 0, \"Tokens generated not extracted\"\n",
    "    assert finish_reason is not None, \"Finish reason not extracted\"\n",
    "    assert isinstance(total_tokens, int) and total_tokens > tokens_generated, \"Total tokens not extracted\"\n",
    "    print(\"‚úì All metrics extracted correctly!\")\n",
    "    return True\n",
    "\n",
    "test_metrics_extraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Experiment with Parameters (15 minutes)\n",
    "\n",
    "Now let's experiment with different parameters to understand their effects. We'll focus on the three most important parameters:\n",
    "\n",
    "1. **temperature**: Controls randomness (0.0 = deterministic, 2.0 = very random)\n",
    "2. **top_p**: Nucleus sampling - considers tokens with cumulative probability p\n",
    "3. **max_tokens**: Maximum length of generated text\n",
    "\n",
    "### Understanding Temperature\n",
    "\n",
    "Temperature affects the probability distribution:\n",
    "- **Low (0.0-0.3)**: More focused, deterministic, repetitive\n",
    "- **Medium (0.7-1.0)**: Balanced creativity and coherence\n",
    "- **High (1.5-2.0)**: More random, creative, potentially incoherent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for clean generation\n",
    "def generate_text(prompt, temperature=0.7, top_p=0.9, max_tokens=50):\n",
    "    \"\"\"Generate text with specified parameters.\"\"\"\n",
    "    output = llm(\n",
    "        prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        echo=False\n",
    "    )\n",
    "    return output['choices'][0]['text'].strip()\n",
    "\n",
    "# Test it\n",
    "test_prompt = \"The best thing about AI is\"\n",
    "result = generate_text(test_prompt)\n",
    "print(f\"Prompt: {test_prompt}\")\n",
    "print(f\"Response: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Temperature Comparison\n",
    "\n",
    "Generate responses to the same prompt with three different temperatures and observe the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explain quantum computing in one sentence:\"\n",
    "temperatures = [0.1, 0.7, 1.5]\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "# TODO: Generate responses with different temperatures\n",
    "# YOUR CODE HERE\n",
    "\n",
    "for temp in temperatures:\n",
    "    # Generate response\n",
    "    response = None  # Call generate_text with appropriate temperature\n",
    "    print(f\"Temperature {temp}:\")\n",
    "    print(f\"{response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2: Measuring Generation Speed\n",
    "\n",
    "Implement a function that measures tokens per second during generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_generation_speed(prompt, max_tokens=100):\n",
    "    \"\"\"\n",
    "    Generate text and measure performance.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains 'text', 'tokens', 'time_seconds', 'tokens_per_second'\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # Generate text\n",
    "    # Measure time\n",
    "    # Calculate tokens per second\n",
    "    \n",
    "    return {\n",
    "        'text': None,\n",
    "        'tokens': None,\n",
    "        'time_seconds': None,\n",
    "        'tokens_per_second': None\n",
    "    }\n",
    "\n",
    "# Test your implementation\n",
    "result = measure_generation_speed(\"Write a short poem about AI:\", max_tokens=50)\n",
    "print(f\"Generated {result['tokens']} tokens in {result['time_seconds']:.2f} seconds\")\n",
    "print(f\"Speed: {result['tokens_per_second']:.2f} tokens/second\")\n",
    "print(f\"\\nText: {result['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.3: Batch Testing\n",
    "\n",
    "Create a function that tests multiple prompts and compares their generation characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts = [\n",
    "    \"What is Python?\",\n",
    "    \"Explain neural networks:\",\n",
    "    \"The future of AI will\",\n",
    "    \"In one sentence, describe how LLMs work:\"\n",
    "]\n",
    "\n",
    "# TODO: Generate responses for all prompts and create a summary\n",
    "# YOUR CODE HERE\n",
    "\n",
    "results = []\n",
    "for prompt in test_prompts:\n",
    "    # Generate and measure\n",
    "    # Store results\n",
    "    pass\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n=== Generation Summary ===\")\n",
    "# Print average tokens/second, total tokens, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Advanced Parameter Exploration (Optional)\n",
    "\n",
    "### Understanding More Parameters\n",
    "\n",
    "llama.cpp supports many more parameters:\n",
    "\n",
    "- **repeat_penalty**: Penalize repeated tokens (1.0 = no penalty, >1.0 = penalize)\n",
    "- **top_k**: Consider only the top K tokens (0 = disabled)\n",
    "- **stop**: Stop sequences to end generation\n",
    "- **stream**: Stream tokens as they're generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using stop sequences\n",
    "prompt = \"Count from 1 to 10:\\n1\\n2\\n3\\n\"\n",
    "\n",
    "output = llm(\n",
    "    prompt,\n",
    "    max_tokens=50,\n",
    "    temperature=0.1,\n",
    "    stop=[\"\\n8\"],  # Stop at 8\n",
    "    echo=False\n",
    ")\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\nGeneration (stops at 8):\\n{output['choices'][0]['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Streaming generation\n",
    "prompt = \"The three laws of robotics are:\"\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(\"Streaming response:\")\n",
    "\n",
    "stream = llm(\n",
    "    prompt,\n",
    "    max_tokens=100,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for output in stream:\n",
    "    text = output['choices'][0]['text']\n",
    "    print(text, end='', flush=True)\n",
    "\n",
    "print(\"\\n\\n‚úì Streaming complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Validation\n",
    "\n",
    "Run this cell to validate you've completed all required exercises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_lab():\n",
    "    \"\"\"Validate lab completion.\"\"\"\n",
    "    checks = []\n",
    "    \n",
    "    # Check 1: Dependencies installed\n",
    "    try:\n",
    "        import llama_cpp\n",
    "        import psutil\n",
    "        import tqdm\n",
    "        import requests\n",
    "        checks.append((\"Dependencies installed\", True))\n",
    "    except ImportError:\n",
    "        checks.append((\"Dependencies installed\", False))\n",
    "    \n",
    "    # Check 2: Model downloaded\n",
    "    checks.append((\"Model downloaded\", MODEL_FILE.exists()))\n",
    "    \n",
    "    # Check 3: Model loaded\n",
    "    checks.append((\"Model loaded\", llm is not None))\n",
    "    \n",
    "    # Check 4: Basic inference works\n",
    "    try:\n",
    "        test_output = llm(\"Test\", max_tokens=5)\n",
    "        checks.append((\"Basic inference\", True))\n",
    "    except:\n",
    "        checks.append((\"Basic inference\", False))\n",
    "    \n",
    "    # Print results\n",
    "    print(\"=== Lab Validation ===\")\n",
    "    all_passed = True\n",
    "    for check_name, passed in checks:\n",
    "        status = \"‚úì\" if passed else \"‚úó\"\n",
    "        print(f\"{status} {check_name}\")\n",
    "        if not passed:\n",
    "            all_passed = False\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    if all_passed:\n",
    "        print(\"üéâ Congratulations! You've completed Lab 1!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Please complete all exercises before moving on.\")\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "validate_lab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Extension Challenges\n",
    "\n",
    "Ready for more? Try these challenges:\n",
    "\n",
    "### Challenge 1: Parameter Optimizer\n",
    "Create a function that automatically finds the optimal temperature for a given task by testing multiple values and measuring output quality.\n",
    "\n",
    "### Challenge 2: Prompt Templates\n",
    "Implement a prompt template system that formats prompts for different tasks (Q&A, summarization, code generation).\n",
    "\n",
    "### Challenge 3: Response Caching\n",
    "Build a simple cache system that stores previous responses to avoid re-generating identical prompts.\n",
    "\n",
    "### Challenge 4: Multi-Model Comparison\n",
    "If you have multiple models, create a comparison tool that generates the same prompt with different models and compares results.\n",
    "\n",
    "### Challenge 5: Memory Monitor\n",
    "Use `psutil` to monitor and log memory usage during model loading and inference. Create a visualization of memory consumption over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extension Challenge: Your implementation here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "In this lab, you learned:\n",
    "\n",
    "1. **Setup**: How to install llama.cpp Python bindings and download models\n",
    "2. **Basic Inference**: Loading models and generating text\n",
    "3. **Parameters**: Understanding temperature, top_p, and max_tokens\n",
    "4. **Performance**: Measuring tokens per second and generation speed\n",
    "5. **Advanced Features**: Streaming generation and stop sequences\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Lab 2**: Explore GGUF format internals and quantization\n",
    "- **Lab 3**: Memory profiling and optimization\n",
    "- **Module 1 Docs**: Read about model architecture and inference pipelines\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "**Model loading is slow**: This is normal for the first load. Subsequent loads may be faster due to OS caching.\n",
    "\n",
    "**Out of memory**: Try a smaller model or reduce `n_ctx` parameter.\n",
    "\n",
    "**Installation fails**: Make sure you have C++ build tools installed. Check llama-cpp-python documentation for platform-specific instructions.\n",
    "\n",
    "---\n",
    "\n",
    "**Lab Created By**: Agent 4 (Lab Designer)  \n",
    "**Last Updated**: 2025-11-18  \n",
    "**Feedback**: [Submit feedback](../../feedback/)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
