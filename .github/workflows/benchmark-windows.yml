name: Windows Self-Hosted Benchmarks

on:
  push:
    branches:
      - feat/ultra-low-bit-quantization

env:
  MODEL_REPO: 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'
  MODEL_NAME: 'tinyllama-1.1b'

jobs:
  benchmark-ryzen:
    runs-on: [self-hosted, Windows, X64]
    steps:
      - uses: actions/checkout@v4

      - name: Install MSYS2, CMake, and GCC
        uses: msys2/setup-msys2@v2
        with:
          msystem: UCRT64
          update: true
          install: >-
            git
            make
            mingw-w64-ucrt-x86_64-gcc
            mingw-w64-ucrt-x86_64-cmake

      - name: Install Python dependencies
        run: |
          pip install torch transformers sentencepiece protobuf safetensors huggingface_hub[cli]

      - name: Build llama.cpp
        shell: msys2 {0}
        run: |
          cmake -B build
          cmake --build build --config Release -j 12
          
          mkdir -p bin
          find build -type f \( -name "llama-quantize.exe" -o -name "llama-quantize" \) -exec cp {} bin/llama-quantize.exe \; || true
          find build -type f \( -name "llama-perplexity.exe" -o -name "llama-perplexity" \) -exec cp {} bin/llama-perplexity.exe \; || true
          find build -type f \( -name "llama-bench.exe" -o -name "llama-bench" \) -exec cp {} bin/llama-bench.exe \; || true

      - name: Download model and data
        run: |
          python -c "from huggingface_hub import snapshot_download; snapshot_download(repo_id='${{ env.MODEL_REPO }}', local_dir='models/hf-model')"
          python convert_hf_to_gguf.py models/hf-model --outfile models\model-F16.gguf --outtype f16
          
          Invoke-WebRequest -Uri "https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip" -OutFile "models\wikitext-2-raw.zip"
          Expand-Archive -Path "models\wikitext-2-raw.zip" -DestinationPath "models" -Force

      - name: Run Quantization
        shell: msys2 {0}
        run: |
          for type in Q8_0 Q2_K TQ1_0 TQ2_0 Q1_5_K Q2_K_S_NEW; do
            echo "Quantizing $type"
            ./bin/llama-quantize.exe models/model-F16.gguf "models/model-${type}.gguf" "$type"
          done

      - name: Run Perplexity
        shell: msys2 {0}
        run: |
          mkdir -p results
          echo "Running Baseline Perplexity"
          ./bin/llama-perplexity.exe -m models/model-Q8_0.gguf -f models/wikitext-2-raw/wiki.test.raw --save-all-logits models/logits-Q8_0.kld --chunks 256 -t 12 > results/ppl-Q8_0.txt 2>&1 || true

          for type in Q2_K TQ1_0 TQ2_0 Q1_5_K Q2_K_S_NEW; do
            echo "Running Perplexity $type"
            ./bin/llama-perplexity.exe -m "models/model-${type}.gguf" -f models/wikitext-2-raw/wiki.test.raw --kl-divergence --kl-divergence-base models/logits-Q8_0.kld --chunks 256 -t 12 > results/ppl-${type}.txt 2>&1 || true
          done

      - name: Run Benchmark
        shell: msys2 {0}
        run: |
          for type in Q8_0 Q2_K TQ1_0 TQ2_0 Q1_5_K Q2_K_S_NEW; do
            echo "Running Benchmark $type"
            ./bin/llama-bench.exe -m "models/model-${type}.gguf" -t 12 -ngl 0 -r 3 > results/bench-${type}.txt 2>&1 || true
          done

      - name: Generate Report
        shell: msys2 {0}
        run: |
          TIMESTAMP=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
          REPORT="results/benchmark-Windows-Ryzen9.md"
          
          {
            echo "# Benchmark Results (Ryzen 9 7900X)"
            echo "**Model:** ${MODEL_REPO}"
            echo "**Date:** ${TIMESTAMP}"
            echo "**Runner:** Self-Hosted Windows x64 (12 Cores)"
            echo ""
            echo "## Perplexity & KLD"
            echo "| Type | PPL | Mean KLD |"
            echo "|------|-----|----------|"
          } > $REPORT

          Q8_PPL=$(grep -a "Final estimate" results/ppl-Q8_0.txt | grep -oP 'PPL = \K[0-9.]+' || echo "N/A")
          echo "| Q8_0 | $Q8_PPL | 0 |" >> $REPORT
          
          for type in Q2_K TQ1_0 TQ2_0 Q1_5_K Q2_K_S_NEW; do
            PPL=$(grep -a "Final estimate" results/ppl-${type}.txt | grep -oP 'PPL = \K[0-9.]+' || echo "N/A")
            KLD=$(grep -a "Mean    KLD" results/ppl-${type}.txt | awk '{print $3}' || echo "N/A")
            echo "| **$type** | **$PPL** | $KLD |" >> $REPORT
          done
          
          {
            echo ""
            echo "## Performance (12 threads)"
            echo "| Type | Prompt (t/s) | Generation (t/s) |"
            echo "|------|-------------|------------------|"
          } >> $REPORT
          
          for type in Q8_0 Q2_K TQ1_0 TQ2_0 Q1_5_K Q2_K_S_NEW; do
            FILE="results/bench-${type}.txt"
            PP=$(grep -a -oP '\d+\.\d+ ± \d+\.\d+(?= t/s)' "$FILE" | head -1 || echo "N/A")
            TG=$(grep -a -oP '\d+\.\d+ ± \d+\.\d+(?= t/s)' "$FILE" | tail -1 || echo "N/A")
            echo "| $type | $PP | $TG |" >> $REPORT
          done
          
          cat $REPORT

      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: result-windows-ryzen
          path: results/
