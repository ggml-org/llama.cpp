name: Windows Self-Hosted Benchmarks

on:
  push:
    branches:
      - feat/ultra-low-bit-quantization

env:
  MODEL_REPO: 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'
  MODEL_NAME: 'tinyllama-1.1b'

jobs:
  benchmark-ryzen:
    runs-on: [self-hosted, Windows, X64]
    steps:
      - uses: actions/checkout@v4

      - name: Install MSYS2, CMake, and GCC
        uses: msys2/setup-msys2@v2
        with:
          msystem: UCRT64
          update: true
          install: >-
            git
            make
            mingw-w64-ucrt-x86_64-gcc
            mingw-w64-ucrt-x86_64-cmake

      - name: Install Python dependencies
        run: |
          pip install torch transformers sentencepiece protobuf safetensors huggingface_hub[cli]

      - name: Build llama.cpp
        shell: msys2 {0}
        run: |
          cmake -B build
          cmake --build build --config Release -j 12
          
          mkdir -p bin
          cp build/bin/llama-quantize.exe bin/ || cp build/llama-quantize.exe bin/ || cp build/bin/Release/llama-quantize.exe bin/ || true
          cp build/bin/llama-perplexity.exe bin/ || cp build/llama-perplexity.exe bin/ || cp build/bin/Release/llama-perplexity.exe bin/ || true
          cp build/bin/llama-bench.exe bin/ || cp build/llama-bench.exe bin/ || cp build/bin/Release/llama-bench.exe bin/ || true

      - name: Download model and data
        run: |
          python -c "from huggingface_hub import snapshot_download; snapshot_download(repo_id='${{ env.MODEL_REPO }}', local_dir='models/hf-model')"
          python convert_hf_to_gguf.py models/hf-model --outfile models\model-F16.gguf --outtype f16
          
          Invoke-WebRequest -Uri "https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip" -OutFile "models\wikitext-2-raw.zip"
          Expand-Archive -Path "models\wikitext-2-raw.zip" -DestinationPath "models" -Force

      - name: Run Quantization
        run: |
          $types = @("Q8_0", "Q2_K", "TQ1_0", "TQ2_0", "Q1_5_K", "Q2_K_S_NEW")
          foreach ($type in $types) {
            Write-Host "Quantizing $type"
            .\bin\llama-quantize.exe models\model-F16.gguf "models\model-${type}.gguf" $type
          }

      - name: Run Perplexity
        run: |
          New-Item -ItemType Directory -Force -Path results
          $ErrorActionPreference = "SilentlyContinue"
          
          Write-Host "Running Baseline Perplexity"
          .\bin\llama-perplexity.exe -m models\model-Q8_0.gguf -f models\wikitext-2-raw\wiki.test.raw --save-all-logits models\logits-Q8_0.kld --chunks 256 -t 12 2>&1 | Out-File -FilePath results\ppl-Q8_0.txt -Encoding utf8

          $types = @("Q2_K", "TQ1_0", "TQ2_0", "Q1_5_K", "Q2_K_S_NEW")
          foreach ($type in $types) {
            Write-Host "Running Perplexity $type"
            .\bin\llama-perplexity.exe -m "models\model-${type}.gguf" -f models\wikitext-2-raw\wiki.test.raw --kl-divergence --kl-divergence-base models\logits-Q8_0.kld --chunks 256 -t 12 2>&1 | Out-File -FilePath results\ppl-${type}.txt -Encoding utf8
          }

      - name: Run Benchmark
        run: |
          $types = @("Q8_0", "Q2_K", "TQ1_0", "TQ2_0", "Q1_5_K", "Q2_K_S_NEW")
          $ErrorActionPreference = "SilentlyContinue"
          foreach ($type in $types) {
            Write-Host "Running Benchmark $type"
            .\bin\llama-bench.exe -m "models\model-${type}.gguf" -t 12 -ngl 0 -r 3 2>&1 | Out-File -FilePath results\bench-${type}.txt -Encoding utf8
          }

      - name: Generate Report
        run: |
          $timestamp = (Get-Date).ToUniversalTime().ToString("yyyy-MM-ddTHH:mm:ssZ")
          $report = "results\benchmark-Windows-Ryzen9.md"
          
          $content = @"
          # Benchmark Results (Ryzen 9 7900X)
          **Model:** ${{ env.MODEL_REPO }}
          **Date:** $timestamp
          **Runner:** Self-Hosted Windows x64 (12 Cores)
          
          ## Perplexity & KLD
          | Type | PPL | Mean KLD |
          |------|-----|----------|
          "@
          
          $q8_content = Get-Content results\ppl-Q8_0.txt -Raw
          $q8_ppl = "N/A"
          if ($q8_content -match "PPL\s*=\s*([0-9.]+)") { $q8_ppl = $matches[1] }
          $content += "`n| Q8_0 | $q8_ppl | 0 |"
          
          $types = @("Q2_K", "TQ1_0", "TQ2_0", "Q1_5_K", "Q2_K_S_NEW")
          foreach ($type in $types) {
            $ppl_content = Get-Content results\ppl-${type}.txt -Raw
            $ppl = "N/A"
            $kld = "N/A"
            
            if ($ppl_content -match "PPL\s*=\s*([0-9.]+)") { $ppl = $matches[1] }
            if ($ppl_content -match "Mean\s*KLD\s*=\s*([0-9.]+)") { $kld = $matches[1] }
            
            $content += "`n| **$type** | **$ppl** | $kld |"
          }
          
          $content += @"
          
          ## Performance (12 threads)
          | Type | Prompt (t/s) | Generation (t/s) |
          |------|-------------|------------------|
          "@
          
          $types_all = @("Q8_0", "Q2_K", "TQ1_0", "TQ2_0", "Q1_5_K", "Q2_K_S_NEW")
          foreach ($type in $types_all) {
            $bench_content = Get-Content results\bench-${type}.txt
            $pp = "N/A"
            $tg = "N/A"
            
            $speeds = $bench_content | Select-String -Pattern "([0-9.]+\s*Â±\s*[0-9.]+)\s*t/s" -AllMatches
            if ($speeds.Matches.Count -ge 2) {
              $pp = $speeds.Matches[0].Groups[1].Value
              $tg = $speeds.Matches[$speeds.Matches.Count - 1].Groups[1].Value
            }
            
            $content += "`n| $type | $pp | $tg |"
          }
          
          Set-Content -Path $report -Value $content -Encoding utf8
          Write-Host $content

      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: result-windows-ryzen
          path: results\
