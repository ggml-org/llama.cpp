name: Build and Run LLaMA.cpp

on:
  workflow_dispatch:

jobs:
  build-and-run:
    runs-on: ubuntu-latest

    steps:
      - name: ğŸ“¥ Checkout repository
        uses: actions/checkout@v3

      - name: ğŸ› ï¸ Install dependencies
        run: |
          sudo apt update
          sudo apt install -y cmake build-essential

      - name: ğŸ—ï¸ Build llama.cpp using cmake
        run: |
          mkdir -p build
          cd build
          cmake ..
          make -j

      - name: ğŸ“ Create model folder and download model
        run: |
          mkdir -p models
          wget -O models/tinyllama-1.1b-chat-v1.0.Q8_0.gguf \
          https://raw.githubusercontent.com/issa261/github-workflows-download-model.yml/main/tinyllama-1.1b-chat-v1.0.Q8_0.gguf

      - name: ğŸš€ Run the model server
        run: |
          ./build/server -m models/tinyllama-1.1b-chat-v1.0.Q8_0.gguf -c 512 -n 256 --host 0.0.0.0 --port 8080
