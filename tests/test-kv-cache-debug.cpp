// KV Cache Debug Tool - View cell allocation and usage
//
// This tool provides in-depth analysis of KV cache internals in llama.cpp, including:
// 1. Cache cell allocation and deallocation process
// 2. Dynamic changes in tensor dimensions with token count
// 3. Memory layout for concurrent multi-sequence storage
// 4. Impact of sequence operations on cache state
//
// KV Cache Fundamentals:
// - Each transformer layer has independent K(key) and V(value) caches
// - Cache is managed in "cells", each storing K/V vectors for one token
// - Supports concurrent storage of multiple sequences, each with independent position encoding
// - Fixed cache size triggers reorganization or overwrite when full
//
// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
// â”‚                    KV Cache Architecture                        â”‚
// â”‚                                                                 â”‚
// â”‚  Layer 0:  [Kâ‚€] [Vâ‚€]     Layer 1:  [Kâ‚] [Vâ‚]                   â”‚
// â”‚            â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”              â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”                â”‚
// â”‚  Cell 0 â†’  â”‚ â€¢ â”‚ â”‚ â€¢ â”‚    Cell 0 â†’  â”‚ â€¢ â”‚ â”‚ â€¢ â”‚                â”‚
// â”‚  Cell 1 â†’  â”‚ â€¢ â”‚ â”‚ â€¢ â”‚    Cell 1 â†’  â”‚ â€¢ â”‚ â”‚ â€¢ â”‚                â”‚
// â”‚  Cell 2 â†’  â”‚ â€¢ â”‚ â”‚ â€¢ â”‚    Cell 2 â†’  â”‚ â€¢ â”‚ â”‚ â€¢ â”‚                â”‚
// â”‚  ...       â”‚...â”‚ â”‚...â”‚    ...       â”‚...â”‚ â”‚...â”‚                â”‚
// â”‚  Cell N â†’  â”‚ â€¢ â”‚ â”‚ â€¢ â”‚    Cell N â†’  â”‚ â€¢ â”‚ â”‚ â€¢ â”‚                â”‚
// â”‚            â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜              â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜                â”‚
// â”‚                                                                 â”‚
// â”‚  Each cell stores one token's K/V vectors for attention         â”‚
// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

#include "../src/llama-arch.h"
#include "../src/llama-batch.h"
#include "../src/llama-hparams.h"
#include "../src/llama-impl.h"
#include "../src/llama-kv-cache.h"
#include "../src/llama-model.h"

#include "../common/common.h"
#include "llama.h"
#include "ggml.h"

#include <algorithm>
#include <cassert>
#include <cmath>
#include <cstdio>
#include <cstring>
#include <iostream>
#include <iomanip>
#include <map>
#include <memory>
#include <numeric>
#include <random>
#include <string>
#include <vector>

/*- Helper Functions ----------------------------------------------------------*/

// Create minimal test model
// Constructs a simplified llama_model instance for KV cache testing
//
// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
// â”‚                     Model Construction                          â”‚
// â”‚                                                                 â”‚
// â”‚  Input Parameters:                                              â”‚
// â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
// â”‚  â”‚    arch     â”‚  â”‚   n_layer   â”‚  â”‚   n_head    â”‚             â”‚
// â”‚  â”‚ (LLM_ARCH_  â”‚  â”‚ (# of       â”‚  â”‚ (attention  â”‚             â”‚
// â”‚  â”‚  LLAMA)     â”‚  â”‚  layers)    â”‚  â”‚  heads)     â”‚             â”‚
// â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
// â”‚         â”‚                â”‚                â”‚                    â”‚
// â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
// â”‚                          â–¼                                     â”‚
// â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
// â”‚                 â”‚  llama_model    â”‚                            â”‚
// â”‚                 â”‚   instance      â”‚                            â”‚
// â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
static std::shared_ptr<llama_model> _make_model(
    llm_arch arch = LLM_ARCH_LLAMA,
    uint32_t n_layer = 2,
    uint32_t n_embd_head_k = 32,
    uint32_t n_embd_head_v = 32,
    uint32_t n_head = 4,
    uint32_t n_head_kv = 1) {

    llama_model_params params;
    params.tensor_buft_overrides = nullptr;
    std::shared_ptr<llama_model> model(new llama_model(params));
    model->hparams = llama_hparams();
    model->arch = arch;

    // Set model parameters that determine KV cache structure
    model->hparams.n_layer = n_layer;
    model->hparams.n_embd_head_k = n_embd_head_k;
    model->hparams.n_embd_head_v = n_embd_head_v;

    // Configure same head settings for all layers
    // In real models, different layers may have different head counts
    if (n_head > 0) {
        auto& n_head_arr = model->hparams.n_head_arr;
        std::fill(n_head_arr.begin(), n_head_arr.end(), n_head);
    }
    if (n_head_kv > 0) {
        auto& n_head_kv_arr = model->hparams.n_head_kv_arr;
        std::fill(n_head_kv_arr.begin(), n_head_kv_arr.end(), n_head_kv);
    }

    return model;
}

/*- Cache Debug Functions -----------------------------------------------------*/

// Print basic KV cache status
// Displays core metrics to understand memory usage
//
// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
// â”‚                    Cache Status Monitor                         â”‚
// â”‚                                                                 â”‚
// â”‚  Cache Metrics:                                                 â”‚
// â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
// â”‚  â”‚ Total Size  â”‚  â”‚ Current N   â”‚  â”‚ Can Shift   â”‚              â”‚
// â”‚  â”‚ (capacity)  â”‚  â”‚ (active)    â”‚  â”‚ (K-shift)   â”‚              â”‚
// â”‚  â”‚     64      â”‚  â”‚     16      â”‚  â”‚    Yes      â”‚              â”‚
// â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
// â”‚                                                                 â”‚
// â”‚  Cache Layout:                                                  â”‚
// â”‚  â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”              â”‚
// â”‚  â”‚ 0 â”‚ 1 â”‚ 2 â”‚ 3 â”‚ 4 â”‚ 5 â”‚ 6 â”‚ 7 â”‚ 8 â”‚...â”‚   â”‚63 â”‚              â”‚
// â”‚  â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜              â”‚
// â”‚   â–²                       â–²                                     â”‚
// â”‚   â”‚                       â”‚                                     â”‚
// â”‚  head                   active                                  â”‚
// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
static void print_kv_cache_status(llama_kv_cache_unified * kv_cache, const std::string & title) {
    if (!kv_cache) {
        printf("%s: No KV cache available\n", title.c_str());
        return;
    }
    
    printf("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n");
    printf("â•‘                            %-46s â•‘\n", title.c_str());
    printf("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    // get_size(): Returns total cache capacity (cell count)
    // Fixed at creation time, doesn't change dynamically
    printf("Cache Size: %u cells\n", kv_cache->get_size());
    
    // get_n(): Returns current active cache size
    // Grows with token additions, affects attention computation range
    // Note: Not equal to actual cell count, but attention window size
    printf("Current N (active): %u\n", kv_cache->get_n());
    
    // get_can_shift(): Indicates if cache supports K-shift operation
    // K-shift is an optimization allowing position encoding adjustment
    printf("Can Shift: %s\n", kv_cache->get_can_shift() ? "Yes" : "No");
    
    // Note: total_size(), size_k_bytes(), size_v_bytes() are private
    // These methods provide detailed memory usage but aren't accessible
    printf("Memory Usage: (private methods not accessible)\n");
    
    printf("\n");
}

// Analyze layer tensor structure and memory layout
// Examines detailed state of tensors in KV cache
//
// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
// â”‚                    Tensor Structure Analysis                    â”‚
// â”‚                                                                 â”‚
// â”‚  K Tensor Layout:                                               â”‚
// â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
// â”‚  â”‚ Dimension 0: n_embd_head_k (32)                         â”‚    â”‚
// â”‚  â”‚ Dimension 1: n_head_kv (1)                              â”‚    â”‚
// â”‚  â”‚ Dimension 2: sequence_length (dynamic: 0â†’8â†’16)          â”‚    â”‚
// â”‚  â”‚ Dimension 3: batch_size (1)                             â”‚    â”‚
// â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
// â”‚  â”‚ Dimension 0: n_embd_head_k (32)                         â”‚    â”‚
// â”‚  â”‚ Dimension 1: n_head_kv (1)                              â”‚    â”‚
// â”‚  â”‚ Dimension 2: sequence_length (dynamic: 0â†’8â†’16)          â”‚    â”‚
// â”‚  â”‚ Dimension 3: batch_size (1)                             â”‚    â”‚
// â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
// â”‚                                                                 â”‚
// â”‚  Memory Evolution:                                              â”‚
// â”‚  Initial:  [32, 1, 0, 1] â†’ 0 bytes                              â”‚
// â”‚  Batch 1:  [32, 1, 8, 1] â†’ 512 bytes                            â”‚
// â”‚  Batch 3:  [32, 1, 16, 1] â†’ 1024 bytes                          â”‚
// â”‚                                                                 â”‚
// â”‚  V Tensor: Same structure as K tensor                           â”‚
// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
static void print_cache_tensors_info(llama_kv_cache_unified * kv_cache, 
                                    const llama_model & model, 
                                    const std::string & title) {
    if (!kv_cache) {
        printf("%s: No KV cache available\n", title.c_str());
        return;
    }
    
    printf("\n=== %s - Tensor Information ===\n", title.c_str());
    
    // åˆ›å»ºä¸´æ—¶çš„ggml contextç”¨äºè·å–tensorè§†å›¾
    // è¿™ä¸ä¼šåˆ†é…å®é™…å†…å­˜ï¼Œåªæ˜¯ä¸ºäº†è®¿é—®tensorçš„å…ƒæ•°æ®
    ggml_init_params ctx_params = {
        /*.mem_size   =*/ 16 * 1024 * 1024,  // 16MB
        /*.mem_buffer =*/ NULL,
        /*.no_alloc   =*/ false,
    };
    ggml_context * ctx = ggml_init(ctx_params);
    
    if (!ctx) {
        printf("Failed to create ggml context\n");
        return;
    }
    
    // éå†æ¯ä¸€å±‚ï¼Œæ£€æŸ¥å…¶KV tensorçš„çŠ¶æ€
    for (int32_t il = 0; il < (int32_t)model.hparams.n_layer; ++il) {
        printf("Layer %d:\n", il);
        
        try {
            // get_k()/get_v()è¿”å›æŒ‡å‘cacheä¸­K/V tensorçš„è§†å›¾
            // è¿™äº›tensorçš„ç»´åº¦ä¼šéšç€cacheçŠ¶æ€åŠ¨æ€å˜åŒ–
            ggml_tensor * k_tensor = kv_cache->get_k(ctx, il);
            ggml_tensor * v_tensor = kv_cache->get_v(ctx, il);
            
            if (k_tensor) {
                // K tensorçš„ç»´åº¦è§£é‡Šï¼š
                // ne[0]: æ¯ä¸ªheadçš„Kå‘é‡ç»´åº¦ (n_embd_head_k)
                // ne[1]: å½“å‰å±‚çš„KV headæ•°é‡ (n_head_kv)  
                // ne[2]: å½“å‰æ´»è·ƒçš„åºåˆ—é•¿åº¦ (å¯¹åº”get_n()çš„å€¼)
                // ne[3]: batchç»´åº¦ï¼Œé€šå¸¸ä¸º1
                printf("  K tensor: [%ld, %ld, %ld, %ld] type=%s, size=%zu bytes\n",
                       k_tensor->ne[0], k_tensor->ne[1], k_tensor->ne[2], k_tensor->ne[3],
                       ggml_type_name(k_tensor->type), ggml_nbytes(k_tensor));
                
                // æ£€æŸ¥tensoræ˜¯å¦æœ‰å®é™…çš„æ•°æ®æŒ‡é’ˆ
                // NULLæŒ‡é’ˆè¡¨ç¤ºtensorè¿˜æ²¡æœ‰åˆ†é…å†…å­˜æˆ–å·²è¢«é‡Šæ”¾
                if (k_tensor->data) {
                    printf("    Data pointer: %p (has data)\n", k_tensor->data);
                } else {
                    printf("    Data pointer: NULL (no data)\n");
                }
            } else {
                printf("  K tensor: NULL\n");
            }
            
            if (v_tensor) {
                // V tensorçš„ç»´åº¦ç»“æ„ä¸K tensorç±»ä¼¼
                // ä½†æ ¹æ®v_transå‚æ•°ï¼ŒV tensorå¯èƒ½è¢«è½¬ç½®å­˜å‚¨ä»¥ä¼˜åŒ–å†…å­˜è®¿é—®
                printf("  V tensor: [%ld, %ld, %ld, %ld] type=%s, size=%zu bytes\n",
                       v_tensor->ne[0], v_tensor->ne[1], v_tensor->ne[2], v_tensor->ne[3],
                       ggml_type_name(v_tensor->type), ggml_nbytes(v_tensor));
                
                if (v_tensor->data) {
                    printf("    Data pointer: %p (has data)\n", v_tensor->data);
                } else {
                    printf("    Data pointer: NULL (no data)\n");
                }
            } else {
                printf("  V tensor: NULL\n");
            }
            
        } catch (const std::exception& e) {
            printf("  Error accessing layer %d: %s\n", il, e.what());
        }
    }
    
    ggml_free(ctx);
    printf("\n");
}

// è·Ÿè¸ªå’Œæ˜¾ç¤ºåºåˆ—åœ¨cacheä¸­çš„åˆ†å¸ƒæƒ…å†µ
// è¿™ä¸ªå‡½æ•°å¸®åŠ©ç†è§£å¤šåºåˆ—å¹¶å‘å­˜å‚¨çš„å†…å­˜å¸ƒå±€
//
// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
// â”‚                    Sequence Distribution Map                    â”‚
// â”‚                                                                 â”‚
// â”‚  Cache Cells:                                                   â”‚
// â”‚  â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”              â”‚
// â”‚  â”‚ 0 â”‚ 1 â”‚ 2 â”‚ 3 â”‚ 4 â”‚ 5 â”‚ 6 â”‚ 7 â”‚ 8 â”‚ 9 â”‚10 â”‚11 â”‚              â”‚
// â”‚  â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜              â”‚
// â”‚                                                                 â”‚
// â”‚  Sequence Mapping:                                              â”‚
// â”‚  Seq 42: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ [0,3] (4 tokens)                      â”‚
// â”‚  Seq 84: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ [4,6] (3 tokens)                      â”‚
// â”‚  Seq 126:â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ [0,3] (4 tokens, copied from 42)      â”‚
// â”‚                                                                 â”‚
// â”‚  Legend: â–ˆ = occupied, â–‘ = empty                                â”‚
// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
static void print_sequence_info(llama_kv_cache_unified * kv_cache, 
                               const std::vector<llama_seq_id> & seq_ids,
                               const std::string & title) {
    if (!kv_cache) {
        printf("%s: No KV cache available\n", title.c_str());
        return;
    }
    
    printf("\n=== %s - Sequence Information ===\n", title.c_str());
    
    for (auto seq_id : seq_ids) {
        // seq_pos_min/max()è¿”å›æŒ‡å®šåºåˆ—åœ¨cacheä¸­çš„ä½ç½®èŒƒå›´
        // è¿™äº›ä½ç½®å¯¹åº”äºtransformerä¸­çš„ç»å¯¹ä½ç½®ç¼–ç 
        llama_pos min_pos = kv_cache->seq_pos_min(seq_id);
        llama_pos max_pos = kv_cache->seq_pos_max(seq_id);
        
        printf("Sequence %d: ", seq_id);
        if (min_pos == -1 && max_pos == -1) {
            // è¿”å›-1è¡¨ç¤ºè¯¥åºåˆ—åœ¨cacheä¸­ä¸å­˜åœ¨
            printf("empty\n");
        } else {
            // æ˜¾ç¤ºåºåˆ—çš„ä½ç½®èŒƒå›´å’Œtokenæ•°é‡
            // æ³¨æ„ï¼šä½ç½®æ˜¯è¿ç»­çš„ï¼Œä½†åœ¨cacheä¸­çš„å­˜å‚¨å¯èƒ½ä¸è¿ç»­
            printf("range [%d, %d], length %d\n", min_pos, max_pos, max_pos - min_pos + 1);
        }
    }
    printf("\n");
}

/*- Test Functions ------------------------------------------------------------*/

// ä¸»è¦çš„KV cacheæµ‹è¯•å‡½æ•°
// è¿™ä¸ªå‡½æ•°é€šè¿‡ä¸€ç³»åˆ—æ“ä½œæ¼”ç¤ºcacheçš„å·¥ä½œæœºåˆ¶
//
// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
// â”‚                    Test Execution Flow                          â”‚
// â”‚                                                                 â”‚
// â”‚  Step 1: Model Creation                                         â”‚
// â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                â”‚
// â”‚  â”‚ Create      â”‚                                                â”‚
// â”‚  â”‚ Test Model  â”‚                                                â”‚
// â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                â”‚
// â”‚         â”‚                                                       â”‚
// â”‚         â–¼                                                       â”‚
// â”‚  Step 2: Cache Initialization                                   â”‚
// â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                â”‚
// â”‚  â”‚ Initialize  â”‚                                                â”‚
// â”‚  â”‚ KV Cache    â”‚                                                â”‚
// â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                â”‚
// â”‚         â”‚                                                       â”‚
// â”‚         â–¼                                                       â”‚
// â”‚  Step 3-7: Token Operations & Analysis                          â”‚
// â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
// â”‚  â”‚ Add Batch 1 â”‚  â”‚ Add Batch 2 â”‚  â”‚ Extend Seq  â”‚              â”‚
// â”‚  â”‚ (Seq 42)    â”‚  â”‚ (Seq 84)    â”‚  â”‚ (Seq 42)    â”‚              â”‚
// â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
// â”‚         â”‚                â”‚                â”‚                     â”‚
// â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
// â”‚                          â–¼                                      â”‚
// â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
// â”‚  â”‚ Copy Seq    â”‚  â”‚ Remove Seq  â”‚  â”‚ Clear Cache â”‚              â”‚
// â”‚  â”‚ (42â†’126)    â”‚  â”‚ (84)        â”‚  â”‚ (All)       â”‚              â”‚
// â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
static void test_kv_cache_debug() {
    printf("=== Testing KV Cache Debug Tools ===\n");
    
    /*
     * Step 1: Model Creation
     * 
     * â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     * â”‚                    Model Architecture                       â”‚
     * â”‚                                                             â”‚
     * â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
     * â”‚  â”‚   Layer 0   â”‚    â”‚   Layer 1   â”‚                         â”‚
     * â”‚  â”‚             â”‚    â”‚             â”‚                         â”‚
     * â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                         â”‚
     * â”‚  â”‚ â”‚ 4 Heads â”‚ â”‚    â”‚ â”‚ 4 Heads â”‚ â”‚                         â”‚
     * â”‚  â”‚ â”‚ 32 dim  â”‚ â”‚    â”‚ â”‚ 32 dim  â”‚ â”‚                         â”‚
     * â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                         â”‚
     * â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
     * â”‚                                                             â”‚
     * â”‚  Each layer will have independent K/V cache storage         â”‚
     * â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     */
    auto model = _make_model(LLM_ARCH_LLAMA, 2, 32, 32, 4, 1);
    printf("âœ“ Test model created (2 layers, 4 heads)\n");
    
    /*
     * Step 2: Cache Initialization
     * 
     * â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     * â”‚                    Cache Configuration                      â”‚
     * â”‚                                                             â”‚
     * â”‚  Cache Parameters:                                          â”‚
     * â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
     * â”‚  â”‚   Size: 64  â”‚  â”‚ Type: F16   â”‚  â”‚ Seqs: 4     â”‚          â”‚
     * â”‚  â”‚   cells     â”‚  â”‚ precision   â”‚  â”‚ max         â”‚          â”‚
     * â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
     * â”‚                                                             â”‚
     * â”‚  Initial Cache Layout:                                      â”‚
     * â”‚  â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”          â”‚
     * â”‚  â”‚ âˆ… â”‚ âˆ… â”‚ âˆ… â”‚ âˆ… â”‚ âˆ… â”‚ âˆ… â”‚ âˆ… â”‚ âˆ… â”‚...â”‚ âˆ… â”‚ âˆ… â”‚ âˆ… â”‚          â”‚
     * â”‚  â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜          â”‚
     * â”‚   0   1   2   3   4   5   6   7       60  61  62  63        â”‚
     * â”‚                                                             â”‚
     * â”‚  Legend: âˆ… = empty cell                                     â”‚
     * â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     */
    llama_kv_cache_unified::layer_filter_cb filter = [](int32_t il) { 
        (void)il; 
        return true; 
    };
    
    auto kv_cache = std::make_unique<llama_kv_cache_unified>(
        *model, 
        std::move(filter),
        GGML_TYPE_F16,  // K type
        GGML_TYPE_F16,  // V type
        false,          // v_trans
        false,          // offload
        64,             // kv_size
        4,              // n_seq_max
        8,              // n_pad
        0,              // n_swa
        LLAMA_SWA_TYPE_NONE
    );
    
    printf("âœ“ KV cache created\n");
    
    // æ˜¾ç¤ºåˆå§‹çŠ¶æ€ï¼šcacheä¸ºç©ºï¼Œæ‰€æœ‰tensorç»´åº¦ä¸º0
    print_kv_cache_status(kv_cache.get(), "Initial State");
    print_cache_tensors_info(kv_cache.get(), *model, "Initial State");
    
    /*
     * Step 3: First Token Batch Addition
     * 
     * â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     * â”‚                    Batch 1 Processing                      â”‚
     * â”‚                                                             â”‚
     * â”‚  Input Tokens:                                              â”‚
     * â”‚  â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”                                  â”‚
     * â”‚  â”‚ 101 â”‚ 102 â”‚ 103 â”‚ 104 â”‚                                  â”‚
     * â”‚  â”‚ pos â”‚ pos â”‚ pos â”‚ pos â”‚                                  â”‚
     * â”‚  â”‚  0  â”‚  1  â”‚  2  â”‚  3  â”‚                                  â”‚
     * â”‚  â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜                                  â”‚
     * â”‚                                                             â”‚
     * â”‚  Cache After Allocation:                                    â”‚
     * â”‚  â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”          â”‚
     * â”‚  â”‚42 â”‚42 â”‚42 â”‚42 â”‚ âˆ… â”‚ âˆ… â”‚ âˆ… â”‚ âˆ… â”‚...â”‚ âˆ… â”‚ âˆ… â”‚ âˆ… â”‚          â”‚
     * â”‚  â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜          â”‚
     * â”‚   0   1   2   3   4   5   6   7       60  61  62  63        â”‚
     * â”‚                                                             â”‚
     * â”‚  Sequence 42: [0,3] length=4                               â”‚
     * â”‚  Active window: 8 cells (due to padding)                   â”‚
     * â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     */
    printf("\n=== Adding First Batch of Tokens ===\n");
    
    llama_seq_id seq_id_1 = 42;
    llama_batch batch1 = llama_batch_init(4, 0, 1);
    
    // common_batch_add()å°†tokenæ·»åŠ åˆ°batchä¸­
    // å‚æ•°ï¼štoken_id, position, sequence_ids, need_logits
    // positionæ˜¯è¯¥tokenåœ¨åºåˆ—ä¸­çš„ç»å¯¹ä½ç½®
    common_batch_add(batch1, 101, 0, {seq_id_1}, false);
    common_batch_add(batch1, 102, 1, {seq_id_1}, false);
    common_batch_add(batch1, 103, 2, {seq_id_1}, false);
    common_batch_add(batch1, 104, 3, {seq_id_1}, true);  // æœ€åä¸€ä¸ªtokenéœ€è¦logits
    
    // llama_sbatchå°†batchè½¬æ¢ä¸ºå†…éƒ¨å¤„ç†æ ¼å¼
    // è¿™ä¸ªè¿‡ç¨‹ä¼šåˆ†æåºåˆ—ç»“æ„å’Œtokenåˆ†å¸ƒ
    llama_sbatch sbatch1(batch1, model->hparams.n_embd, true, false);
    llama_ubatch ubatch1 = sbatch1.split_simple(4);
    
    printf("Batch 1: %u tokens, %u seqs\n", ubatch1.n_tokens, ubatch1.n_seqs);
    
    // find_slot()æ˜¯cacheåˆ†é…çš„æ ¸å¿ƒå‡½æ•°
    // å®ƒä¼šåœ¨cacheä¸­å¯»æ‰¾è¶³å¤Ÿçš„è¿ç»­ç©ºé—´æ¥å­˜å‚¨æ–°çš„tokens
    if (kv_cache->find_slot(ubatch1)) {
        // commit()ç¡®è®¤åˆ†é…ï¼Œä½¿æ›´æ”¹ç”Ÿæ•ˆ
        // åœ¨æ­¤ä¹‹å‰ï¼Œåˆ†é…æ˜¯ä¸´æ—¶çš„ï¼Œå¯ä»¥é€šè¿‡restore()æ’¤é”€
        kv_cache->commit();
        printf("âœ“ First batch added to cache\n");
        
        print_kv_cache_status(kv_cache.get(), "After First Batch");
        print_cache_tensors_info(kv_cache.get(), *model, "After First Batch");
        print_sequence_info(kv_cache.get(), {seq_id_1}, "After First Batch");
    } else {
        printf("âœ— Failed to add first batch to cache\n");
    }
    
    llama_batch_free(batch1);
    
    /*
     * Step 4: Second Sequence Addition
     * 
     * â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     * â”‚                    Batch 2 Processing                       â”‚
     * â”‚                                                             â”‚
     * â”‚  Input Tokens (New Sequence):                               â”‚
     * â”‚  â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”                                        â”‚
     * â”‚  â”‚ 201 â”‚ 202 â”‚ 203 â”‚                                        â”‚
     * â”‚  â”‚ pos â”‚ pos â”‚ pos â”‚                                        â”‚
     * â”‚  â”‚  0  â”‚  1  â”‚  2  â”‚                                        â”‚
     * â”‚  â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜                                        â”‚
     * â”‚                                                             â”‚
     * â”‚  Cache After Allocation:                                    â”‚
     * â”‚  â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”          â”‚
     * â”‚  â”‚42 â”‚42 â”‚42 â”‚42 â”‚84 â”‚84 â”‚84 â”‚ âˆ… â”‚...â”‚ âˆ… â”‚ âˆ… â”‚ âˆ… â”‚          â”‚
     * â”‚  â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜          â”‚
     * â”‚   0   1   2   3   4   5   6   7       60  61  62  63        â”‚
     * â”‚                                                             â”‚
     * â”‚  Sequence 42: [0,3] length=4                                â”‚
     * â”‚  Sequence 84: [0,2] length=3                                â”‚
     * â”‚  Active window: 8 cells (unchanged)                         â”‚
     * â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     */
    printf("\n=== Adding Second Batch of Tokens (Different Sequence) ===\n");
    
    llama_seq_id seq_id_2 = 84;
    llama_batch batch2 = llama_batch_init(3, 0, 1);
    
    // æ³¨æ„ï¼šè¿™ä¸ªåºåˆ—çš„positionä»0å¼€å§‹ï¼Œå› ä¸ºå®ƒæ˜¯ç‹¬ç«‹çš„åºåˆ—
    // æ¯ä¸ªåºåˆ—éƒ½æœ‰è‡ªå·±çš„ä½ç½®ç¼–ç ç©ºé—´
    common_batch_add(batch2, 201, 0, {seq_id_2}, false);
    common_batch_add(batch2, 202, 1, {seq_id_2}, false);
    common_batch_add(batch2, 203, 2, {seq_id_2}, true);
    
    llama_sbatch sbatch2(batch2, model->hparams.n_embd, true, false);
    llama_ubatch ubatch2 = sbatch2.split_simple(3);
    
    printf("Batch 2: %u tokens, %u seqs\n", ubatch2.n_tokens, ubatch2.n_seqs);
    
    if (kv_cache->find_slot(ubatch2)) {
        kv_cache->commit();
        printf("âœ“ Second batch added to cache\n");
        
        print_kv_cache_status(kv_cache.get(), "After Second Batch");
        print_cache_tensors_info(kv_cache.get(), *model, "After Second Batch");
        print_sequence_info(kv_cache.get(), {seq_id_1, seq_id_2}, "After Second Batch");
    } else {
        printf("âœ— Failed to add second batch to cache\n");
    }
    
    llama_batch_free(batch2);
    
    /*
     * Step 5: Sequence Extension
     * 
     * â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     * â”‚                    Sequence Growth                          â”‚
     * â”‚                                                             â”‚
     * â”‚  Extending Sequence 42:                                     â”‚
     * â”‚  â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”                                              â”‚
     * â”‚  â”‚ 105 â”‚ 106 â”‚                                              â”‚
     * â”‚  â”‚ pos â”‚ pos â”‚                                              â”‚
     * â”‚  â”‚  4  â”‚  5  â”‚                                              â”‚
     * â”‚  â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜                                              â”‚
     * â”‚                                                             â”‚
     * â”‚  Cache After Extension:                                     â”‚
     * â”‚  â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”          â”‚
     * â”‚  â”‚42 â”‚42 â”‚42 â”‚42 â”‚84 â”‚84 â”‚84 â”‚42 â”‚42 â”‚ âˆ… â”‚...â”‚ âˆ… â”‚          â”‚
     * â”‚  â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜          â”‚
     * â”‚   0   1   2   3   4   5   6   7   8   9       63            â”‚
     * â”‚                                                             â”‚
     * â”‚  Sequence 42: [0,5] length=6 (extended!)                    â”‚
     * â”‚  Sequence 84: [0,2] length=3 (unchanged)                    â”‚
     * â”‚  Active window: 16 cells (expanded to fit longer sequence)  â”‚
     * â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     */
    printf("\n=== Continuing First Sequence ===\n");
    
    llama_batch batch3 = llama_batch_init(2, 0, 1);
    
    // ç»§ç»­åºåˆ—42ï¼Œpositionä»4å¼€å§‹ï¼ˆæ¥ç»­ä¹‹å‰çš„[0,3]ï¼‰
    common_batch_add(batch3, 105, 4, {seq_id_1}, false);
    common_batch_add(batch3, 106, 5, {seq_id_1}, true);
    
    llama_sbatch sbatch3(batch3, model->hparams.n_embd, true, false);
    llama_ubatch ubatch3 = sbatch3.split_simple(2);
    
    printf("Batch 3: %u tokens, %u seqs\n", ubatch3.n_tokens, ubatch3.n_seqs);
    
    if (kv_cache->find_slot(ubatch3)) {
        kv_cache->commit();
        printf("âœ“ Third batch added to cache\n");
        
        print_kv_cache_status(kv_cache.get(), "After Third Batch");
        print_sequence_info(kv_cache.get(), {seq_id_1, seq_id_2}, "After Third Batch");
    } else {
        printf("âœ— Failed to add third batch to cache\n");
    }
    
    llama_batch_free(batch3);
    
    /*
     * Step 6: Sequence Operations
     * 
     * â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     * â”‚                    Sequence Manipulation                    â”‚
     * â”‚                                                             â”‚
     * â”‚  Operation 1: Copy Sequence 42 â†’ 126                        â”‚
     * â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    copy     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
     * â”‚  â”‚ Sequence 42     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Sequence 126    â”‚        â”‚
     * â”‚  â”‚ [0,1,2,3,4,5]   â”‚             â”‚ [0,1,2,3,4,5]   â”‚        â”‚
     * â”‚  â”‚ (original)      â”‚             â”‚ (duplicate)     â”‚        â”‚
     * â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
     * â”‚                                                             â”‚
     * â”‚  Operation 2: Remove Sequence 84                            â”‚
     * â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   remove    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
     * â”‚  â”‚ Sequence 84     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚     Empty       â”‚        â”‚
     * â”‚  â”‚ [0,1,2]         â”‚             â”‚     Cells       â”‚        â”‚
     * â”‚  â”‚ (deleted)       â”‚             â”‚   Available     â”‚        â”‚
     * â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
     * â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     */
    printf("\n=== Testing Sequence Operations ===\n");
    
    // seq_cp()å¤åˆ¶åºåˆ—ï¼šå°†æºåºåˆ—çš„æ‰€æœ‰K/Væ•°æ®å¤åˆ¶åˆ°ç›®æ ‡åºåˆ—
    // è¿™æ˜¯ä¸€ä¸ªæ·±æ‹·è´æ“ä½œï¼Œç›®æ ‡åºåˆ—è·å¾—ç‹¬ç«‹çš„æ•°æ®å‰¯æœ¬
    llama_seq_id seq_id_3 = 126;
    printf("Copying sequence %d to %d...\n", seq_id_1, seq_id_3);
    kv_cache->seq_cp(seq_id_1, seq_id_3, -1, -1);  // -1è¡¨ç¤ºå¤åˆ¶æ•´ä¸ªåºåˆ—
    print_sequence_info(kv_cache.get(), {seq_id_1, seq_id_2, seq_id_3}, "After Sequence Copy");
    
    // seq_rm()åˆ é™¤åºåˆ—ï¼šé‡Šæ”¾åºåˆ—å ç”¨çš„cacheç©ºé—´
    // è¢«åˆ é™¤çš„cellså˜ä¸ºå¯ç”¨çŠ¶æ€ï¼Œå¯ä»¥è¢«æ–°çš„tokensä½¿ç”¨
    printf("Removing sequence %d...\n", seq_id_2);
    kv_cache->seq_rm(seq_id_2, -1, -1);  // -1è¡¨ç¤ºåˆ é™¤æ•´ä¸ªåºåˆ—
    print_sequence_info(kv_cache.get(), {seq_id_1, seq_id_2, seq_id_3}, "After Sequence Remove");
    print_kv_cache_status(kv_cache.get(), "After Sequence Remove");
    
    /*
     * Step 7: Cache Cleanup
     * 
     * â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     * â”‚                    Cache Reset Operation                    â”‚
     * â”‚                                                             â”‚
     * â”‚  Before Clear:                                              â”‚
     * â”‚  â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”          â”‚
     * â”‚  â”‚42 â”‚42 â”‚42 â”‚42 â”‚ âˆ… â”‚ âˆ… â”‚ âˆ… â”‚42 â”‚42 â”‚126â”‚...â”‚ âˆ… â”‚          â”‚
     * â”‚  â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜          â”‚
     * â”‚                                                             â”‚
     * â”‚  After Clear:                                               â”‚
     * â”‚  â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”          â”‚
     * â”‚  â”‚ âˆ… â”‚ âˆ… â”‚ âˆ… â”‚ âˆ… â”‚ âˆ… â”‚ âˆ… â”‚ âˆ… â”‚ âˆ… â”‚ âˆ… â”‚ âˆ… â”‚...â”‚ âˆ… â”‚          â”‚
     * â”‚  â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜          â”‚
     * â”‚                                                             â”‚
     * â”‚  All sequences removed, cache ready for reuse               â”‚
     * â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     */
    printf("\n=== Clearing Cache ===\n");
    kv_cache->clear();
    
    print_kv_cache_status(kv_cache.get(), "After Clear");
    print_sequence_info(kv_cache.get(), {seq_id_1, seq_id_2, seq_id_3}, "After Clear");
    
    printf("âœ“ KV Cache debug test completed successfully!\n");
}

/*- Main ----------------------------------------------------------------------*/

// ä¸»å‡½æ•°ï¼šåˆå§‹åŒ–ç¯å¢ƒå¹¶è¿è¡Œæµ‹è¯•
//
// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
// â”‚                    Program Execution Flow                      â”‚
// â”‚                                                                 â”‚
// â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                â”‚
// â”‚  â”‚ Initialize  â”‚                                                â”‚
// â”‚  â”‚ Backend     â”‚                                                â”‚
// â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                â”‚
// â”‚         â”‚                                                       â”‚
// â”‚         â–¼                                                       â”‚
// â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                â”‚
// â”‚  â”‚ Run Cache   â”‚                                                â”‚
// â”‚  â”‚ Debug Tests â”‚                                                â”‚
// â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                â”‚
// â”‚         â”‚                                                       â”‚
// â”‚         â–¼                                                       â”‚
// â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                â”‚
// â”‚  â”‚ Cleanup &   â”‚                                                â”‚
// â”‚  â”‚ Exit        â”‚                                                â”‚
// â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                â”‚
// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
int main(int argc, char ** argv) {
    (void)argc;  // Suppress unused parameter warning
    (void)argv;  // Suppress unused parameter warning
    
    printf("=== KV Cache Debug Tool ===\n\n");
    
    // åˆå§‹åŒ–ggml backendç³»ç»Ÿ
    // è¿™ä¼šåŠ è½½æ‰€æœ‰å¯ç”¨çš„è®¡ç®—åç«¯ï¼ˆCPU, GPUç­‰ï¼‰
    ggml_backend_load_all();
    printf("ggml backend initialized\n\n");
    
    try {
        test_kv_cache_debug();
        
        printf("\nğŸ‰ All KV cache debug tests completed!\n");
        
    } catch (const std::exception& e) {
        std::cerr << "âŒ Test failed with exception: " << e.what() << "\n";
        return 1;
    }
    
    // æ¸…ç†backendèµ„æº
    llama_backend_free();
    
    return 0;
}
