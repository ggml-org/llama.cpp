<!--START_SECTION:navbar-->
<div align="center">
  <a href="../README.md">ЁЯЗ║ЁЯЗ╕ English</a> | <a href="README.de.md">ЁЯЗйЁЯЗк Deutsch</a> | <a href="README.es.md">ЁЯЗкЁЯЗ╕ Espa├▒ol</a> | <a href="README.fr.md">ЁЯЗлЁЯЗ╖ Fran├зais</a> | <a href="README.hi.md">ЁЯЗоЁЯЗ│ рд╣рд┐рдВрджреА</a> | <a href="README.ja.md">ЁЯЗпЁЯЗ╡ цЧецЬмшкЮ</a> | <a href="README.ko.md">ЁЯЗ░ЁЯЗ╖ эХЬъ╡ньЦ┤</a> | <a href="README.pt.md">ЁЯЗ╡ЁЯЗ╣ Portugu├кs</a> | <a href="README.ru.md">ЁЯЗ╖ЁЯЗ║ ╨а╤Г╤Б╤Б╨║╨╕╨╣</a> | <a href="README.zh.md">ЁЯЗиЁЯЗ│ ф╕нцЦЗ</a>
</div>
<!--END_SECTION:navbar-->

# llama.cpp

![llama](https://user-images.githubusercontent.com/1991296/230134379-7181e485-c521-4d23-a0d6-f7b3b61ba524.png)

[![License: MIT](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![Release](https://img.shields.io/github/v/release/ggml-org/llama.cpp)](https://github.com/ggml-org/llama.cpp/releases)
[![Server](https://github.com/ggml-org/llama.cpp/actions/workflows/server.yml/badge.svg)](https://github.com/ggml-org/llama.cpp/actions/workflows/server.yml)

[Manifesto](https://github.com/ggml-org/llama.cpp/discussions/205) / [ggml](https://github.com/ggml-org/ggml) / [ops](https://github.com/ggml-org/llama.cpp/blob/master/docs/ops.md)

LLM inference in C/C++

## рд╣рд╛рд▓рд┐рдпрд╛ API рдмрджрд▓

- [libllama API рдХреЗ рд▓рд┐рдП рдЪреЗрдВрдЬрд▓реЙрдЧ](https://github.com/ggml-org/llama.cpp/issues/9289)
- [llama-server REST API рдХреЗ рд▓рд┐рдП рдЪреЗрдВрдЬрд▓реЙрдЧ](https://github.com/ggml-org/llama.cpp/issues/9291)

## рдЧрд░реНрдо рд╡рд┐рд╖рдп

- **[рдЧрд╛рдЗрдб : рдирдП WebUI рдХрд╛ рдЙрдкрдпреЛрдЧ llama.cpp рдореЗрдВ](https://github.com/ggml-org/llama.cpp/discussions/16938)**
- [рдЧрд╛рдЗрдб : gpt-oss рдХреЗ рд╕рд╛рде llama.cpp рдХреЗ рдЪрд▓рд╛рдирд╛](https://github.com/ggml-org/llama.cpp/discussions/15396)
- [[рдлреАрдбрдмреИрдХ] llama.cpp рдХреЗ рд▓рд┐рдП рдмреЗрд╣рддрд░ рдкреИрдХреЗрдЬрд┐рдВрдЧ рдбрд╛рдЙрдирд╕реНрдЯреНрд░реАрдо рдХрдВрдЬреНрдпреВрдорд░реНрд╕ рдХреЗ рд╕рдорд░реНрдерди рдХреЗ рд▓рд┐рдП ЁЯдЧ](https://github.com/ggml-org/llama.cpp/discussions/15313)
- `gpt-oss` рдореЙрдбрд▓ рдХреЗ рд▓рд┐рдП рд╕рдорд░реНрдерди рдЬреЛрдбрд╝рд╛ рдЧрдпрд╛ рд╣реИ, рдЬреЛ рдореВрд▓ MXFP4 рдкреНрд░рд╛рд░реВрдк рдХреЗ рд╕рд╛рде рдХрд╛рдо рдХрд░рддрд╛ рд╣реИ | [PR](https://github.com/ggml-org/llama.cpp/pull/15091) | [NVIDIA рдХреЗ рд╕рд╛рде рд╕рд╣рдпреЛрдЧ](https://blogs.nvidia.com/blog/rtx-ai-garage-openai-oss) | [рдЯрд┐рдкреНрдкрдгреА](https://github.com/ggml-org/llama.cpp/discussions/15095)
- `llama-server` рдореЗрдВ multimodal рд╕рдорд░реНрдерди рдЖ рдЧрдпрд╛ рд╣реИ: [#12898](https://github.com/ggml-org/llama.cpp/pull/12898) | [рджрд╕реНрддрд╛рд╡реЗрдЬреАрдХрд░рдг](.././docs/multimodal.md)
- FIM рдкреВрд░реНрдгрддрд╛ рдХреЗ рд▓рд┐рдП VS Code рдПрдХреНрд╕рдЯреЗрдВрд╢рди: https://github.com/ggml-org/llama.vscode
- FIM рдкреВрд░реНрдгрддрд╛ рдХреЗ рд▓рд┐рдП Vim/Neovim рдкреНрд▓рдЧрдЗрди: https://github.com/ggml-org/llama.vim
- Hugging Face Inference Endpoints рдЕрдм GGUF рдХреЗ рд╕рдорд░реНрдерди рдХреЗ рд▓рд┐рдП рдмреЙрдХреНрд╕ рдореЗрдВ рд╕рдорд░реНрдерди рджреЗрддрд╛ рд╣реИ! https://github.com/ggml-org/llama.cpp/discussions/9669
- Hugging Face GGUF рд╕рдВрдкрд╛рджрдХ: [рдЪрд░реНрдЪрд╛](https://github.com/ggml-org/llama.cpp/discussions/9268) | [рдЙрдкрдХрд░рдг](https://huggingface.co/spaces/CISCai/gguf-editor)

## рддреНрд╡рд░рд┐рдд рд╢реБрд░реБрдЖрдд

llama.cpp рдХреЗ рд╕рд╛рде рд╢реБрд░реБрдЖрдд рдХрд░рдирд╛ рдЖрд╕рд╛рди рд╣реИред рдЕрдкрдиреЗ рдХрдВрдкреНрдпреВрдЯрд░ рдкрд░ рдЗрд╕реЗ рдЗрди рддрд░реАрдХреЛрдВ рд╕реЗ рдЗрдВрд╕реНрдЯреЙрд▓ рдХрд░реЗрдВ:

- [brew, nix рдпрд╛ winget](../docs/install.md) рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рдХреЗ `llama.cpp` рдХреЛ рдЗрдВрд╕реНрдЯреЙрд▓ рдХрд░реЗрдВ
- рдбреЙрдХрд░ рдХреЗ рд╕рд╛рде рдЪрд▓рд╛рдПрдВ - рд╣рдорд╛рд░реЗ [рдбреЙрдХрд░ рджрд╕реНрддрд╛рд╡реЗрдЬрд╝](../docs/docker.md) рдХреЛ рджреЗрдЦреЗрдВ
- [рд░рд┐рд▓реАрдЬ рдкреЗрдЬ](https://github.com/ggml-org/llama.cpp/releases) рд╕реЗ рдкреВрд░реНрд╡ рдирд┐рд░реНрдорд┐рдд рдмрд╛рдЗрдирд░реА рдбрд╛рдЙрдирд▓реЛрдб рдХрд░реЗрдВ
- рдЗрд╕ рд░рд┐рдкреЙрдЬрд┐рдЯрд░реА рдХреЛ рдХреНрд▓реЛрди рдХрд░рдХреЗ рд╕реНрд░реЛрдд рд╕реЗ рдмрдирд╛рдПрдВ - [рд╣рдорд╛рд░реЗ рдмрд┐рд▓реНрдб рдЧрд╛рдЗрдб](../docs/build.md) рдХреЛ рджреЗрдЦреЗрдВ

рдЗрдВрд╕реНрдЯреЙрд▓ рдХрд░рдиреЗ рдХреЗ рдмрд╛рдж, рдЖрдкрдХреЛ рдХрд╛рдо рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдПрдХ рдореЙрдбрд▓ рдХреА рдЖрд╡рд╢реНрдпрдХрддрд╛ рд╣реЛрдЧреАред рдЕрдзрд┐рдХ рдЬрд╛рдирдиреЗ рдХреЗ рд▓рд┐рдП [рдореЙрдбрд▓ рдкреНрд░рд╛рдкреНрдд рдХрд░реЗрдВ рдФрд░ рдХреНрд╡рд╛рдВрдЯрд╛рдЗрдЬ рдХрд░реЗрдВ](#obtaining-and-quantizing-models) рдЕрдиреБрднрд╛рдЧ рдХреЛ рджреЗрдЦреЗрдВред

рдЙрджрд╛рд╣рд░рдг рдХрдорд╛рдВрдб:

```sh
# Use a local model file
llama-cli -m my_model.gguf

# Or download and run a model directly from Hugging Face
llama-cli -hf ggml-org/gemma-3-1b-it-GGUF

# Launch OpenAI-compatible API server
llama-server -hf ggml-org/gemma-3-1b-it-GGUF
```

## рд╡рд┐рд╡рд░рдг

`llama.cpp` рдХрд╛ рдореБрдЦреНрдп рдЙрджреНрджреЗрд╢реНрдп рдПрд▓рдИрдИ (LLM) рдЕрдиреБрдорд╛рди рдХреЛ рдиреНрдпреВрдирддрдо рд╕реЗрдЯрдЕрдк рдФрд░ рд╡реНрдпрд╛рдкрдХ рд░реВрдк рд╕реЗ рд╡рд┐рднрд┐рдиреНрди рд╣рд╛рд░реНрдбрд╡реЗрдпрд░ рдкрд░ рдЙрддреНрдХреГрд╖реНрдЯ рдкреНрд░рджрд░реНрд╢рди рдХреЗ рд╕рд╛рде рд╕рдХреНрд╖рдо рдХрд░рдирд╛ рд╣реИ - рд╕реНрдерд╛рдиреАрдп рдФрд░ рдХреНрд▓рд╛рдЙрдб рдореЗрдВред

- рдХреЛрдИ рднреА рдирд┐рд░реНрднрд░рддрд╛ рдХреЗ рдмрд┐рдирд╛ рд╕рд╛рдорд╛рдиреНрдп C/C++ рдХрд╛рд░реНрдпрд╛рдиреНрд╡рдпрди
- рдПрдкреНрдкрд▓ рд╕рд┐рд▓рд┐рдХреЙрди рдкрд╣рд▓рд╛ рд╢реНрд░реЗрд╖реНрда рдирд╛рдЧрд░рд┐рдХ рд╣реИ - ARM NEON, Accelerate рдФрд░ Metal рдлреНрд░реЗрдорд╡рд░реНрдХ рдХреЗ рдорд╛рдзреНрдпрдо рд╕реЗ рдЕрдиреБрдХреВрд▓рд┐рдд
- x86 рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░ рдХреЗ рд▓рд┐рдП AVX, AVX2, AVX512 рдФрд░ AMX рдХрд╛ рд╕рдорд░реНрдерди
- RISC-V рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░ рдХреЗ рд▓рд┐рдП RVV, ZVFH, ZFH, ZICBOP рдФрд░ ZIHINTPAUSE рдХрд╛ рд╕рдорд░реНрдерди
- рддреНрд╡рд░рд┐рдд рдЕрдиреБрдорд╛рди рдФрд░ рдХрдо рдпрд╛рджреГрдЪреНрдЫрд┐рдХ рдкреНрд░рд╡реЗрд╢ рдХреЗ рд▓рд┐рдП 1.5-рдмрд┐рдЯ, 2-рдмрд┐рдЯ, 3-рдмрд┐рдЯ, 4-рдмрд┐рдЯ, 5-рдмрд┐рдЯ, 6-рдмрд┐рдЯ рдФрд░ 8-рдмрд┐рдЯ рдкреВрд░реНрдгрд╛рдВрдХ рдХреНрд╡рд╛рдВрдЯрд╛рдЗрдЬреЗрд╢рди
- рдПрдирд╡реАрдбрд┐рдпрд╛ рдЬреАрдкреАрдпреВ рдкрд░ рдПрд▓рдИрдИ рдХреЗ рдЪрд▓рд╛рдиреЗ рдХреЗ рд▓рд┐рдП рд╡рд┐рд╢реЗрд╖ рдмрдирд╛рдП рдЧрдП рдХреНрдпреВрдбреАрдП (CUDA) рдХрд░реНрдирд▓ (рдПрдордбреА рдЬреАрдкреАрдпреВ рдХреЗ рд▓рд┐рдП HIP рдФрд░ рдореВрд░ рдереНрд░реЗрдбреНрд╕ рдЬреАрдкреАрдпреВ рдХреЗ рд▓рд┐рдП MUSA рдХреЗ рдорд╛рдзреНрдпрдо рд╕реЗ рдПрдордбреА рдЬреАрдкреАрдпреВ рдХреЗ рд▓рд┐рдП рд╕рдорд░реНрдерди)
- рд╡реБрд▓реНрдХрди рдФрд░ рд╕рд┐рдХрд▓ (SYCL) рдмреИрдХрдПрдВрдб рдХрд╛ рд╕рдорд░реНрдерди
- рд╕реАрдкреАрдпреВ+рдЬреАрдкреАрдпреВ рд╣рд╛рдЗрдмреНрд░рд┐рдб рдЕрдиреБрдорд╛рди рдЬрд┐рд╕рд╕реЗ рдХреБрд▓ рд╡реАрдЖрд░рдПрдПрдо рдХреНрд╖рдорддрд╛ рд╕реЗ рдЕрдзрд┐рдХ рдЖрдХрд╛рд░ рдХреЗ рдореЙрдбрд▓реЛрдВ рдХреЗ рдЕрдВрд╢рддрдГ рддреНрд╡рд░рдг

`llama.cpp` рдкрд░рд┐рдпреЛрдЬрдирд╛ [ggml](https://github.com/ggml-org/ggml) рдкреБрд╕реНрддрдХрд╛рд▓рдп рдХреЗ рд▓рд┐рдП рдирдП рд╡рд┐рд╢реЗрд╖рддрд╛рдУрдВ рдХреЗ рд╡рд┐рдХрд╛рд╕ рдХреЗ рдореБрдЦреНрдп рдЦреЗрд▓ рдХреНрд╖реЗрддреНрд░ рд╣реИред

<details>
<summary>Models</summary>

Typically finetunes of the base models below are supported as well.

Instructions for adding support for new models: [HOWTO-add-model.md](../docs/development/HOWTO-add-model.md)

#### Text-only

- [X] LLaMA ЁЯжЩ
- [x] LLaMA 2 ЁЯжЩЁЯжЩ
- [x] LLaMA 3 ЁЯжЩЁЯжЩЁЯжЩ
- [X] [Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-v0.1)
- [x] [Mixtral MoE](https://huggingface.co/models?search=mistral-ai/Mixtral)
- [x] [DBRX](https://huggingface.co/databricks/dbrx-instruct)
- [x] [Jamba](https://huggingface.co/ai21labs)
- [X] [Falcon](https://huggingface.co/models?search=tiiuae/falcon)
- [X] [Chinese LLaMA / Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca) and [Chinese LLaMA-2 / Alpaca-2](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2)
- [X] [Vigogne (French)](https://github.com/bofenghuang/vigogne)
- [X] [BERT](https://github.com/ggml-org/llama.cpp/pull/5423)
- [X] [Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/)
- [X] [Baichuan 1 & 2](https://huggingface.co/models?search=baichuan-inc/Baichuan) + [derivations](https://huggingface.co/hiyouga/baichuan-7b-sft)
- [X] [Aquila 1 & 2](https://huggingface.co/models?search=BAAI/Aquila)
- [X] [Starcoder models](https://github.com/ggml-org/llama.cpp/pull/3187)
- [X] [Refact](https://huggingface.co/smallcloudai/Refact-1_6B-fim)
- [X] [MPT](https://github.com/ggml-org/llama.cpp/pull/3417)
- [X] [Bloom](https://github.com/ggml-org/llama.cpp/pull/3553)
- [x] [Yi models](https://huggingface.co/models?search=01-ai/Yi)
- [X] [StableLM models](https://huggingface.co/stabilityai)
- [x] [Deepseek models](https://huggingface.co/models?search=deepseek-ai/deepseek)
- [x] [Qwen models](https://huggingface.co/models?search=Qwen/Qwen)
- [x] [PLaMo-13B](https://github.com/ggml-org/llama.cpp/pull/3557)
- [x] [Phi models](https://huggingface.co/models?search=microsoft/phi)
- [x] [PhiMoE](https://github.com/ggml-org/llama.cpp/pull/11003)
- [x] [GPT-2](https://huggingface.co/gpt2)
- [x] [Orion 14B](https://github.com/ggml-org/llama.cpp/pull/5118)
- [x] [InternLM2](https://huggingface.co/models?search=internlm2)
- [x] [CodeShell](https://github.com/WisdomShell/codeshell)
- [x] [Gemma](https://ai.google.dev/gemma)
- [x] [Mamba](https://github.com/state-spaces/mamba)
- [x] [Grok-1](https://huggingface.co/keyfan/grok-1-hf)
- [x] [Xverse](https://huggingface.co/models?search=xverse)
- [x] [Command-R models](https://huggingface.co/models?search=CohereForAI/c4ai-command-r)
- [x] [SEA-LION](https://huggingface.co/models?search=sea-lion)
- [x] [GritLM-7B](https://huggingface.co/GritLM/GritLM-7B) + [GritLM-8x7B](https://huggingface.co/GritLM/GritLM-8x7B)
- [x] [OLMo](https://allenai.org/olmo)
- [x] [OLMo 2](https://allenai.org/olmo)
- [x] [OLMoE](https://huggingface.co/allenai/OLMoE-1B-7B-0924)
- [x] [Granite models](https://huggingface.co/collections/ibm-granite/granite-code-models-6624c5cec322e4c148c8b330)
- [x] [GPT-NeoX](https://github.com/EleutherAI/gpt-neox) + [Pythia](https://github.com/EleutherAI/pythia)
- [x] [Snowflake-Arctic MoE](https://huggingface.co/collections/Snowflake/arctic-66290090abe542894a5ac520)
- [x] [Smaug](https://huggingface.co/models?search=Smaug)
- [x] [Poro 34B](https://huggingface.co/LumiOpen/Poro-34B)
- [x] [Bitnet b1.58 models](https://huggingface.co/1bitLLM)
- [x] [Flan T5](https://huggingface.co/models?search=flan-t5)
- [x] [Open Elm models](https://huggingface.co/collections/apple/openelm-instruct-models-6619ad295d7ae9f868b759ca)
- [x] [ChatGLM3-6b](https://huggingface.co/THUDM/chatglm3-6b) + [ChatGLM4-9b](https://huggingface.co/THUDM/glm-4-9b) + [GLMEdge-1.5b](https://huggingface.co/THUDM/glm-edge-1.5b-chat) + [GLMEdge-4b](https://huggingface.co/THUDM/glm-edge-4b-chat)
- [x] [GLM-4-0414](https://huggingface.co/collections/THUDM/glm-4-0414-67f3cbcb34dd9d252707cb2e)
- [x] [SmolLM](https://huggingface.co/collections/HuggingFaceTB/smollm-6695016cad7167254ce15966)
- [x] [EXAONE-3.0-7.8B-Instruct](https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct)
- [x] [FalconMamba Models](https://huggingface.co/collections/tiiuae/falconmamba-7b-66b9a580324dd1598b0f6d4a)
- [x] [Jais](https://huggingface.co/inceptionai/jais-13b-chat)
- [x] [Bielik-11B-v2.3](https://huggingface.co/collections/speakleash/bielik-11b-v23-66ee813238d9b526a072408a)
- [x] [RWKV-6](https://github.com/BlinkDL/RWKV-LM)
- [x] [QRWKV-6](https://huggingface.co/recursal/QRWKV6-32B-Instruct-Preview-v0.1)
- [x] [GigaChat-20B-A3B](https://huggingface.co/ai-sage/GigaChat-20B-A3B-instruct)
- [X] [Trillion-7B-preview](https://huggingface.co/trillionlabs/Trillion-7B-preview)
- [x] [Ling models](https://huggingface.co/collections/inclusionAI/ling-67c51c85b34a7ea0aba94c32)
- [x] [LFM2 models](https://huggingface.co/collections/LiquidAI/lfm2-686d721927015b2ad73eaa38)
- [x] [Hunyuan models](https://huggingface.co/collections/tencent/hunyuan-dense-model-6890632cda26b19119c9c5e7)
- [x] [BailingMoeV2 (Ring/Ling 2.0) models](https://huggingface.co/collections/inclusionAI/ling-v2-68bf1dd2fc34c306c1fa6f86)

#### Multimodal

- [x] [LLaVA 1.5 models](https://huggingface.co/collections/liuhaotian/llava-15-653aac15d994e992e2677a7e), [LLaVA 1.6 models](https://huggingface.co/collections/liuhaotian/llava-16-65b9e40155f60fd046a5ccf2)
- [x] [BakLLaVA](https://huggingface.co/models?search=SkunkworksAI/Bakllava)
- [x] [Obsidian](https://huggingface.co/NousResearch/Obsidian-3B-V0.5)
- [x] [ShareGPT4V](https://huggingface.co/models?search=Lin-Chen/ShareGPT4V)
- [x] [MobileVLM 1.7B/3B models](https://huggingface.co/models?search=mobileVLM)
- [x] [Yi-VL](https://huggingface.co/models?search=Yi-VL)
- [x] [Mini CPM](https://huggingface.co/models?search=MiniCPM)
- [x] [Moondream](https://huggingface.co/vikhyatk/moondream2)
- [x] [Bunny](https://github.com/BAAI-DCAI/Bunny)
- [x] [GLM-EDGE](https://huggingface.co/models?search=glm-edge)
- [x] [Qwen2-VL](https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d)
- [x] [LFM2-VL](https://huggingface.co/collections/LiquidAI/lfm2-vl-68963bbc84a610f7638d5ffa)

</details>

<details>
<summary>Bindings</summary>

- Python: [ddh0/easy-llama](https://github.com/ddh0/easy-llama)
- Python: [abetlen/llama-cpp-python](https://github.com/abetlen/llama-cpp-python)
- Go: [go-skynet/go-llama.cpp](https://github.com/go-skynet/go-llama.cpp)
- Node.js: [withcatai/node-llama-cpp](https://github.com/withcatai/node-llama-cpp)
- JS/TS (llama.cpp server client): [lgrammel/modelfusion](https://modelfusion.dev/integration/model-provider/llamacpp)
- JS/TS (Programmable Prompt Engine CLI): [offline-ai/cli](https://github.com/offline-ai/cli)
- JavaScript/Wasm (works in browser): [tangledgroup/llama-cpp-wasm](https://github.com/tangledgroup/llama-cpp-wasm)
- Typescript/Wasm (nicer API, available on npm): [ngxson/wllama](https://github.com/ngxson/wllama)
- Ruby: [yoshoku/llama_cpp.rb](https://github.com/yoshoku/llama_cpp.rb)
- Rust (more features): [edgenai/llama_cpp-rs](https://github.com/edgenai/llama_cpp-rs)
- Rust (nicer API): [mdrokz/rust-llama.cpp](https://github.com/mdrokz/rust-llama.cpp)
- Rust (more direct bindings): [utilityai/llama-cpp-rs](https://github.com/utilityai/llama-cpp-rs)
- Rust (automated build from crates.io): [ShelbyJenkins/llm_client](https://github.com/ShelbyJenkins/llm_client)
- C#/.NET: [SciSharp/LLamaSharp](https://github.com/SciSharp/LLamaSharp)
- C#/VB.NET (more features - community license): [LM-Kit.NET](https://docs.lm-kit.com/lm-kit-net/index.html)
- Scala 3: [donderom/llm4s](https://github.com/donderom/llm4s)
- Clojure: [phronmophobic/llama.clj](https://github.com/phronmophobic/llama.clj)
- React Native: [mybigday/llama.rn](https://github.com/mybigday/llama.rn)
- Java: [kherud/java-llama.cpp](https://github.com/kherud/java-llama.cpp)
- Java: [QuasarByte/llama-cpp-jna](https://github.com/QuasarByte/llama-cpp-jna)
- Zig: [deins/llama.cpp.zig](https://github.com/Deins/llama.cpp.zig)
- Flutter/Dart: [netdur/llama_cpp_dart](https://github.com/netdur/llama_cpp_dart)
- Flutter: [xuegao-tzx/Fllama](https://github.com/xuegao-tzx/Fllama)
- PHP (API bindings and features built on top of llama.cpp): [distantmagic/resonance](https://github.com/distantmagic/resonance) [(more info)](https://github.com/ggml-org/llama.cpp/pull/6326)
- Guile Scheme: [guile_llama_cpp](https://savannah.nongnu.org/projects/guile-llama-cpp)
- Swift [srgtuszy/llama-cpp-swift](https://github.com/srgtuszy/llama-cpp-swift)
- Swift [ShenghaiWang/SwiftLlama](https://github.com/ShenghaiWang/SwiftLlama)
- Delphi [Embarcadero/llama-cpp-delphi](https://github.com/Embarcadero/llama-cpp-delphi)
- Go (no CGo needed): [hybridgroup/yzma](https://github.com/hybridgroup/yzma)
- Android: [llama.android](/examples/llama.android)

</details>

<details>
<summary>UIs</summary>

*(to have a project listed here, it should clearly state that it depends on `llama.cpp`)*

- [AI Sublime Text plugin](https://github.com/yaroslavyaroslav/OpenAI-sublime-text) (MIT)
- [cztomsik/ava](https://github.com/cztomsik/ava) (MIT)
- [Dot](https://github.com/alexpinel/Dot) (GPL)
- [eva](https://github.com/ylsdamxssjxxdd/eva) (MIT)
- [iohub/collama](https://github.com/iohub/coLLaMA) (Apache-2.0)
- [janhq/jan](https://github.com/janhq/jan) (AGPL)
- [johnbean393/Sidekick](https://github.com/johnbean393/Sidekick) (MIT)
- [KanTV](https://github.com/zhouwg/kantv?tab=readme-ov-file) (Apache-2.0)
- [KodiBot](https://github.com/firatkiral/kodibot) (GPL)
- [llama.vim](https://github.com/ggml-org/llama.vim) (MIT)
- [LARS](https://github.com/abgulati/LARS) (AGPL)
- [Llama Assistant](https://github.com/vietanhdev/llama-assistant) (GPL)
- [LLMFarm](https://github.com/guinmoon/LLMFarm?tab=readme-ov-file) (MIT)
- [LLMUnity](https://github.com/undreamai/LLMUnity) (MIT)
- [LMStudio](https://lmstudio.ai/) (proprietary)
- [LocalAI](https://github.com/mudler/LocalAI) (MIT)
- [LostRuins/koboldcpp](https://github.com/LostRuins/koboldcpp) (AGPL)
- [MindMac](https://mindmac.app) (proprietary)
- [MindWorkAI/AI-Studio](https://github.com/MindWorkAI/AI-Studio) (FSL-1.1-MIT)
- [Mobile-Artificial-Intelligence/maid](https://github.com/Mobile-Artificial-Intelligence/maid) (MIT)
- [Mozilla-Ocho/llamafile](https://github.com/Mozilla-Ocho/llamafile) (Apache-2.0)
- [nat/openplayground](https://github.com/nat/openplayground) (MIT)
- [nomic-ai/gpt4all](https://github.com/nomic-ai/gpt4all) (MIT)
- [ollama/ollama](https://github.com/ollama/ollama) (MIT)
- [oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui) (AGPL)
- [PocketPal AI](https://github.com/a-ghorbani/pocketpal-ai) (MIT)
- [psugihara/FreeChat](https://github.com/psugihara/FreeChat) (MIT)
- [ptsochantaris/emeltal](https://github.com/ptsochantaris/emeltal) (MIT)
- [pythops/tenere](https://github.com/pythops/tenere) (AGPL)
- [ramalama](https://github.com/containers/ramalama) (MIT)
- [semperai/amica](https://github.com/semperai/amica) (MIT)
- [withcatai/catai](https://github.com/withcatai/catai) (MIT)
- [Autopen](https://github.com/blackhole89/autopen) (GPL)

</details>

<details>
<summary>Tools</summary>

- [akx/ggify](https://github.com/akx/ggify) тАУ download PyTorch models from HuggingFace Hub and convert them to GGML
- [akx/ollama-dl](https://github.com/akx/ollama-dl) тАУ download models from the Ollama library to be used directly with llama.cpp
- [crashr/gppm](https://github.com/crashr/gppm) тАУ launch llama.cpp instances utilizing NVIDIA Tesla P40 or P100 GPUs with reduced idle power consumption
- [gpustack/gguf-parser](https://github.com/gpustack/gguf-parser-go/tree/main/cmd/gguf-parser) - review/check the GGUF file and estimate the memory usage
- [Styled Lines](https://marketplace.unity.com/packages/tools/generative-ai/styled-lines-llama-cpp-model-292902) (proprietary licensed, async wrapper of inference part for game development in Unity3d with pre-built Mobile and Web platform wrappers and a model example)
- [unslothai/unsloth](https://github.com/unslothai/unsloth) тАУ ЁЯже exports/saves fine-tuned and trained models to GGUF (Apache-2.0)

</details>

<details>
<summary>Infrastructure</summary>

- [Paddler](https://github.com/intentee/paddler) - Open-source LLMOps platform for hosting and scaling AI in your own infrastructure
- [GPUStack](https://github.com/gpustack/gpustack) - Manage GPU clusters for running LLMs
- [llama_cpp_canister](https://github.com/onicai/llama_cpp_canister) - llama.cpp as a smart contract on the Internet Computer, using WebAssembly
- [llama-swap](https://github.com/mostlygeek/llama-swap) - transparent proxy that adds automatic model switching with llama-server
- [Kalavai](https://github.com/kalavai-net/kalavai-client) - Crowdsource end to end LLM deployment at any scale
- [llmaz](https://github.com/InftyAI/llmaz) - тШ╕я╕П Easy, advanced inference platform for large language models on Kubernetes.
</details>

<details>
<summary>Games</summary>

- [Lucy's Labyrinth](https://github.com/MorganRO8/Lucys_Labyrinth) - A simple maze game where agents controlled by an AI model will try to trick you.

</details>

## рд╕рдорд░реНрдерд┐рдд рдмреИрдХрдПрдВрдб

| рдмреИрдХрдПрдВрдб | рд▓рдХреНрд╖реНрдп рдЙрдкрдХрд░рдг |
| --- | --- |
| [Metal](../docs/build.md#metal-build) | Apple Silicon |
| [BLAS](../docs/build.md#blas-build) | рд╕рднреА |
| [BLIS](../docs/backend/BLIS.md) | рд╕рднреА |
| [SYCL](../docs/backend/SYCL.md) | рдЗрдВрдЯреЗрд▓ рдФрд░ рдПрдирд╡реАрдбрд┐рдпрд╛ рдЬреАрдкреАрдпреВ |
| [MUSA](../docs/build.md#musa) | рдореВрд░ рдереНрд░реЗрдбреНрд╕ рдЬреАрдкреАрдпреВ |
| [CUDA](../docs/build.md#cuda) | рдПрдирд╡реАрдбрд┐рдпрд╛ рдЬреАрдкреАрдпреВ |
| [HIP](../docs/build.md#hip) | рдПрдПрдордбреА рдЬреАрдкреАрдпреВ |
| [ZenDNN](../docs/build.md#zendnn) | рдПрдПрдордбреА рд╕реАрдкреАрдпреВ |
| [Vulkan](../docs/build.md#vulkan) | рдЬреАрдкреАрдпреВ |
| [CANN](../docs/build.md#cann) | рдПрд╕реНрдХреЗрдВрдб рдПрдирдкреАрдпреВ |
| [OpenCL](../docs/backend/OPENCL.md) | рдПрдбреНрд░реЗрдиреЛ рдЬреАрдкреАрдпреВ |
| [IBM zDNN](../docs/backend/zDNN.md) | рдЖрдИрдмреАрдПрдо Z & рд▓рд┐рдирдХреНрд╕рдУрдиреЗ |
| [WebGPU [In Progress]](../docs/build.md#webgpu) | рд╕рднреА |
| [RPC](https://github.com/ggml-org/llama.cpp/tree/master/tools/rpc) | рд╕рднреА |
| [Hexagon [In Progress]](../docs/backend/hexagon/README.md) | рд╕реНрдиреИрдкрдбреНрд░реЙрди |

## рдореЙрдбрд▓ рдкреНрд░рд╛рдкреНрдд рдХрд░рдирд╛ рдФрд░ рдХреНрд╡рд╛рдВрдЯрд╛рдЗрдЬрд╝ рдХрд░рдирд╛

[Hugging Face](https://huggingface.co) рдкреНрд▓реЗрдЯрдлреЙрд░реНрдо `llama.cpp` рд╕реЗ рд╕рдВрдЧрдд [рдХрдИ LLMs](https://huggingface.co/models?library=gguf&sort=trending) рдХреЛ рд╣реЛрд╕реНрдЯ рдХрд░рддрд╛ рд╣реИ:

- [рд▓реЛрдХрдкреНрд░рд┐рдп](https://huggingface.co/models?library=gguf&sort=trending)
- [LLaMA](https://huggingface.co/models?sort=trending&search=llama+gguf)

рдЖрдк GGUF рдлрд╝рд╛рдЗрд▓ рдХреЛ рдореИрдиреБрдЕрд▓ рд░реВрдк рд╕реЗ рдбрд╛рдЙрдирд▓реЛрдб рдХрд░ рд╕рдХрддреЗ рд╣реИрдВ рдпрд╛ [Hugging Face](https://huggingface.co/) рдпрд╛ рдЕрдиреНрдп рдореЙрдбрд▓ рд╣реЛрд╕реНрдЯрд┐рдВрдЧ рд╕рд╛рдЗрдЯреНрд╕, рдЬреИрд╕реЗ [ModelScope](https://modelscope.cn/), рд╕реЗ рд╕реАрдзреЗ рдХреЛрдИ рднреА `llama.cpp`-рд╕рдВрдЧрдд рдореЙрдбрд▓ рдХрд╛ рдЙрдкрдпреЛрдЧ рдЗрд╕ CLI рдЖрд░реНрдЧреБрдореЗрдВрдЯ рдХреЗ рд╕рд╛рде рдХрд░ рд╕рдХрддреЗ рд╣реИрдВ: `-hf <user>/<model>[:quant]`ред рдЙрджрд╛рд╣рд░рдг рдХреЗ рд▓рд┐рдП:

```sh
llama-cli -hf ggml-org/gemma-3-1b-it-GGUF
```

рдбрд┐рдлрд╝реЙрд▓реНрдЯ рд░реВрдк рд╕реЗ, CLI рд╣реБрдЧреНрдЧрд┐рдВрдЧ рдлреЗрд╕ рд╕реЗ рдбрд╛рдЙрдирд▓реЛрдб рдХрд░рддрд╛ рд╣реИ, рдЖрдк `MODEL_ENDPOINT` рдкрд░реНрдпрд╛рд╡рд░рдг рдЪрд░ рдХреЗ рд╕рд╛рде рдЕрдиреНрдп рд╡рд┐рдХрд▓реНрдкреЛрдВ рдореЗрдВ рд╕реНрд╡рд┐рдЪ рдХрд░ рд╕рдХрддреЗ рд╣реИрдВред рдЙрджрд╛рд╣рд░рдг рдХреЗ рд▓рд┐рдП, рдЖрдк `MODEL_ENDPOINT=https://www.modelscope.cn/` рдЬреИрд╕реЗ рдкрд░реНрдпрд╛рд╡рд░рдг рдЪрд░ рдХреЛ рд╕реЗрдЯ рдХрд░рдХреЗ рдореЙрдбрд▓ рдЪреЗрдХрдкреЙрдЗрдВрдЯреНрд╕ рдХреЛ рдореЙрдбрд▓рд╕реНрдХреЛрдк рдпрд╛ рдЕрдиреНрдп рдореЙрдбрд▓ рд╕рд╛рдЭрд╛ рдХрд░рдиреЗ рд╡рд╛рд▓реЗ рд╕рдореБрджрд╛рдпреЛрдВ рд╕реЗ рдбрд╛рдЙрдирд▓реЛрдб рдХрд░рдиреЗ рдХрд╛ рд╡рд┐рдХрд▓реНрдк рдЪреБрди рд╕рдХрддреЗ рд╣реИрдВред

рдореЙрдбрд▓ рдбрд╛рдЙрдирд▓реЛрдб рдХрд░рдиреЗ рдХреЗ рдмрд╛рдж, рдЗрд╕реЗ рд╕реНрдерд╛рдиреАрдп рд░реВрдк рд╕реЗ рдЪрд▓рд╛рдиреЗ рдХреЗ рд▓рд┐рдП CLI рдЙрдкрдХрд░рдгреЛрдВ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВ - рдиреАрдЪреЗ рджреЗрдЦреЗрдВред

`llama.cpp` рдореЙрдбрд▓ рдХреЛ [GGUF](https://github.com/ggml-org/ggml/blob/master/docs/gguf.md) рдлрд╝рд╛рдЗрд▓ рдлреЙрд░реНрдореЗрдЯ рдореЗрдВ рд╕рдВрдЧреНрд░рд╣рд┐рдд рдХрд░рдиреЗ рдХреА рдЖрд╡рд╢реНрдпрдХрддрд╛ рд╣реЛрддреА рд╣реИред рдЕрдиреНрдп рдбреЗрдЯрд╛ рдлреЙрд░реНрдореЗрдЯ рдореЗрдВ рдореЙрдбрд▓ рдХреЛ GGUF рдореЗрдВ рдкрд░рд┐рд╡рд░реНрддрд┐рдд рдХрд┐рдпрд╛ рдЬрд╛ рд╕рдХрддрд╛ рд╣реИ, рдЗрд╕ рд░рд┐рдкреЙрдЬрд┐рдЯрд░реА рдореЗрдВ рдЗрд╕ рд░рд┐рдкреЙрдЬрд┐рдЯрд░реА рдореЗрдВ `convert_*.py` рдкрд╛рдпрдерди рд╕реНрдХреНрд░рд┐рдкреНрдЯ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рдХреЗред

рд╣реБрдЧреНрдЧрд┐рдВрдЧ рдлреЗрд╕ рдкреНрд▓реЗрдЯрдлреЙрд░реНрдо `llama.cpp` рдХреЗ рд▓рд┐рдП рдСрдирд▓рд╛рдЗрди рдЙрдкрдХрд░рдгреЛрдВ рдХреЗ рдПрдХ рд╡рд┐рд╕реНрддреГрдд рд╕реЗрдЯ рдкреНрд░рджрд╛рди рдХрд░рддрд╛ рд╣реИ рдЬрд┐рдирдХрд╛ рдЙрдкрдпреЛрдЧ рдореЙрдбрд▓ рдХреЗ рдкрд░рд┐рд╡рд░реНрддрди, рдХреНрд╡рд╛рдВрдЯрд╛рдЗрдЬрд╝реЗрд╢рди рдФрд░ рдореЗрдЬрдмрд╛рдиреА рдХреЗ рд▓рд┐рдП рдХрд┐рдпрд╛ рдЬрд╛ рд╕рдХрддрд╛ рд╣реИ:

- [GGUF-my-repo space](https://huggingface.co/spaces/ggml-org/gguf-my-repo) рдХрд╛ рдЙрдкрдпреЛрдЧ GGUF рдлреЙрд░реНрдореЗрдЯ рдореЗрдВ рдкрд░рд┐рд╡рд░реНрддрд┐рдд рдХрд░рдиреЗ рдФрд░ рдореЙрдбрд▓ рд╡рдЬрди рдХреЛ рдЫреЛрдЯреЗ рдЖрдХрд╛рд░ рдореЗрдВ рдХреНрд╡рд╛рдВрдЯрд╛рдЗрдЬрд╝ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдХрд░реЗрдВ
- [GGUF-my-LoRA space](https://huggingface.co/spaces/ggml-org/gguf-my-lora) рдХрд╛ рдЙрдкрдпреЛрдЧ LoRA рдПрдбреЗрдкреНрдЯрд░реНрд╕ рдХреЛ GGUF рдлреЙрд░реНрдореЗрдЯ рдореЗрдВ рдкрд░рд┐рд╡рд░реНрддрд┐рдд рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдХрд░реЗрдВ (рдЕрдзрд┐рдХ рдЬрд╛рдирдХрд╛рд░реА: https://github.com/ggml-org/llama.cpp/discussions/10123)
- [GGUF-editor space](https://huggingface.co/spaces/CISCai/gguf-editor) рдХрд╛ рдЙрдкрдпреЛрдЧ рдмреНрд░рд╛рдЙрдЬрд╝рд░ рдореЗрдВ GGUF рдореЗрдЯрд╛ рдбреЗрдЯрд╛ рд╕рдВрдкрд╛рджрд┐рдд рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдХрд░реЗрдВ (рдЕрдзрд┐рдХ рдЬрд╛рдирдХрд╛рд░реА: https://github.com/ggml-org/llama.cpp/discussions/9268)
- [Inference Endpoints](https://ui.endpoints.huggingface.co/) рдХрд╛ рдЙрдкрдпреЛрдЧ `llama.cpp` рдХреЛ рдмрд╛рджрд▓ рдореЗрдВ рд╕реАрдзреЗ рдореЗрдЬрдмрд╛рди рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдХрд░реЗрдВ (рдЕрдзрд┐рдХ рдЬрд╛рдирдХрд╛рд░реА: https://github.com/ggml-org/llama.cpp/discussions/9669)

рдореЙрдбрд▓ рдХреНрд╡рд╛рдВрдЯрд╛рдЗрдЬрд╝реЗрд╢рди рдХреЗ рдмрд╛рд░реЗ рдореЗрдВ рдЕрдзрд┐рдХ рдЬрд╛рдирдиреЗ рдХреЗ рд▓рд┐рдП, [рдЗрд╕ рджрд╕реНрддрд╛рд╡реЗрдЬрд╝ рдХреЛ рдкрдврд╝реЗрдВ](../tools/quantize/README.md)

## [`llama-cli`](../tools/cli)

#### `llama-cli` рдХреЗ рд╕рд╛рде `llama.cpp` рдХреЗ рдЕрдзрд┐рдХрд╛рдВрд╢ рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рддрдХ рдкрд╣реБрдБрдЪрдиреЗ рдФрд░ рдкреНрд░рдпреЛрдЧ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдПрдХ CLI рдЙрдкрдХрд░рдгред


<details open>
    <summary>Run in conversation mode</summary>

    Models with a built-in chat template will automatically activate conversation mode. If this doesn't occur, you can manually enable it by adding `-cnv` and specifying a suitable chat template with `--chat-template NAME`

    ```bash
    llama-cli -m model.gguf

    # > hi, who are you?
    # Hi there! I'm your helpful assistant! I'm an AI-powered chatbot designed to assist and provide information to users like you. I'm here to help answer your questions, provide guidance, and offer support on a wide range of topics. I'm a friendly and knowledgeable AI, and I'm always happy to help with anything you need. What's on your mind, and how can I assist you today?
    # > what is 1+1?
    # Easy peasy! The answer to 1+1 is... 2!
    ```

    </details>


<details>
    <summary>Run in conversation mode with custom chat template</summary>

    ```bash
    # use the "chatml" template (use -h to see the list of supported templates)
    llama-cli -m model.gguf -cnv --chat-template chatml

    # use a custom template
    llama-cli -m model.gguf -cnv --in-prefix 'User: ' --reverse-prompt 'User:'
    ```

    </details>


<details>
    <summary>Constrain the output with a custom grammar</summary>

    ```bash
    llama-cli -m model.gguf -n 256 --grammar-file grammars/json.gbnf -p 'Request: schedule a call at 8pm; Command:'

    # {"appointmentTime": "8pm", "appointmentDetails": "schedule a a call"}
    ```

    The [grammars/](../grammars/) folder contains a handful of sample grammars. To write your own, check out the [GBNF Guide](../grammars/README.md).

    For authoring more complex JSON grammars, check out https://grammar.intrinsiclabs.ai/

    </details>

## [`llama-server`](../tools/server)

#### рдПрдХ рд╣рд▓реНрдХрд╛, [OpenAI API](https://github.com/openai/openai-openapi) рд╕реЗ рд╕рдВрдЧрдд, HTTP рд╕рд░реНрд╡рд░ рдЬреЛ LLMs рдХреЛ рд╕рд░реНрд╡ рдХрд░рддрд╛ рд╣реИред


<details open>
    <summary>Start a local HTTP server with default configuration on port 8080</summary>

    ```bash
    llama-server -m model.gguf --port 8080

    # Basic web UI can be accessed via browser: http://localhost:8080
    # Chat completion endpoint: http://localhost:8080/v1/chat/completions
    ```

    </details>


<details>
    <summary>Support multiple-users and parallel decoding</summary>

    ```bash
    # up to 4 concurrent requests, each with 4096 max context
    llama-server -m model.gguf -c 16384 -np 4
    ```

    </details>


<details>
    <summary>Enable speculative decoding</summary>

    ```bash
    # the draft.gguf model should be a small variant of the target model.gguf
    llama-server -m model.gguf -md draft.gguf
    ```

    </details>


<details>
    <summary>Serve an embedding model</summary>

    ```bash
    # use the /embedding endpoint
    llama-server -m model.gguf --embedding --pooling cls -ub 8192
    ```

    </details>


<details>
    <summary>Serve a reranking model</summary>

    ```bash
    # use the /reranking endpoint
    llama-server -m model.gguf --reranking
    ```

    </details>


<details>
    <summary>Constrain all outputs with a grammar</summary>

    ```bash
    # custom grammar
    llama-server -m model.gguf --grammar-file grammar.gbnf

    # JSON
    llama-server -m model.gguf --grammar-file grammars/json.gbnf
    ```

    </details>

## [`llama-perplexity`](../tools/perplexity)

#### рдПрдХ рдЙрдкрдХрд░реНрдо рдЬреЛ рдПрдХ рджрд┐рдП рдЧрдП рдкрд╛рда рдкрд░ рдореЙрдбрд▓ рдХреЗ [perplexity](../tools/perplexity/README.md) [^1] (рдФрд░ рдЕрдиреНрдп рдЧреБрдгрд╡рддреНрддрд╛ рдорд╛рдкрджрдВрдб) рдХреЛ рдорд╛рдкрдиреЗ рдХреЗ рд▓рд┐рдП рд╣реИред


<details open>
    <summary>Measure the perplexity over a text file</summary>

    ```bash
    llama-perplexity -m model.gguf -f file.txt

    # [1]15.2701,[2]5.4007,[3]5.3073,[4]6.2965,[5]5.8940,[6]5.6096,[7]5.7942,[8]4.9297, ...
    # Final estimate: PPL = 5.4007 +/- 0.67339
    ```

    </details>


<details>
    <summary>Measure KL divergence</summary>

    ```bash
    # TODO
    ```

    </details>

[^1]: [https://huggingface.co/docs/transformers/perplexity](https://huggingface.co/docs/transformers/perplexity)

## [`llama-bench`](../tools/llama-bench)

#### рд╡рд┐рднрд┐рдиреНрди рдкреИрд░рд╛рдореАрдЯрд░ рдХреЗ рдЕрдиреБрдорд╛рди рдХреЗ рдкреНрд░рджрд░реНрд╢рди рдХрд╛ рдкрд░реАрдХреНрд╖рдг рдХрд░реЗрдВред


<details open>
    <summary>Run default benchmark</summary>

    ```bash
    llama-bench -m model.gguf

    # Output:
    # | model               |       size |     params | backend    | threads |          test |                  t/s |
    # | ------------------- | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |
    # | qwen2 1.5B Q4_0     | 885.97 MiB |     1.54 B | Metal,BLAS |      16 |         pp512 |      5765.41 ┬▒ 20.55 |
    # | qwen2 1.5B Q4_0     | 885.97 MiB |     1.54 B | Metal,BLAS |      16 |         tg128 |        197.71 ┬▒ 0.81 |
    # build: 3e0ba0e60 (4229)
    ```

    </details>

## [`llama-run`](../tools/run)

#### рдПрдХ рд╡реНрдпрд╛рдкрдХ рдЙрджрд╛рд╣рд░рдг `llama.cpp` рдореЙрдбрд▓ рдЪрд▓рд╛рдиреЗ рдХреЗ рд▓рд┐рдПред рдЕрдиреБрдорд╛рди рд▓рдЧрд╛рдиреЗ рдХреЗ рд▓рд┐рдП рдЙрдкрдпреЛрдЧреАред RamaLama [^3] рдХреЗ рд╕рд╛рде рдЙрдкрдпреЛрдЧ рдХрд┐рдпрд╛ рдЬрд╛рддрд╛ рд╣реИред


<details>
    <summary>Run a model with a specific prompt (by default it's pulled from Ollama registry)</summary>

    ```bash
    llama-run granite-code
    ```

    </details>

[^3]: [RamaLama](https://github.com/containers/ramalama)

## [`llama-simple`](../examples/simple)

#### `llama.cpp` рдХреЗ рд╕рд╛рде рдПрдкреНрд▓рд┐рдХреЗрд╢рдиреНрд╕ рдХреЛ рд▓рд╛рдЧреВ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдПрдХ рдиреНрдпреВрдирддрдо рдЙрджрд╛рд╣рд░рдгред рд╡рд┐рдХрд╛рд╕рдХрд░реНрддрд╛рдУрдВ рдХреЗ рд▓рд┐рдП рдЙрдкрдпреЛрдЧреАред


<details>
    <summary>Basic text completion</summary>

    ```bash
    llama-simple -m model.gguf

    # Hello my name is Kaitlyn and I am a 16 year old girl. I am a junior in high school and I am currently taking a class called "The Art of
    ```

    </details>

## рдпреЛрдЧрджрд╛рди

- рдпреЛрдЧрджрд╛рдирдХрд░реНрддрд╛ рдкреНрд░реА-рд░рд┐рдХреНрд╡реЗрд╕реНрдЯ (PRs) рдЦреЛрд▓ рд╕рдХрддреЗ рд╣реИрдВ
- рд╕рд╣рдпреЛрдЧреА рдпреЛрдЧрджрд╛рди рдХреЗ рдЖрдзрд╛рд░ рдкрд░ рдЖрдордВрддреНрд░рд┐рдд рдХрд┐рдП рдЬрд╛рдПрдВрдЧреЗ
- рд╕рдВрдЪрд╛рд▓рдХ `llama.cpp` рд░рд┐рдкреЛ рдореЗрдВ рд╢рд╛рдЦрд╛рдУрдВ рдореЗрдВ рдкреБрд╢ рдХрд░ рд╕рдХрддреЗ рд╣реИрдВ рдФрд░ PRs рдХреЛ `рдорд╛рд╕реНрдЯрд░` рд╢рд╛рдЦрд╛ рдореЗрдВ рдорд░реНрдЬ рдХрд░ рд╕рдХрддреЗ рд╣реИрдВ
- рдХрд┐рд╕реА рднреА рдкреНрд░рдХрд╛рд░ рдХреА рд╕рдорд╕реНрдпрд╛рдУрдВ, PRs рдФрд░ рдкреНрд░реЛрдЬреЗрдХреНрдЯреЛрдВ рдХреЗ рдкреНрд░рдмрдВрдзрди рдореЗрдВ рд╕рд╣рд╛рдпрддрд╛ рдмрд╣реБрдд рдЕрдореВрд▓реНрдп рд╣реИ!
- рдкрд╣рд▓реЗ рдпреЛрдЧрджрд╛рди рдХреЗ рд▓рд┐рдП рдЙрдкрдпреБрдХреНрдд рдХрд╛рд░реНрдпреЛрдВ рдХреЗ рд▓рд┐рдП рджреЗрдЦреЗрдВ: [good first issues](https://github.com/ggml-org/llama.cpp/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22)
- рдЕрдзрд┐рдХ рдЬрд╛рдирдХрд╛рд░реА рдХреЗ рд▓рд┐рдП [CONTRIBUTING.md](../CONTRIBUTING.md) рдкрдврд╝реЗрдВ
- рдЗрд╕реЗ рдкрдврд╝рдирд╛ рдирд┐рд╢реНрдЪрд┐рдд рдХрд░реЗрдВ: [Inference at the edge](https://github.com/ggml-org/llama.cpp/discussions/205)
- рдЙрди рд▓реЛрдЧреЛрдВ рдХреЗ рд▓рд┐рдП рдЬреЛ рд░реБрдЪрд┐ рд░рдЦрддреЗ рд╣реИрдВ: [Changelog podcast](https://changelog.com/podcast/532)

## рдЕрдиреНрдп рджрд╕реНрддрд╛рд╡реЗрдЬрд╝

- [cli](../tools/cli/README.md)
- [completion](../tools/completion/README.md)
- [server](../tools/server/README.md)
- [GBNF grammars](../grammars/README.md)

#### рд╡рд┐рдХрд╛рд╕ рджрд╕реНрддрд╛рд╡реЗрдЬ

- [рдХреИрд╕реЗ рдмрдирд╛рдПрдВ](../docs/build.md)
- [рдбреЙрдХрд░ рдкрд░ рдЪрд▓рд╛рдПрдВ](../docs/docker.md)
- [рдПрдВрдбреНрд░реЙрдЗрдб рдкрд░ рдмрдирд╛рдПрдВ](../docs/android.md)
- [рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рд╕рдорд╕реНрдпрд╛ рдирд┐рд░реНрдореВрд▓рди](../docs/development/token_generation_performance_tips.md)
- [GGML рдЯрд┐рдкреНрд╕ & рдЯреНрд░рд┐рдХреНрд╕](https://github.com/ggml-org/llama.cpp/wiki/GGML-Tips-&-Tricks)

#### рдореВрд▓ рдкреЗрдкрд░ рдФрд░ рдореЙрдбрд▓ рдкрд░ рдкреГрд╖реНрдарднреВрдорд┐

рдЕрдЧрд░ рдЖрдкрдХреА рд╕рдорд╕реНрдпрд╛ рдореЙрдбрд▓ рдЙрддреНрдкрд╛рджрди рдЧреБрдгрд╡рддреНрддрд╛ рд╕реЗ рд╕рдВрдмрдВрдзрд┐рдд рд╣реИ, рддреЛ рдХреГрдкрдпрд╛ рдирд┐рдореНрдирд▓рд┐рдЦрд┐рдд рд▓рд┐рдВрдХ рдФрд░ рдкреЗрдкрд░ рдХреЗ рдХрдо рд╕реЗ рдХрдо рд╕реНрдХреИрди рдХрд░реЗрдВ рддрд╛рдХрд┐ рдЖрдк LLaMA рдореЙрдбрд▓ рдХреА рд╕реАрдорд╛рдУрдВ рдХреЛ рд╕рдордЭ рд╕рдХреЗрдВред рдпрд╣ рд╡рд┐рд╢реЗрд╖ рд░реВрдк рд╕реЗ рдорд╣рддреНрд╡рдкреВрд░реНрдг рд╣реИ рдЬрдм рдЙрдкрдпреБрдХреНрдд рдореЙрдбрд▓ рдЖрдХрд╛рд░ рдХреЗ рдЪрдпрди рдХреЗ рджреМрд░рд╛рди рдФрд░ LLaMA рдореЙрдбрд▓ рдФрд░ ChatGPT рдХреЗ рдмреАрдЪ рдорд╣рддреНрд╡рдкреВрд░реНрдг рдФрд░ рдкреНрд░рддрд┐рдмрдВрдзрд┐рдд рдЕрдВрддрд░реЛрдВ рдХреЛ рд╕рдордЭрдиреЗ рдХреЗ рд▓рд┐рдП:
- LLaMA:
    - [LLaMA рдХрд╛ рдкрд░рд┐рдЪрдп: рдПрдХ рдореМрд▓рд┐рдХ, 65-рдЕрд░рдм рдкреИрд░рд╛рдореАрдЯрд░ рд╡рд╛рд▓рд╛ рдмрдбрд╝рд╛ рднрд╛рд╖рд╛ рдореЙрдбрд▓](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)
    - [LLaMA: рдЦреБрд▓рд╛ рдФрд░ рдХреБрд╢рд▓ рдлрд╛рдЙрдВрдбреЗрд╢рди рднрд╛рд╖рд╛ рдореЙрдбрд▓](https://arxiv.org/abs/2302.13971)
- GPT-3
    - [рднрд╛рд╖рд╛ рдореЙрдбрд▓ рдлрд╝реЗрд╡рд░-рд╢реЙрдЯ рд╕реАрдЦрдиреЗ рд╡рд╛рд▓реЗ рд╣реИрдВ](https://arxiv.org/abs/2005.14165)
- GPT-3.5 / InstructGPT / ChatGPT:
    - [рднрд╛рд╖рд╛ рдореЙрдбрд▓ рдХреЛ рдирд┐рд░реНрджреЗрд╢реЛрдВ рдХреЗ рдЕрдиреБрд╕рд╛рд░ рд╕рдВрд░реЗрдЦрд┐рдд рдХрд░реЗрдВ](https://openai.com/research/instruction-following)
    - [рдорд╛рдирд╡ рдкреНрд░рддрд┐рдХреНрд░рд┐рдпрд╛ рдХреЗ рд╕рд╛рде рдирд┐рд░реНрджреЗрд╢реЛрдВ рдХреЗ рдЕрдиреБрд╕рд╛рд░ рднрд╛рд╖рд╛ рдореЙрдбрд▓ рдХреЗ рдкреНрд░рд╢рд┐рдХреНрд╖рдг](https://arxiv.org/abs/2203.02155)

## XCFramework

XCFramework рдПрдХ рдкреВрд░реНрд╡-рдХрдВрдкрд╛рдЗрд▓ рдХрд┐рдП рдЧрдП рдкрд╛рдпрдерди рдкреБрд╕реНрддрдХрд╛рд▓рдп рдХрд╛ рд╕рдВрд╕реНрдХрд░рдг рд╣реИ рдЬреЛ iOS, visionOS, tvOS,
рдФрд░ macOS рдХреЗ рд▓рд┐рдП рд╣реИред рдпрд╣ рд╕реНрд╡рд┐рдлреНрдЯ рдкреНрд░реЛрдЬреЗрдХреНрдЯ рдореЗрдВ рдЙрдкрдпреЛрдЧ рдХрд┐рдпрд╛ рдЬрд╛ рд╕рдХрддрд╛ рд╣реИ рдмрд┐рдирд╛ рдкреБрд╕реНрддрдХрд╛рд▓рдп рдХреЛ рд╕реНрд░реЛрдд рд╕реЗ рдХрдВрдкрд╛рдЗрд▓ рдХрд░рдиреЗ рдХреА рдЖрд╡рд╢реНрдпрдХрддрд╛ рдХреЗред рдЙрджрд╛рд╣рд░рдг рдХреЗ рд▓рд┐рдП:

```swift
// swift-tools-version: 5.10
// The swift-tools-version declares the minimum version of Swift required to build this package.

import PackageDescription

let package = Package(
    name: "MyLlamaPackage",
    targets: [
        .executableTarget(
            name: "MyLlamaPackage",
            dependencies: [
                "LlamaFramework"
            ]),
        .binaryTarget(
            name: "LlamaFramework",
            url: "https://github.com/ggml-org/llama.cpp/releases/download/b5046/llama-b5046-xcframework.zip",
            checksum: "c19be78b5f00d8d29a25da41042cb7afa094cbf6280a225abe614b03b20029ab"
```

рдЙрдкрд░реЛрдХреНрдд рдЙрджрд╛рд╣рд░рдг рд▓рд╛рдЗрдмреНрд░реЗрд░реА рдХреЗ рдПрдХ рдордзреНрдпрд╕реНрде рдмрд┐рд▓реНрдб `b5046` рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░ рд░рд╣рд╛ рд╣реИред рдЗрд╕реЗ рдЕрд▓рдЧ рд╡рд░реНрдЬрди рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП URL рдФрд░ рдЪреЗрдХрд╕рдо рдХреЛ рдмрджрд▓рдХрд░ рд╕рдВрд╢реЛрдзрд┐рдд рдХрд┐рдпрд╛ рдЬрд╛ рд╕рдХрддрд╛ рд╣реИред

## рдкреВрд░реНрдгрддрд╛

рдХрдорд╛рдВрдб рд▓рд╛рдЗрди рдкреВрд░реНрдгрддрд╛ рдХреБрдЫ рдкрд░рд┐рд╡реЗрд╢реЛрдВ рдХреЗ рд▓рд┐рдП рдЙрдкрд▓рдмреНрдз рд╣реИред

#### Bash рдкреВрд░реНрдгрддрд╛

```bash
$ build/bin/llama-cli --completion-bash > ~/.llama-completion.bash
$ source ~/.llama-completion.bash
```

рд╡реИрдХрд▓реНрдкрд┐рдХ рд░реВрдк рд╕реЗ, рдЗрд╕реЗ рдЕрдкрдиреЗ `.bashrc` рдпрд╛ `.bash_profile` рдореЗрдВ рдЬреЛрдбрд╝рдХрд░ рдЗрд╕реЗ рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд░реВрдк рд╕реЗ рд▓реЛрдб рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдбрд╛рд▓рд╛ рдЬрд╛ рд╕рдХрддрд╛ рд╣реИред рдЙрджрд╛рд╣рд░рдг рдХреЗ рд▓рд┐рдП:

```console
$ echo "source ~/.llama-completion.bash" >> ~/.bashrc
```

## рдирд┐рд░реНрднрд░рддрд╛рдПрдВ

- [yhirose/cpp-httplib](https://github.com/yhirose/cpp-httplib) - рдПрдХ рд╣реЗрдбрд░ рдлрд╝рд╛рдЗрд▓ HTTP рд╕рд░реНрд╡рд░, `llama-server` рджреНрд╡рд╛рд░рд╛ рдЙрдкрдпреЛрдЧ рдХрд┐рдпрд╛ рдЬрд╛рддрд╛ рд╣реИ - MIT рд▓рд╛рдЗрд╕реЗрдВрд╕
- [stb-image](https://github.com/nothings/stb) - рдПрдХ рд╣реЗрдбрд░ рдлрд╝рд╛рдЗрд▓ рдЗрдореЗрдЬ рдлреЙрд░реНрдореЗрдЯ рдбрд┐рдХреЛрдбрд░, рдкреЛрд▓реАрдореЛрдбрд▓ рдкреНрд░рдгрд╛рд▓реА рджреНрд╡рд╛рд░рд╛ рдЙрдкрдпреЛрдЧ рдХрд┐рдпрд╛ рдЬрд╛рддрд╛ рд╣реИ - рд╕рд╛рд░реНрд╡рдЬрдирд┐рдХ рдбреЛрдореЗрди
- [nlohmann/json](https://github.com/nlohmann/json) - рдПрдХ рд╣реЗрдбрд░ рдлрд╝рд╛рдЗрд▓ JSON рдкреБрд╕реНрддрдХрд╛рд▓рдп, рд╡рд┐рднрд┐рдиреНрди рдЙрдкрдХрд░рдгреЛрдВ/рдЙрджрд╛рд╣рд░рдгреЛрдВ рджреНрд╡рд╛рд░рд╛ рдЙрдкрдпреЛрдЧ рдХрд┐рдпрд╛ рдЬрд╛рддрд╛ рд╣реИ - MIT рд▓рд╛рдЗрд╕реЗрдВрд╕
- [minja](https://github.com/google/minja) - C++ рдореЗрдВ рдиреНрдпреВрдирддрдо Jinja рдкрд╛рд░реНрд╕рд░, рд╡рд┐рднрд┐рдиреНрди рдЙрдкрдХрд░рдгреЛрдВ/рдЙрджрд╛рд╣рд░рдгреЛрдВ рджреНрд╡рд╛рд░рд╛ рдЙрдкрдпреЛрдЧ рдХрд┐рдпрд╛ рдЬрд╛рддрд╛ рд╣реИ - MIT рд▓рд╛рдЗрд╕реЗрдВрд╕
- [linenoise.cpp](.././tools/run/linenoise.cpp/linenoise.cpp) - C++ рдкреБрд╕реНрддрдХрд╛рд▓рдп рдЬреЛ readline-рдЬреИрд╕реА рд▓рд╛рдЗрди рд╕рдВрдкрд╛рджрди рдХреНрд╖рдорддрд╛рдПрдВ рдкреНрд░рджрд╛рди рдХрд░рддрд╛ рд╣реИ, `llama-run` рджреНрд╡рд╛рд░рд╛ рдЙрдкрдпреЛрдЧ рдХрд┐рдпрд╛ рдЬрд╛рддрд╛ рд╣реИ - BSD 2-рдХреНрд▓реЙрдЬ рд▓рд╛рдЗрд╕реЗрдВрд╕
- [curl](https://curl.se/) - рдХреНрд▓рд╛рдЗрдВрдЯ-рдкрдХреНрд╖ URL рд╕реНрдерд╛рдирд╛рдВрддрд░рдг рдкреБрд╕реНрддрдХрд╛рд▓рдп, рд╡рд┐рднрд┐рдиреНрди рдЙрдкрдХрд░рдгреЛрдВ/рдЙрджрд╛рд╣рд░рдгреЛрдВ рджреНрд╡рд╛рд░рд╛ рдЙрдкрдпреЛрдЧ рдХрд┐рдпрд╛ рдЬрд╛рддрд╛ рд╣реИ - [CURL рд▓рд╛рдЗрд╕реЗрдВрд╕](https://curl.se/docs/copyright.html)
- [miniaudio.h](https://github.com/mackron/miniaudio) - рдПрдХ рд╣реЗрдбрд░ рдлрд╝рд╛рдЗрд▓ рдСрдбрд┐рдпреЛ рдлреЙрд░реНрдореЗрдЯ рдбрд┐рдХреЛрдбрд░, рдкреЛрд▓реАрдореЛрдбрд▓ рдкреНрд░рдгрд╛рд▓реА рджреНрд╡рд╛рд░рд╛ рдЙрдкрдпреЛрдЧ рдХрд┐рдпрд╛ рдЬрд╛рддрд╛ рд╣реИ - рд╕рд╛рд░реНрд╡рдЬрдирд┐рдХ рдбреЛрдореЗрди
- [subprocess.h](https://github.com/sheredom/subprocess.h) - C рдФрд░ C++ рдХреЗ рд▓рд┐рдП рдПрдХ рд╣реЗрдбрд░ рдлрд╝рд╛рдЗрд▓ рдкреНрд░рдХреНрд░рд┐рдпрд╛ рд▓реЙрдиреНрдЪрд┐рдВрдЧ рд╕рдорд╛рдзрд╛рди - рд╕рд╛рд░реНрд╡рдЬрдирд┐рдХ рдбреЛрдореЗрди

