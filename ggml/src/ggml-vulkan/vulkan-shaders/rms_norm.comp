#version 450

#include "generic_binary_head.comp"
#include "types.comp"

#extension GL_EXT_control_flow_attributes : enable
#define BLOCK_SIZE 128

layout (constant_id = 1) const bool do_multiply = false;

layout(local_size_x = BLOCK_SIZE, local_size_y = 1, local_size_z = 1) in;

layout (binding = 3) readonly buffer AtomBuf {float precomputed_sum;};

shared FLOAT_TYPE sumsh[BLOCK_SIZE];

void rms_norm(uint num_iters, bool use_atomic_add) {
    const uint ncols     = p.ne00;
    const uint nrows     = gl_NumWorkGroups.x;
    const uint nchannels = gl_NumWorkGroups.y;

    const uint row       = use_atomic_add ? 0 : gl_WorkGroupID.x;
    const uint channel   = gl_WorkGroupID.y;
    const uint samp      = gl_WorkGroupID.z;
    // When using atomic add, the work is split across multiple workgroups in the x dimension
    const uint tid       = use_atomic_add ? gl_GlobalInvocationID.x : gl_LocalInvocationID.x;

    const uint stride_row       = p.nb01;
    const uint stride_channel   = p.nb02;
    const uint stride_sample    = p.nb03;

    uint32_t a_offset = samp*stride_sample + channel*stride_channel + row*stride_row + get_aoffset();
    uint32_t b_offset = src1_idx(0, row, channel, samp) + get_boffset();
    uint32_t d_offset = ((samp*nchannels + channel)*nrows + row)*ncols + get_doffset();

    FLOAT_TYPE sum = FLOAT_TYPE(0.0f); // partial sum for thread in warp

    if (use_atomic_add) {
        sum = precomputed_sum;
    } else {
        [[unroll]] for (uint col = tid, idx = 0; idx < num_iters; col += BLOCK_SIZE, ++idx) {
            FLOAT_TYPE xi = FLOAT_TYPE(0);
            if (col < ncols) {
                xi = FLOAT_TYPE(data_a[a_offset + col]);
            }
            sum += xi * xi;
        }

        sumsh[tid] = sum;
        // sum up partial sums and write back result
        barrier();
        [[unroll]] for (int s = BLOCK_SIZE / 2; s > 0; s >>= 1) {
            if (tid < s) {
                sum += sumsh[tid + s];
                sumsh[tid] = sum;
            }
            barrier();
        }
        sum = sumsh[0];
    }

    const FLOAT_TYPE mean = sum / FLOAT_TYPE(ncols);
    const FLOAT_TYPE scale = inversesqrt(mean + FLOAT_TYPE(p.param1));

    if (use_atomic_add) {
        // One element per thread when using the atomic add
        uint col = tid;
        if (do_multiply) {
            if (ncols > p.ne10) {
                if (col < ncols) {
                    data_d[d_offset + col] = D_TYPE(scale * FLOAT_TYPE(data_a[a_offset + col]) * FLOAT_TYPE(data_b[b_offset + fastmod(col, p.ne10)]));
                }
            } else {
                if (col < ncols) {
                    data_d[d_offset + col] = D_TYPE(scale * FLOAT_TYPE(data_a[a_offset + col]) * FLOAT_TYPE(data_b[b_offset + col]));
                }
            }
        } else {
            if (col < ncols) {
                data_d[d_offset + col] = D_TYPE(scale * FLOAT_TYPE(data_a[a_offset + col]));
            }
        }
    } else {
        if (do_multiply) {
            if (ncols > p.ne10) {
                [[unroll]] for (uint col = tid, idx = 0; idx < num_iters; col += BLOCK_SIZE, ++idx) {
                    if (col >= ncols) {
                        continue;
                    }
                    data_d[d_offset + col] = D_TYPE(scale * FLOAT_TYPE(data_a[a_offset + col]) * FLOAT_TYPE(data_b[b_offset + fastmod(col, p.ne10)]));
                }
            } else {
                [[unroll]] for (uint col = tid, idx = 0; idx < num_iters; col += BLOCK_SIZE, ++idx) {
                    if (col >= ncols) {
                        continue;
                    }
                    data_d[d_offset + col] = D_TYPE(scale * FLOAT_TYPE(data_a[a_offset + col]) * FLOAT_TYPE(data_b[b_offset + col]));
                }
            }
        } else {
            [[unroll]] for (uint col = tid, idx = 0; idx < num_iters; col += BLOCK_SIZE, ++idx) {
                if (col >= ncols) {
                    continue;
                }
                data_d[d_offset + col] = D_TYPE(scale * FLOAT_TYPE(data_a[a_offset + col]));
            }
        }
    }
}

void main() {
    const bool use_atomic_add = p.param3 != 0;
    if (use_atomic_add) {
        rms_norm(1, true);
        return;
    }

    // instantiate the rms_norm function for several different
    // dimensions, to allow loop unrolling
    uint num_blocks = (p.ne00 + BLOCK_SIZE - 1) / BLOCK_SIZE;
    if (num_blocks > 32) {
        rms_norm(num_blocks, false);
    } else if (num_blocks > 16) {
        rms_norm(32, false);
    } else if (num_blocks > 8) {
        rms_norm(16, false);
    } else if (num_blocks > 4) {
        rms_norm(8, false);
    } else if (num_blocks == 4) {
        rms_norm(4, false);
    } else if (num_blocks == 3) {
        rms_norm(3, false);
    } else if (num_blocks == 2) {
        rms_norm(2, false);
    } else if (num_blocks == 1) {
        rms_norm(1, false);
    }
}
