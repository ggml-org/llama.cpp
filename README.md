# ThunderLLAMA

**Apple Silicon Paged Attention for llama.cpp**

> Enabling efficient KV cache management on M1/M2/M3/M4 GPUs

---

## Overview

ThunderLLAMA is a fork of [llama.cpp](https://github.com/ggml-org/llama.cpp) with enhanced Paged Attention support for Apple Silicon GPUs. It addresses a critical issue in the upstream implementation where paged attention was effectively disabled even when the `LLAMA_PAGED_ATTENTION` flag was set.

## Architecture

### Block Pool Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Block Pool Architecture                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   llama_kv_cache                                                â”‚
â”‚       â”‚                                                         â”‚
â”‚       â”œâ”€â”€ block_pool: llama_block_pool                          â”‚
â”‚       â”‚       â”œâ”€â”€ k_pool: vector<ggml_tensor*> (per layer)     â”‚
â”‚       â”‚       â”œâ”€â”€ v_pool: vector<ggml_tensor*> (per layer)     â”‚
â”‚       â”‚       â”œâ”€â”€ block_table_gpu: ggml_tensor*                â”‚
â”‚       â”‚       â”œâ”€â”€ block_size: uint32_t                         â”‚
â”‚       â”‚       â””â”€â”€ n_blocks: uint32_t                           â”‚
â”‚       â”‚                                                         â”‚
â”‚       â””â”€â”€ llama_graph                                           â”‚
â”‚               â””â”€â”€ build_attn_mha()                              â”‚
â”‚                       â””â”€â”€ ggml_flash_attn_ext_set_paged()       â”‚
â”‚                               â”œâ”€â”€ cur (attention node)          â”‚
â”‚                               â”œâ”€â”€ block_table (actual tensor)   â”‚
â”‚                               â”œâ”€â”€ use_paged = 1                 â”‚
â”‚                               â””â”€â”€ strides...                    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

| Component | File | Description |
|-----------|------|-------------|
| `llama_block_pool` | `llama-block-pool.cpp/h` | Manages paged KV cache blocks |
| `ggml_flash_attn_ext_set_paged` | `ggml.c` | API to enable paged mode |
| `llama_kv_cache` | `llama-kv-cache.cpp/h` | Integrates block pool |
| `llama_context` | `llama-context.cpp` | Context-level paged params |
| `llama_graph` | `llama-graph.cpp` | Passes block_table to FA |

### Data Flow

```
1. Context Initialization
   llama_context::init()
       â†’ llama_memory_params.use_paged_attention = true
       â†’ llama_kv_cache::init() with block_pool

2. Block Pool Creation
   llama_block_pool::init()
       â†’ create k_pool[n_layers], v_pool[n_layers]
       â†’ create block_table_gpu tensor
       â†’ allocate GPU memory

3. Graph Building
   llama_graph::build_attn_mha()
       â†’ get block_pool from kv_cache
       â†’ ggml_flash_attn_ext_set_paged(cur, block_table, 1, ...)

4. Inference
   Metal Flash Attention kernel
       â†’ uses block_table for paged access
       â†’ computes attention with block strides
```

## The Right KPIs for Paged Attention

> **Paged Attention çš„ä»·å€¼ä¸æ˜¯è®©å•æ¬¡æ¨ç†æ›´å¿«ï¼Œè€Œæ˜¯è®©ç³»ç»Ÿæ›´ç¨³å®šã€æ›´å¯é **

vLLM çš„ PagedAttention æŠŠå®ƒå½“æˆ"KV cache çš„ OS paging"ï¼Œæ ¸å¿ƒæ”¶ç›Šæ˜¯ï¼š

### æ­£ç¡®çš„ KPI

| KPI | è¯´æ˜ | Paged Advantage |
|-----|------|-----------------|
| **CAPACITY** | åŒå†…å­˜é¢„ç®—ä¸‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ | æ›´é•¿ context / æ›´å¤šå¹¶å‘åºåˆ— |
| **OPERABILITY** | P95/P99 å»¶è¿ŸæŠ–åŠ¨ | æ›´ç¨³å®šï¼Œæ—  defrag é£™å‡ |
| **RELIABILITY** | é•¿æ—¶é—´è¿è¡Œç¨³å®šæ€§ | **ç»“æ„æ€§ç§»é™¤ defrag é—®é¢˜** |

### llama.cpp çš„ defrag é—®é¢˜

llama.cpp æœ‰çœŸå®æ¡ˆä¾‹ï¼š**defrag è§¦å‘åè¾“å‡ºä¹±ç ç›´åˆ°é‡å¯**

```
Contiguous KV Cache:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
æ—¶é—´ â†’ å†…å­˜ç¢ç‰‡ç§¯ç´¯ â†’ è§¦å‘ defrag â†’ è¾“å‡ºä¹±ç  â†’ é‡å¯

Paged KV Cache:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Block Pool â†’ æŒ‰éœ€åˆ†é… â†’ æ— ç¢ç‰‡ â†’ æ—  defrag â†’ ç¨³å®šè¿è¡Œ
```

**Paged Attention çš„ä»·å€¼ = æŠŠ defrag ä»ç³»ç»Ÿé‡Œ"ç»“æ„æ€§ç§»é™¤"**

### Performance Parity (åŸºçº¿éªŒè¯)

è™½ç„¶å•æ¬¡é€Ÿåº¦ä¸æ˜¯ KPIï¼Œä½†æˆ‘ä»¬éªŒè¯äº†æ€§èƒ½å¯¹ç­‰ï¼š

| Model | Mode | pp512 | tg128 |
|-------|------|-------|-------|
| TinyLlama 1.1B | Contiguous | 2907 t/s | 239 t/s |
| TinyLlama 1.1B | **Paged** | 2830 t/s | 247 t/s |
| Qwen3-30B MoE | Contiguous | 714 t/s | 74.4 t/s |
| Qwen3-30B MoE | **Paged** | 702 t/s | 73.5 t/s |

**ç»“è®º**: æ€§èƒ½å·®å¼‚ <3%ï¼ŒPaged æ¨¡å¼ä¸ç‰ºç‰²å•æ¬¡æ€§èƒ½

### å®æµ‹æ•°æ® (Qwen3-30B-A3B MoE, Apple M4)

| KPI | Contiguous | Paged | ç»“è®º |
|-----|-----------|-------|------|
| **CAPACITY (pp8192)** | 465 t/s | **497 t/s (+6.8%)** | âœ… Paged æ›´å¿« |
| **OPERABILITY (jitter)** | 6.0% | **0.7%** | âœ… Paged æ›´ç¨³å®š 8x |

### Benchmark Scripts

æˆ‘ä»¬æä¾›äº†æ­£ç¡® KPI çš„æµ‹è¯•è„šæœ¬ï¼š

```bash
# æµ‹è¯• CAPACITY / OPERABILITY / RELIABILITY
./benchmarks/paged-attention-kpi-v2.sh /path/to/model.gguf
```

### When to Use Paged Attention

| åœºæ™¯ | æ¨è |
|------|------|
| å•ç”¨æˆ·çŸ­å¯¹è¯ | Contiguous (æ›´ç®€å•) |
| é•¿ä¸Šä¸‹æ–‡ (>16K) | **Paged** (å†…å­˜æ•ˆç‡) |
| å¤šå¹¶å‘è¯·æ±‚ | **Paged** (åºåˆ—éš”ç¦») |
| ç”Ÿäº§ç¯å¢ƒæœåŠ¡ | **Paged** (ç¨³å®šæ€§) |
| é•¿æ—¶é—´è¿è¡Œ | **Paged** (æ—  defrag é£é™©) |

## Build Instructions

### Prerequisites

- macOS with Apple Silicon (M1/M2/M3/M4)
- Xcode Command Line Tools
- CMake >= 3.16

### Build

```bash
# Clone
git clone https://github.com/lisihao/ThunderLLAMA.git
cd ThunderLLAMA

# Build
cmake -B build
cmake --build build --config Release -j$(sysctl -n hw.ncpu)
```

### Run

```bash
# With Paged Attention + Flash Attention (recommended)
LLAMA_PAGED_ATTENTION=1 ./build/bin/llama-cli \
  -m /path/to/model.gguf \
  -fa 1 \
  -ngl 99 \
  -p "Hello, world!"
```

## Usage Examples

### CLI Inference

```bash
# Paged attention mode
LLAMA_PAGED_ATTENTION=1 ./build/bin/llama-cli \
  -m model.gguf -fa 1 -ngl 99 -c 4096 \
  -p "Explain quantum computing in simple terms"
```

### Benchmark

```bash
# Compare Contiguous vs Paged
echo "=== Contiguous ===" && ./build/bin/llama-bench -m model.gguf -fa 1 -p 512 -n 128
echo "=== Paged ===" && LLAMA_PAGED_ATTENTION=1 ./build/bin/llama-bench -m model.gguf -fa 1 -p 512 -n 128
```

### Server Mode

```bash
# Start server with paged attention
LLAMA_PAGED_ATTENTION=1 ./build/bin/llama-server \
  -m model.gguf -fa 1 --port 8080
```

## Technical Details

### The Bug We Fixed

**Before (Upstream)**:
```cpp
// llama-graph.cpp:1816-1820 (old code)
if (use_paged) {
    ggml_flash_attn_ext_set_paged(
        cur,
        nullptr,  // â† block_table was null
        0,        // â† use_paged was 0
        0, 0, 0, 0, 0
    );
}
```

**After (ThunderLLAMA)**:
```cpp
// llama-graph.cpp (fixed)
if (use_paged) {
    const auto * block_pool = kv_ctx->get_block_pool();
    if (block_pool && !block_pool->k_pool.empty()) {
        ggml_tensor * block_table = kv_ctx->get_block_table();
        ggml_flash_attn_ext_set_paged(
            cur,
            block_table,  // â† actual tensor
            1,            // â† use_paged = 1
            block_pool->block_size,
            block_stride_k, block_stride_v,
            token_stride_k, token_stride_v
        );
    }
}
```

### k_pool/v_pool Design

**Before**: Single tensor per cache (wrong for multi-layer)
```cpp
ggml_tensor * k_pool;  // One tensor for all layers
ggml_tensor * v_pool;
```

**After**: Per-layer vectors
```cpp
std::vector<ggml_tensor *> k_pool;  // One tensor per layer
std::vector<ggml_tensor *> v_pool;
```

### Memory Layout

```
Block Pool Memory Layout:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚ Layer 0  â”‚ Layer 1  â”‚ ... â”‚ Layer N-1 â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     â”‚           â”‚               â”‚
     â–¼           â–¼               â–¼
  k_pool[0]  k_pool[1]      k_pool[N-1]
  v_pool[0]  v_pool[1]      v_pool[N-1]

Block Table:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚ Block 0 â”‚ Block 1 â”‚ ... â”‚ Block M-1 â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     â”‚
     â””â”€â”€ Maps logical â†’ physical blocks
```

## Roadmap

### Phase 1: Core Implementation âœ…
- [x] Block pool implementation
- [x] Paged attention API in ggml
- [x] Integration with llama_kv_cache
- [x] Flash attention support
- [x] Performance validation

### Phase 2A: Prefix Caching (Copy-on-Write) ğŸ¯ NEXT
- [ ] Block sharing for common prefixes
- [ ] Copy-on-Write for diverging suffix
- [ ] Agent/Tool-chain åœºæ™¯ TTFT å¤§å¹…ä¸‹é™
- [ ] å‚è€ƒ: vLLM æŠ€æœ¯æŠ¥å‘Š "å…±äº«å‰ç¼€ç›´æ¥å¤ç”¨ç‰©ç†å—"

### Phase 2B: Continuous Batching + å¤šåºåˆ—è°ƒåº¦
- [ ] å¹¶å‘åºåˆ—è°ƒåº¦å™¨
- [ ] ç¢ç‰‡/defrag/åˆ†é…æŠ–åŠ¨æ§åˆ¶
- [ ] ååé‡ä¼˜åŠ¿ä½“ç°

### Phase 2C: Chunked Prefill
- [ ] é•¿ prompt åˆ‡å—å¤„ç†
- [ ] å‡å°‘ prefill å¸¦å®½å³°å€¼
- [ ] P99 å»¶è¿Ÿæ”¹å–„

### Phase 3: Advanced Features (Future)
- [ ] Speculative decoding integration
- [ ] Distributed inference support
- [ ] Cache eviction policies

### Phase 4: Production Readiness (Future)
- [ ] Comprehensive test suite
- [ ] Documentation and examples
- [ ] Performance profiling tools
- [ ] Integration with llama-server

## Contributing

Contributions are welcome! Please see:

1. **Issues**: Report bugs or request features
2. **Pull Requests**: Submit improvements
3. **Discussions**: Share ideas and use cases

### Development Setup

```bash
# Debug build
cmake -B build-debug -DCMAKE_BUILD_TYPE=Debug
cmake --build build-debug

# Run tests
./build-debug/bin/test-backend-ops
```

## Related Projects

- [llama.cpp](https://github.com/ggml-org/llama.cpp) - Upstream project
- [vLLM](https://github.com/vllm-project/vllm) - Paged attention paper
- [ggml](https://github.com/ggml-org/ggml) - Tensor library

## References

1. [Paged Attention Paper](https://arxiv.org/abs/2309.06180) - vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention
2. [Flash Attention](https://arxiv.org/abs/2205.14135) - Fast and Memory-Efficient Exact Attention
3. [Metal Performance Shaders](https://developer.apple.com/metal/) - Apple's GPU framework

## License

Same as llama.cpp (MIT License)

## Acknowledgments

- llama.cpp team for the excellent codebase
- vLLM team for the paged attention concept
- Apple for Metal framework and developer tools

---

**ThunderLLAMA** - Making Paged Attention roar on Apple Silicon ğŸâš¡
