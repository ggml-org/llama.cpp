name: Run LLaMA Server

on:
  workflow_dispatch:

jobs:
  run-llama:
    runs-on: ubuntu-latest

    steps:
      - name: ğŸ§¬ Checkout
        uses: actions/checkout@v3

      - name: ğŸ“ Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø¬Ù„Ø¯Ø§Øª
        run: |
          mkdir -p models
          mkdir -p build

      - name: â¬‡ï¸ ØªÙ†Ø²ÙŠÙ„ Ù…Ù„Ù server
        run: |
          wget -O build/server https://github.com/issa261/llama.cpp/raw/main/build/server
          chmod +x build/server

      - name: â¬‡ï¸ ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ gguf
        run: |
          wget -O models/tinyllama-1.1b-chat-v1.0.Q8_0.gguf \
          https://raw.githubusercontent.com/issa261/github-workflows-download-model.yml/main/tinyllama-1.1b-chat-v1.0.Q8_0.gguf

      - name: ğŸš€ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙŠØ±ÙØ±
        run: |
          ./build/server -m models/tinyllama-1.1b-chat-v1.0.Q8_0.gguf -c 512 -n 256 --host 0.0.0.0 --port 8080
