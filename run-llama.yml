name: Run LLaMA Server

on:
  workflow_dispatch:  # ÙŠØ´ØºÙ„ Ø§Ù„Ø³ÙƒØ±Ø¨Øª ÙŠØ¯ÙˆÙŠÙ‹Ø§ Ù…Ù† GitHub

jobs:
  run-llama:
    runs-on: ubuntu-latest

    steps:
      - name: ğŸ§¬ Ù†Ø³Ø® Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹
        uses: actions/checkout@v3

      - name: ğŸ“ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯Ø§Øª Ø¶Ø±ÙˆØ±ÙŠØ©
        run: |
          mkdir -p models
          mkdir -p build

      - name: â¬‡ï¸ ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠ server
        run: |
          wget -O build/server https://github.com/issa261/llama.cpp/raw/main/build/server
          chmod +x build/server

      - name: â¬‡ï¸ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ gguf
        run: |
          wget -O models/tinyllama-1.1b-chat-v1.0.Q8_0.gguf \
          https://raw.githubusercontent.com/issa261/github-workflows-download-model.yml/main/tinyllama-1.1b-chat-v1.0.Q8_0.gguf

      - name: ğŸš€ ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        run: |
          ./build/server -m models/tinyllama-1.1b-chat-v1.0.Q8_0.gguf -c 512 -n 256 --host 0.0.0.0 --port 8080
