name: Run LLaMA Server

on:
  workflow_dispatch:  # Ù„ØªØ´ØºÙŠÙ„Ù‡ ÙŠØ¯ÙˆÙŠØ§Ù‹ Ù…Ù† GitHub UI

jobs:
  run-llama:
    runs-on: ubuntu-latest

    steps:
      - name: ğŸ§¬ Ù†Ø³Ø® Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹
        uses: actions/checkout@v3

      - name: ğŸ“‚ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯Ø§Øª
        run: |
          mkdir -p models
          mkdir -p build

      - name: â¬‡ï¸ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠ (server)
        run: |
          wget -O build/server https://github.com/issa261/llama.cpp/raw/main/build/server
          chmod +x build/server

      - name: â¬‡ï¸ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ TinyLLaMA
        run: |
          wget -O models/tinyllama-1.1b-chat-v1.0.Q8_0.gguf \
          https://raw.githubusercontent.com/issa261/github-workflows-download-model.yml/main/tinyllama-1.1b-chat-v1.0.Q8_0.gguf

      - name: ğŸš€ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙŠØ±ÙØ±
        run: |
          ./build/server -m models/tinyllama-1.1b-chat-v1.0.Q8_0.gguf -c 512 -n 256 --host 0.0.0.0 --port 8080
